<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Nhu Anh Quynh Hoang">

  
  
  
    
  
  <meta name="description" content="Discussion of 5 popular techniques to speed up training in deep neuron net">

  
  <link rel="alternate" hreflang="en-us" href="/2020/05/30/speed-up-training-in-deep-neuron-net/">

  


  
  
  
  <meta name="theme-color" content="#101073">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/agate.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/agate.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:700%7CWork+Sans&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158640946-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-158640946-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hubeb4088d9e1ef3a12a3be812bce5943c_42313_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hubeb4088d9e1ef3a12a3be812bce5943c_42313_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/2020/05/30/speed-up-training-in-deep-neuron-net/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@nhu_hoang">
  <meta property="twitter:creator" content="@nhu_hoang">
  
  <meta property="og:site_name" content="Nhu Hoang">
  <meta property="og:url" content="/2020/05/30/speed-up-training-in-deep-neuron-net/">
  <meta property="og:title" content="Speed up training in deep neuron net | Nhu Hoang">
  <meta property="og:description" content="Discussion of 5 popular techniques to speed up training in deep neuron net"><meta property="og:image" content="/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_300x300_fit_lanczos_2.png">
  <meta property="twitter:image" content="/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_300x300_fit_lanczos_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-05-30T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-05-30T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/2020/05/30/speed-up-training-in-deep-neuron-net/"
  },
  "headline": "Speed up training in deep neuron net",
  
  "datePublished": "2020-05-30T00:00:00Z",
  "dateModified": "2020-05-30T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Nhu Anh Quynh Hoang"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Nhu Hoang",
    "logo": {
      "@type": "ImageObject",
      "url": "img//"
    }
  },
  "description": "Discussion of 5 popular techniques to speed up training in deep neuron net"
}
</script>

  

  


  


  





  <title>Speed up training in deep neuron net | Nhu Hoang</title>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  






  


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/"><img src="/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_0x70_resize_lanczos_2.png" alt="Nhu Hoang"></a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/"><img src="/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_0x70_resize_lanczos_2.png" alt="Nhu Hoang"></a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Portfolio</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#portfolios"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Speed up training in deep neuron net</h1>

  
  <p class="page-subtitle">Speed up training in deep neuron net</p>
  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Nhu Anh Quynh Hoang</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 30, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    10 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/2020/05/30/speed-up-training-in-deep-neuron-net/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/data-science/">Data Science</a>, <a href="/categories/deep-learning/">Deep learning</a>, <a href="/categories/classification/">Classification</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Training a large deep neuron network is a time and computation power consuming task and was the main reason for the unpopularity of DNN 20 years ago. As several techniques have been found out to improve the training speed, Deep learning has come back to the light. So which technique to use, how and when to use which. Let's discuss it here!</p>
<h2 id="1-choosing-the-appropriate-initialization">1. Choosing the appropriate Initialization</h2>
<p>Initialization is one of the first technique that fastens the training time of Neuron Network. Let's briefly explain its importance. In ANN, there are several connection between different neurons in the net. One neuron in the current layer connects to several neurons in the next layers and is attached to numerous in the previous layer. If 2 neurons interact frequently than another pair, their connection (i.e the weights) will be stronger than the other one.</p>
<p>However, one problem with the ANN is that at the beginning of training, the weight can either be too small or get too large which makes it too tiny or too massive to use further across the network. In other word, they are called <strong>The Vanishing Gradients</strong> or <strong>The Exploding Gradients</strong> problems.</p>
<p>So if the weights are set a a suitable random values at the beginning of the training, these problem can be avoid. This technique was propose by <a href="http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot and Bengio</a> and significantly lift the unstable problem. This initialization strategy is called <em>Xavier initialization</em> or <em>Glorot initialization</em>.</p>
<p>In this technique, the connection weight between neurons will be initialized randomly using normal distribution with $mean=0$ and variance $\sigma^2 = \frac{2}{fan_{in}+fan_{out}}$ with $fan_{in}$ is the number of input neurons and $fan_{out}$ is the number of output neurons.</p>
<p>There are 3 popular initialization technique: <strong>Glorot</strong> (used by default in Keras), <strong>He</strong> and <strong>LeCun</strong>. This technique should come along with <strong>activation function</strong>.</p>
<p>Let's examine different initialization techniques&rsquo; effect on model performance and training time with <code>fashion MNIST</code> dataset.</p>
<pre><code class="language-python"># Import package and split the data
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train_full = X_train_full / 255.0
X_test = X_test / 255.0
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
</code></pre>
<p>First, let's go with the default setting of Keras on a network of 5 hidden layers with 300, 100, 50, 50, 50 neurons respectively.</p>
<pre><code class="language-python">tf.random.set_seed(50)
np.random.seed(50)

model_default = keras.models.Sequential()
model_default.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_layers in (300, 100, 50, 50, 50):
  model_default.add(keras.layers.Dense(n_layers, activation ='relu'))
model_default.add(keras.layers.Dense(10, activation='softmax'))
    
model_default.compile(loss=&quot;sparse_categorical_crossentropy&quot;,
                      optimizer=keras.optimizers.SGD(lr=1e-3),
                      metrics=[&quot;accuracy&quot;])

start_time = time.time()
history = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)
print(&quot;--- %s seconds ---&quot; % (time.time() - start_time))
</code></pre>
<p>Result</p>
<pre><code class="language-python">Epoch 1/20
1688/1688 [==============================] - 5s 3ms/step - loss: 1.9231 - accuracy: 0.3496 - val_loss: 1.4457 - val_accuracy: 0.5502
Epoch 2/20
1688/1688 [==============================] - 5s 3ms/step - loss: 1.1028 - accuracy: 0.6481 - val_loss: 0.8583 - val_accuracy: 0.7220
Epoch 3/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.7838 - accuracy: 0.7289 - val_loss: 0.7118 - val_accuracy: 0.7373
Epoch 4/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.6772 - accuracy: 0.7588 - val_loss: 0.6423 - val_accuracy: 0.7728
Epoch 5/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.6184 - accuracy: 0.7789 - val_loss: 0.5866 - val_accuracy: 0.7867
Epoch 6/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.5805 - accuracy: 0.7921 - val_loss: 0.5568 - val_accuracy: 0.8062
Epoch 7/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.5535 - accuracy: 0.8019 - val_loss: 0.5344 - val_accuracy: 0.8135
Epoch 8/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.5321 - accuracy: 0.8112 - val_loss: 0.5351 - val_accuracy: 0.8087
Epoch 9/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.5156 - accuracy: 0.8178 - val_loss: 0.5096 - val_accuracy: 0.8203
Epoch 10/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.5017 - accuracy: 0.8233 - val_loss: 0.4980 - val_accuracy: 0.8243
Epoch 11/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4899 - accuracy: 0.8284 - val_loss: 0.4851 - val_accuracy: 0.8343
Epoch 12/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4789 - accuracy: 0.8319 - val_loss: 0.4800 - val_accuracy: 0.8343
Epoch 13/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4687 - accuracy: 0.8358 - val_loss: 0.4861 - val_accuracy: 0.8260
Epoch 14/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4605 - accuracy: 0.8390 - val_loss: 0.4591 - val_accuracy: 0.8393
Epoch 15/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4527 - accuracy: 0.8409 - val_loss: 0.4675 - val_accuracy: 0.8330
Epoch 16/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4443 - accuracy: 0.8443 - val_loss: 0.4623 - val_accuracy: 0.8305
Epoch 17/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4374 - accuracy: 0.8471 - val_loss: 0.4515 - val_accuracy: 0.8440
Epoch 18/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4299 - accuracy: 0.8495 - val_loss: 0.4454 - val_accuracy: 0.8428
Epoch 19/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4238 - accuracy: 0.8512 - val_loss: 0.4332 - val_accuracy: 0.8493
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4185 - accuracy: 0.8526 - val_loss: 0.4256 - val_accuracy: 0.8518
--- 99.03307843208313 seconds ---
</code></pre>
<p>85.18% with 99.3 seconds of training. This performance can be improved. If <code>activation ='relu'</code> is not set, the accuracy is only 84.95% with 104.5 seconds needed to train on.</p>
<p>Another Initialization that can be considered to use is <code>He</code>. Initialize this by adding <code>kernel_initializer=&quot;he_normal&quot;</code> to the hidden layers.</p>
<p>Result</p>
<pre><code class="language-python">Epoch 1/20
1688/1688 [==============================] - 5s 3ms/step - loss: 1.7039 - accuracy: 0.4326 - val_loss: 1.0764 - val_accuracy: 0.6665
Epoch 2/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.8365 - accuracy: 0.7089 - val_loss: 0.6898 - val_accuracy: 0.7625
Epoch 3/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.6503 - accuracy: 0.7769 - val_loss: 0.5991 - val_accuracy: 0.7937
Epoch 4/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.5770 - accuracy: 0.8010 - val_loss: 0.5531 - val_accuracy: 0.8080
Epoch 5/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.5362 - accuracy: 0.8133 - val_loss: 0.5147 - val_accuracy: 0.8193
Epoch 6/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.5091 - accuracy: 0.8217 - val_loss: 0.4960 - val_accuracy: 0.8275
Epoch 7/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4896 - accuracy: 0.8287 - val_loss: 0.4846 - val_accuracy: 0.8312
Epoch 8/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4732 - accuracy: 0.8343 - val_loss: 0.4764 - val_accuracy: 0.8303
Epoch 9/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4587 - accuracy: 0.8391 - val_loss: 0.4588 - val_accuracy: 0.8373
Epoch 10/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4479 - accuracy: 0.8436 - val_loss: 0.4445 - val_accuracy: 0.8422
Epoch 11/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4373 - accuracy: 0.8463 - val_loss: 0.4472 - val_accuracy: 0.8452
Epoch 12/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4282 - accuracy: 0.8500 - val_loss: 0.4341 - val_accuracy: 0.8517
Epoch 13/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4197 - accuracy: 0.8535 - val_loss: 0.4310 - val_accuracy: 0.8492
Epoch 14/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4129 - accuracy: 0.8557 - val_loss: 0.4338 - val_accuracy: 0.8487
Epoch 15/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4060 - accuracy: 0.8576 - val_loss: 0.4341 - val_accuracy: 0.8462
Epoch 16/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3992 - accuracy: 0.8594 - val_loss: 0.4179 - val_accuracy: 0.8502
Epoch 17/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3929 - accuracy: 0.8623 - val_loss: 0.4194 - val_accuracy: 0.8543
Epoch 18/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3871 - accuracy: 0.8634 - val_loss: 0.4079 - val_accuracy: 0.8587
Epoch 19/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3824 - accuracy: 0.8663 - val_loss: 0.3993 - val_accuracy: 0.8610
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3780 - accuracy: 0.8672 - val_loss: 0.3924 - val_accuracy: 0.8637
--- 99.76096153259277 seconds ---
</code></pre>
<p>The accuracy actually improved but the the running time was half a seconds slower than <strong>Glorot Initialization</strong></p>
<p>There are also discussions about the performance of <strong>normal distribution</strong> and <strong>uniform distribution</strong> in initialization technique, but there is in fact no better one between them. The result of <code>init = keras.initializers.VarianceScaling(scale=2.,mode='fan_avg',distribution='uniform')</code> does not improve for this data set (Train set accuracy: 87.05%, Val set: 86.27% and took 100.82 seconds to run)</p>
<h2 id="2-get-along-with-the-right-activation-function">2. Get along with the right Activation function</h2>
<p>Choosing an unfit activation function is one of the reason leading to poor model performance. <code>sigmoid</code> might be a good choice but I prefer to use <code>SELU</code>, <code>ReLU</code> and its variants.</p>
<p>Let's talk about ReLU first. Simply saying, if the value is larger than 0, the function returns the value itself; else it returns 0. This activation is fast to compute but in return there will be a case that it stops outputting anything other than 0 (i.e neurons were died). This issue usually happens in case of large learning rate.</p>
<p>Some of the solutions for this problem is to use some alternative versions of ReLU: <strong>LeakyReLU, Randomized LeakyReLU or Scaled ReLU (SELU)</strong>.</p>
<p>With <strong>LeakyReLU</strong>:</p>
<pre><code class="language-python">if x&gt;0:
  return x
else:
  return ax
</code></pre>
<p>in which a is $\alpha$, the slope of the $x$ given $x&lt;0$. $\alpha$ is usually set at 0.01, serve as a small leak (that's why this technique is called LeakyReLU). Using $\alpha$ helps to stop the dying problem (slope=0).</p>
<p>In case of <strong>Randomized LeakyReLU</strong>, $\alpha$ is selected randomly given a range. Given this method, we can expect it reduces the Overfitting issue.</p>
<p>One of the outperformed activation function for deep NN is <strong>Scaled ReLU (SELU)</strong>.</p>
<pre><code class="language-python">if x&gt;0:
  return Lambda*x
else:
  return Lambda*(alpha*exp(x)-alpha)
</code></pre>
<p>In this function, each layer outputs&rsquo; mean is 0 and standard deviation is 1. Note when using this activation function:</p>
<ul>
<li><input checked="" disabled="" type="checkbox">It must be used with <code>kernel_initializer=&quot;lecun_normal&quot;</code></li>
<li><input checked="" disabled="" type="checkbox">The input features must be standardized</li>
<li><input checked="" disabled="" type="checkbox">The NN's architecture must be sequential</li>
</ul>
<p>Let's try different Activation function on the <code>fashion MNIST</code> dataset.</p>
<p>Result of <strong>LeakyReLU</strong></p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3791 - accuracy: 0.8670 - val_loss: 0.3910 - val_accuracy: 0.8615
--- 101.87710905075073 seconds ---
</code></pre>
<p>Result of <strong>Randomized LeakyReLU</strong></p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 6s 3ms/step - loss: 0.3779 - accuracy: 0.8667 - val_loss: 0.3918 - val_accuracy: 0.8630
--- 113.58738899230957 seconds ---
</code></pre>
<p>Result of <strong>SELU</strong></p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 19/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3526 - accuracy: 0.8763 - val_loss: 0.3755 - val_accuracy: 0.8647
--- 106.25733232498169 seconds ---
</code></pre>
<p><strong>SELU</strong> seems to achieve slightly better performance over SeLU and its variants but is slower than ReLU and LeakyReLU (as expected).</p>
<span class="markup-quote"><strong>If the NN performs relatively well at low learning rate, ReLU is the optimal choice given the fastest training time. In case of deep NN, SELU is the excellent choice</strong></span>
<p>Detailed explanation about these activations are here: <a href="http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">ReLU</a>, <a href="https://arxiv.org/abs/1505.00853">LeakyReLU, Randomized LeakyReLU</a> and <a href="https://arxiv.org/abs/1706.02515">SELU</a></p>
<h2 id="3-batch-normalization">3. Batch Normalization</h2>
<p>To ensure Vanishong/Exploding Gradients problems do not happen again during training (as Initialization and Activation function can help reduce these issues at the beginning of the training)</p>
<p>Basically, <strong>Batch Normalization</strong> zeros centers and normalizes each input, then scales and shifts the result using 1 parameter vector for scaling and 1 for shifting. This technique evaluates the $mean$ and $standard  deviation$ of the input over the current mini-batch and repeats this calculation across all mini-batches of the training set.</p>
<p>If using <strong>Batch Normalization</strong>, the input data will not need to be standardized prior training.</p>
<p>When implementing <strong>Batch Normalization</strong>, the vector of input means $\mu$ and vector of input standard devition $\sigma$ will be non-trainable parameters (i.e. untouchable by backpropagation) and to be used to compute the moving averages at the end of the training and these final parameters will be used to normalize new data to make prediction.</p>
<pre><code class="language-python">tf.random.set_seed(50)
np.random.seed(50)

model_default = keras.models.Sequential()
model_default.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_layers in (300, 100, 50, 50, 50):
  model_default.add(keras.layers.BatchNormalization())
  model_default.add(keras.layers.Dense(n_layers, activation ='relu', kernel_initializer=&quot;he_normal&quot;))
model_default.add(keras.layers.Dense(10, activation='softmax'))
    
model_default.compile(loss=&quot;sparse_categorical_crossentropy&quot;,
                      optimizer=keras.optimizers.SGD(lr=1e-3),
                      metrics=[&quot;accuracy&quot;])

start_time = time.time()
history = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)
print(&quot;--- %s seconds ---&quot; % (time.time() - start_time))
</code></pre>
<p>Result</p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 8s 5ms/step - loss: 0.3799 - accuracy: 0.8645 - val_loss: 0.3571 - val_accuracy: 0.8685
--- 167.6186249256134 seconds ---
</code></pre>
<div class="alert alert-warning">
  <div>
    Batch Normalization is strictly implemented in Recurrent NN
  </div>
</div>
<h2 id="gradient-clipping">Gradient Clipping</h2>
<p>As <strong>Batch Normalization</strong> is recommended not to use with Recurrent NN, <strong>Gradient Clipping</strong> is the alternative choice for RNN.</p>
<h2 id="summary-of-the-result">Summary of the result</h2>
<table>
<thead>
<tr>
<th>Initialization</th>
<th align="center">Activation fuction</th>
<th align="center">Train set accuracy</th>
<th align="center">Val set accuracy</th>
<th align="right">Running time (seconds)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Glorot</td>
<td align="center">No Activation</td>
<td align="center">85.32%</td>
<td align="center">84.95%</td>
<td align="right">104.5</td>
</tr>
<tr>
<td>Glorot - Normal Dist</td>
<td align="center">ReLU</td>
<td align="center">85.26%</td>
<td align="center">85.18%</td>
<td align="right">99.03</td>
</tr>
<tr>
<td>He - Normal Dist</td>
<td align="center">ReLU</td>
<td align="center">86.72%</td>
<td align="center">86.37%</td>
<td align="right">99.76</td>
</tr>
<tr>
<td>He - Uniform  Dist</td>
<td align="center">ReLU</td>
<td align="center">87.05%</td>
<td align="center">86.27%</td>
<td align="right">100.82</td>
</tr>
<tr>
<td>He - Normal  Dist</td>
<td align="center">Leaky ReLU</td>
<td align="center">86.7%</td>
<td align="center">86.15%</td>
<td align="right">101.87</td>
</tr>
<tr>
<td>He - Normal  Dist</td>
<td align="center">Randomized LeakyReLU</td>
<td align="center">86.67%</td>
<td align="center">86.3%</td>
<td align="right">113.58</td>
</tr>
<tr>
<td>LeCun</td>
<td align="center">SELU</td>
<td align="center">87.63%</td>
<td align="center">86.47%</td>
<td align="right">106.25</td>
</tr>
<tr>
<td>Batch normalization He - Normal  Dist</td>
<td align="center">ReLU</td>
<td align="center">86.45%</td>
<td align="center">86.85%</td>
<td align="right">167.618</td>
</tr>
</tbody>
</table>
<div class="alert alert-note">
  <div>
    These performances are subject to change depending on the dataset and NN's architecture
  </div>
</div>
<figure>
  <img src="polynomial_feature.png" alt="" style="width:80%">
  <figcaption>After quadratic feature added, instances  are now distintively separated into 2 classes</figcaption>
</figure>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/classification/">Classification</a>
  
  <a class="badge badge-light" href="/tags/application/">Application</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/2020/05/30/speed-up-training-in-deep-neuron-net/&amp;text=Speed%20up%20training%20in%20deep%20neuron%20net" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/2020/05/30/speed-up-training-in-deep-neuron-net/&amp;t=Speed%20up%20training%20in%20deep%20neuron%20net" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Speed%20up%20training%20in%20deep%20neuron%20net&amp;body=/2020/05/30/speed-up-training-in-deep-neuron-net/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/2020/05/30/speed-up-training-in-deep-neuron-net/&amp;title=Speed%20up%20training%20in%20deep%20neuron%20net" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Speed%20up%20training%20in%20deep%20neuron%20net%20/2020/05/30/speed-up-training-in-deep-neuron-net/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/2020/05/30/speed-up-training-in-deep-neuron-net/&amp;title=Speed%20up%20training%20in%20deep%20neuron%20net" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  
    
  
  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu6a3e1909338aab76a89db8a31cb94a9e_2179125_250x250_fill_q90_lanczos_center.JPG" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Nhu Anh Quynh Hoang</a></h5>
      <h6 class="card-subtitle">Data Scientist / Portfolio Planning Specialist - Amway Japan</h6>
      <p class="card-text">My interest is Time series forecasting, Customer segmentation, classification; Machine learning and Deeep learning. I earned Master Degree in Interdisciplinary Studies from Tohoku University and have decent experience in various industries.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/nhu-hoang/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/geniusnhu" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@hoanganhquynhnhu" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>




<section id="comments">
  
    

  
</section>




<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/2020/04/30/support-vector-machine-explanation-and-application/" rel="prev">Support Vector Machine explanation and application</a>
  </div>
  
</div>

</div>



  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/2020/04/30/support-vector-machine-explanation-and-application/">Support Vector Machine explanation and application</a></li>
      
      <li><a href="/project/the-beauty-of-transformer-in-bringing-more-applications-to-life/">The beauty of Transformer in bringing more applications to life</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/markdown.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a0d331bcd05dbe8b31e244f796710f08.js"></script>

    


  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  <p class="powered-by">
    Â© 2019 Nhu Hoang<br/>Served by <a href="https://geniusnhu.netlify.app/">Netlify</a> &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "nhu-hoang" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
