<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Application | Nhu Hoang</title>
    <link>/tags/application/</link>
      <atom:link href="/tags/application/index.xml" rel="self" type="application/rss+xml" />
    <description>Application</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019 Nhu HoangServed by [Netlify](https://geniusnhu.netlify.app/)</copyright><lastBuildDate>Thu, 30 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_300x300_fit_lanczos_2.png</url>
      <title>Application</title>
      <link>/tags/application/</link>
    </image>
    
    <item>
      <title>Support Vector Machine explanation and application</title>
      <link>/2020/04/30/support-vector-machine-explanation-and-application/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/2020/04/30/support-vector-machine-explanation-and-application/</guid>
      <description>&lt;p&gt;In a classification task, there are several ways to do the trick. It can be solved by separating classes by linear (straight) line, or using a tree to split up attributes according to certain thresholds until reaching to the expected level, or calculating the probability of the event to belong to which class.&lt;/p&gt;
&lt;p&gt;Support Vector Machine is a &lt;strong&gt;non-probabilistic binary linear classifier&lt;/strong&gt; and a versatile Machine Learning algorithm that can perform both &lt;strong&gt;classification and regression tasks&lt;/strong&gt;. Another advantages of SVM is its ability to solve on both &lt;strong&gt;linear and non-linear datasets&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Given these numerous benefits, there are many concepts and solutions in SVM that I found just a few articles/videos really gives an easily understandable explanation, especially targeting ones who are new to SVM. I hope this post reach you in the most comprehensive way.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;original-concept&#34;&gt;Original concept&lt;/h2&gt;
&lt;p&gt;All of this started with the idea of using a line (with 2D dataset) or a hyperplane (more than 3D) to separate the instances into 2 classes, and try to &lt;strong&gt;maximize the distance between the line and the closest instances&lt;/strong&gt;. This distance is denoted as &lt;strong&gt;Margin&lt;/strong&gt;. The below figure illustrates this.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;margin.png&#34; alt=&#34;&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Why it needs to maximize this margin? The reason is that a Decision boundary lies right between 2 classes is much better than one that falls nearer at one class than another.&lt;/p&gt;
&lt;p&gt;However, imagine that there is an &lt;strong&gt;outlier&lt;/strong&gt; of the Orange class and it lies closers to Blue class than its own. If we strictly impose the above concept to this dataset, it will result into the below picture. Now, the margin satisfies the requirement but turns out to be much smaller than the above one. This is called &lt;strong&gt;Hard margin&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;hard_margin1.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;If there is a new Blue class data that falls near this orange instance, that new data will be misclassified as Orange, which in other word, means the model performs worse on new data than on train one (which we never wants to have with our model).&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;hard_margin2.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;soft-margin&#34;&gt;Soft margin&lt;/h2&gt;
&lt;p&gt;There is one way to solve this, by allowing some misclassification on outliers of train set to maximize the margin on the rest of training data. This concept was named &lt;strong&gt;Soft margin&lt;/strong&gt; or in other word, &lt;strong&gt;Support Vector Machine&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;soft_margin1.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Once there is new data, it will be correctly classified as Blue.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;soft_margin2.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Then, one question arises. How can we decide the Soft margin? (How do we know which instances to be misclassified in training?).&lt;/p&gt;
&lt;p&gt;Actually, there is no perfect answer for this. You train the data on several values of margin decides to use the optimal one for your problem. The hyperparameter controls this in VC models in scikit-learn is denoted as $C$. If the model is overfitting, reduce $C$.&lt;/p&gt;
&lt;p&gt;It is also because that SVM uses only 1 linear line or hyperplane to do the classification job, it is a binary classification solver. In case of multiclass problem, +One-versus-All (or One-versus-Rest) strategy* will be implied.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Therefore, one of the most important rule of SVM algorithm is that it tries to find a good balance between maximizing the margin street, and limiting the Margin violation (misclassification)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;svm-on-non-linear-dataset&#34;&gt;SVM on non-linear dataset&lt;/h2&gt;
&lt;p&gt;However, for non-linear separable data, how can we use this trick? Looking at the below illustration, we will need 3 lines to separate the data into 2 classes, and with more complex data, we will need even more. This is computationally inefficient.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;non_linear.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Then, here comes the &lt;strong&gt;Kernel trick&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Instead of teaching the model on 2D data, the &lt;strong&gt;kernel trick&lt;/strong&gt; will add other features such as polynomial features and then SVM will utilize a hyperplane to split up data into 2 classes. The above data after adding 2-degree polynomial feature will look like this:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;polynomial_feature.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;After quadratic feature added, instances  are now distintively separated into 2 classes&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;p&gt;Let&#39;s use data to further understand this.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random
np.random.seed(42)
m = 500
X1 = 2 * np.random.rand(m, 1)
X2 = (4 + 3 * X1**2 + np.random.randn(m, 1)).ravel()
X12 = np.column_stack((X1,X2))
y1 = np.zeros((500))
X3 = np.random.rand(m, 1)
X4 = (1 + X1**1 + 2*np.random.randn(m, 1)).ravel()
X34 = np.column_stack((X3,X4))
y2 = np.ones((500))
X = np.concatenate((X12, X34), axis=0)
y = np.concatenate((y1, y2), axis=0)

def plot_dataset(X, y, axes):
    plt.plot(X[:, 0][y==0], X[:, 1][y==0], &amp;quot;bs&amp;quot;)
    plt.plot(X[:, 0][y==1], X[:, 1][y==1], &amp;quot;g^&amp;quot;)
    plt.axis(axes)
    plt.grid(True, which=&#39;both&#39;)
    plt.xlabel(&amp;quot;Feature 1&amp;quot;, fontsize=20)
    plt.ylabel(&amp;quot;Feature 2&amp;quot;, fontsize=20, rotation=0)
    
plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])
plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;data_plot.png&#34; alt=&#34;&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;Vizualize data with 2 classes&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;polynomial-kernel&#34;&gt;Polynomial kernel&lt;/h2&gt;
&lt;p&gt;I used &lt;em&gt;SVC&lt;/em&gt; class in scikit-learn with polynomial kernel at 3 degree with $coef$ hyperparameter equals to 1 (it controls how much the model is influenced by high-degree vs low-degree polynomials). $LinearSVC(loss=&amp;quot;hinge&amp;rdquo;)$ with an prior $PolynomialFeatures(degree=3)$ transformer will do the same trick.&lt;/p&gt;
&lt;p&gt;If you have very large dataset, go ahead with $LinearSVC$ because it is faster than $SVC$ in handling big data.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;One thing to remember, always scaling data before training SVM&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;poly_kernel_svm_clf = Pipeline([
	(&amp;quot;scaler&amp;quot;, StandardScaler()),
	(&amp;quot;svm_clf&amp;quot;, SVC(kernel=&amp;quot;poly&amp;quot;, degree=3, coef0=1, C=0.001))
])
poly_kernel_svm_clf.fit(X_train, y_train)

poly_kernel_svm_clf10 = Pipeline([
	(&amp;quot;scaler&amp;quot;, StandardScaler()),
	(&amp;quot;svm_clf&amp;quot;, SVC(kernel=&amp;quot;poly&amp;quot;, degree=3, coef0=1, C=10))
])
poly_kernel_svm_clf10.fit(X_train, y_train)

# Plot the model overall prediction
def plot_predictions(model, axes):
    &amp;quot;&amp;quot;&amp;quot;
    Vizualize the classification result of the model to see how it
    corresponds to training data
    &amp;quot;&amp;quot;&amp;quot;
    x0s = np.linspace(axes[0], axes[1], 1000)
    x1s = np.linspace(axes[2], axes[3], 1000)
    x0, x1 = np.meshgrid(x0s, x1s)
    X = np.c_[x0.ravel(), x1.ravel()]
    y_pred = model.predict(X).reshape(x0.shape)
    y_decision = model.decision_function(X).reshape(x0.shape)
    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)
    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)
    
fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)
plt.sca(axes[0])
plot_predictions(poly_kernel_svm_clf, [-0.25,2.25,-5,20])
plot_dataset(X_train, y_train)
plt.title(r&amp;quot;$degree=3, C=0.001$&amp;quot;, fontsize=18)

plt.sca(axes[1])
plot_predictions(poly_kernel_svm_clf10, [-0.25,2.25,-5,20])
plot_dataset(X_train, y_train)
plt.title(r&amp;quot;$degree=3, C=10$&amp;quot;, fontsize=18)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;poly_kernel.png&#34; alt=&#34;&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;Vizualize data with 2 classes&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The model with value of C equals to 10 seems to get to the point quite well, let&#39;s measure its performance on test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import f1_score
model_list = [rbf_kernel_svm_clf,rbf_kernel_svm_clf10]

for model in model_list:
    y_pred = model.predict(X_test)
    print(f1_score(y_test, y_pred, average=&#39;weighted&#39;))
    
    
0.6459770114942529
0.8542027171311809
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;gaussian-rbf-kernel&#34;&gt;Gaussian RBF Kernel&lt;/h2&gt;
&lt;p&gt;Now, I want to try a different kernel with this data, I will use  &lt;strong&gt;Gaussian RBF Kernel&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As my data is not too large, &lt;em&gt;Gaussian RBF Kernel&lt;/em&gt; does not take much time. However, with a large dataset, &lt;em&gt;Gaussian RBF Kernel&lt;/em&gt; will consume quite amount of your time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42)

# Create pipeline for training
rbf_kernel_svm_clf = Pipeline([
	(&amp;quot;scaler&amp;quot;, StandardScaler()),
	(&amp;quot;svm_clf&amp;quot;, SVC(kernel=&amp;quot;rbf&amp;quot;, gamma=0.1, C=0.001))
])
rbf_kernel_svm_clf.fit(X_train, y_train)

rbf_kernel_svm_clf10 = Pipeline([
	(&amp;quot;scaler&amp;quot;, StandardScaler()),
	(&amp;quot;svm_clf&amp;quot;, SVC(kernel=&amp;quot;rbf&amp;quot;, gamma=5, C=10))
])
rbf_kernel_svm_clf10.fit(X_train, y_train)

# Plot the model overall prediction
fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)
plt.sca(axes[0])
plot_predictions(rbf_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])
plot_dataset(X_train, y_train)
plt.title(r&amp;quot;$gamma=5, C=0.001$&amp;quot;, fontsize=18)

plt.sca(axes[1])
plot_predictions(rbf_kernel_svm_clf10, [-1.5, 2.5, -1, 1.5])
plot_dataset(X_train, y_train)
plt.title(r&amp;quot;$gamma=5, C=10$&amp;quot;, fontsize=18)
plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;RBF_kernel.png&#34; alt=&#34;&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;Vizualize data with 2 classes&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;2 values of C seems to produce similar model. Let&#39;s predict test set and evaluate with metrics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import f1_score
model_list = [rbf_kernel_svm_clf,rbf_kernel_svm_clf10]

for model in model_list:
    y_pred = model.predict(X_test)
    print(f1_score(y_test, y_pred, average=&#39;weighted&#39;))
    
    
0.8417207792207791
0.8544599213495534
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, 2 models perform quite equivalent with C = 10 has slightly higher value and also slightly higher than the polynomial kernel model above. We can improve this with tuning hyperparameter, cross validation, add other type of feature transformation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The beauty of Transformer in bringing more applications to life</title>
      <link>/project/the-beauty-of-transformer-in-bringing-more-applications-to-life/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/project/the-beauty-of-transformer-in-bringing-more-applications-to-life/</guid>
      <description>&lt;p&gt;My 2020 started awesomely with a Machine Learning Forum @Google Japan and followed with 2 Meetups about Natural Language Processing application. NLP has been the focus of the Machine Learning society for the last decade and it is reaching to its ultimate point with several outbreak of innovations and application.&lt;/p&gt;
&lt;p&gt;At this moment, 2020, NLP is heading towards &lt;strong&gt;speed and big data&lt;/strong&gt;, which means that the increasing of the speed and size of data is the key objective of future NLP innovations. At the moment, I am impressed with &lt;strong&gt;BERT - Bidirectional Encoder Representations from Transformers&lt;/strong&gt;, a powerful state-of-the-art NPL model introduced by Google in 2018; and *&lt;strong&gt;Google Meena&lt;/strong&gt;, a lift toward dealing with big-NLP database using the Transformer base introducted on Jan 28, 2020.&lt;/p&gt;
&lt;h3 id=&#34;1why-are-bert-and-transformer-being-called-the-revolution-of-the-nlp-world&#34;&gt;1.	Why are BERT and Transformer being called the revolution of the NLP world?&lt;/h3&gt;
&lt;p&gt;To understand &lt;strong&gt;Transformer model&lt;/strong&gt; inclduing BERT, we need to take a look of the progress from &lt;strong&gt;Seq2Seq (sequence to sequence)&lt;/strong&gt; and its evolution to attention and to BERT.&lt;/p&gt;
&lt;h4 id=&#34;11-seq2seq-model&#34;&gt;1.1 Seq2Seq model:&lt;/h4&gt;
&lt;p&gt;In NLP, the end that a machine is expected to understand is the meaning of the sentence, not only word by word. &lt;strong&gt;Seq2Seq&lt;/strong&gt; is a technique to train the machine in which it takes a sequence of an item and generates another sequence as output.&lt;/p&gt;
&lt;p&gt;Within the model, there contains &lt;strong&gt;Encoder&lt;/strong&gt; and &lt;strong&gt;Decoder&lt;/strong&gt;. The &lt;strong&gt;Encoder&lt;/strong&gt; receives the original text and convert the text into a &lt;strong&gt;Context vector&lt;/strong&gt; that the Machine can read. Then, the &lt;strong&gt;Decoder&lt;/strong&gt; does the job of generating a new sequence of items based on the &lt;strong&gt;Context vector&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In Seq2Seq, a sentence does not need to go through both Encoder and Decoder, it can stop at Encoder. Some example of Encoder only is the suggested word &lt;em&gt;&amp;ldquo;message&amp;rdquo;&lt;/em&gt; after you type &lt;em&gt;&amp;ldquo;Thank you for your&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt; or &lt;strong&gt;Context vector&lt;/strong&gt; is a vector of floats representing the input sequence. &lt;strong&gt;&amp;ldquo;Word Embedding&amp;rdquo;&lt;/strong&gt; is the algorithm used to transform text into vector, and the size of vector is usually 265, 512 or 1024 dimensions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Seq2Seq_visual.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;12-attention&#34;&gt;1.2 Attention&lt;/h4&gt;
&lt;p&gt;One of the disadvantages of Context in Seq2Seq is the &lt;strong&gt;dealing with long sentences and handling the sequence of output&lt;/strong&gt;. Context is generated by the Word embedding algorithm and the longer the sentence or the paragraph, the bigger the size of vector and the more memory consuming.&lt;/p&gt;
&lt;p&gt;Moreover, the context in Seq2Seq &lt;strong&gt;was not built to figure out the similarity between words&lt;/strong&gt; because it does not focus on the &lt;strong&gt;relevancy of the words in the sentence&lt;/strong&gt;. This leads to the issue that a sentence in English cannot be translated correctly to Japanese which has the reserve order in sentence structure.&lt;/p&gt;
&lt;p&gt;The concept of Attention was introduced in &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Bahdanau et al., 2014&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1508.04025&#34;&gt;Luong et al., 2015&lt;/a&gt; in which it takes into account the relevant parts in the sentence.&lt;/p&gt;
&lt;p&gt;Instead of passing the last hidden state to the &lt;strong&gt;Decoder&lt;/strong&gt;, the Attention model &lt;strong&gt;passes all the hidden states to the Decoder&lt;/strong&gt; with the summary process as below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Give each hidden state a score&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Softmax function&lt;/strong&gt; to multiply each hidden state. This brings about high Hidden state scores and low hidden state scores or in other word, it generates the probability of each hidden state associating with the input word.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Decoder&lt;/strong&gt; will sum up all the weighted softmax Hidden state vectors into a context vector and concatenate it with its original hidden state vector.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Attention_Visual.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The advantage of this model is the capability to &lt;strong&gt;choose the decoding word based on the probability of that word in associating with the original input without losing the sequential characteristics of the sentence&lt;/strong&gt;. This has high effectiveness in dealing with translating of common words such as &amp;ldquo;the&amp;rdquo;, &amp;ldquo;his&amp;rdquo;, &amp;ldquo;of&amp;rdquo;, etc. and the sequence of different languages.&lt;/p&gt;
&lt;h4 id=&#34;13-transformer&#34;&gt;1.3 Transformer:&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; is built on the foundation of &lt;strong&gt;Attention&lt;/strong&gt; model. Therefore, &lt;strong&gt;Transformer&lt;/strong&gt; can deal with the relevancy of the sentence rather than converting from word to word in &lt;strong&gt;Seq2Seq&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The biggest difference of &lt;strong&gt;Transformer&lt;/strong&gt; vs &lt;strong&gt;Seq2Seq&lt;/strong&gt; is that instead of generating 1 vector from &lt;strong&gt;Encoder&lt;/strong&gt;, &lt;strong&gt;Transformer&lt;/strong&gt; model uses 3 vectors in order to decide which other parts of the sentence are important (or unimportant) to that word.&lt;/p&gt;
&lt;p&gt;The below table show more details of the calculation.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Word&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Q vector&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;K vector&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;V vector&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Softmax&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Sum&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;First word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q1xK1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S1=Q1xK1/8xV1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;love&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q1xK2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S2=Q1xK2/8xV2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Z1 = S1+S2+S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q1xK3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S3=Q1xK3/8xV3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Second word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q2xK1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S1=Q2xK1/8xV1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;love&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q2xK2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S2=Q2xK2/8xV2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Z2 = S1+S2+S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q2xK3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S3=Q2xK3/8xV3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Third word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q3xK1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S1=Q3xK1/8xV1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;love&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q3xK2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S2=Q3xK2/8xV2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Z3 = S1+S2+S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q3xK3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S3=Q3xK3/8xV3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;One example of this model application is the suggestion of relevant words when typing sentence. Gmail can suggest &lt;strong&gt;&amp;ldquo;message&amp;rdquo;, &amp;ldquo;reply&amp;rdquo;, &amp;ldquo;call&amp;rdquo;&lt;/strong&gt; at the same time based on the typed sentence &lt;strong&gt;&amp;ldquo;Thank you for your&lt;/strong&gt;&amp;quot;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a brief introduction on the transition from Seq2Seq to Transformer and how the &lt;strong&gt;Transformer model&lt;/strong&gt; outstands &lt;strong&gt;Seq2Seq&lt;/strong&gt; at the moment.&lt;/p&gt;
&lt;h3 id=&#34;2-business-applications-of-nlp&#34;&gt;2. Business Applications of NLP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chatbot&lt;/strong&gt;: This is obviously the forefront application of NLP, which can be seen across all industries and companies. Given its popularity, there are several tools to support building a chatbot such as Google DialogFlow, Microsoft LUIS. These tools can be customized based on the user&#39;s needs; however, they can only deal with simple requests.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Machine translation&lt;/strong&gt;: such as Google translate or pocket translator device.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Search engine&lt;/strong&gt;: 5 years ago, when you searched something on search engine, whether you type key words &amp;ldquo;to Tokyo&amp;rdquo; or the whole sentences &amp;ldquo;How to go to Tokyo&amp;rdquo;, the machine would generate quite similar results. However, with the evolution of BERT and Transformer, searching the whole sentence will throw you to a better search result.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monitoring of brand and product - Sentiment analysis:&lt;/strong&gt; This is the field that I used to analyze during my first job. I used sentiment analysis on big scale online platforms including online forums, social network, brand website and e-commerce sites to understand the reaction of consumers toward a campaign or a brand in order to react promptly toward negative trend related to the brand.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Place to display an advertisement&lt;/strong&gt;: display ads based on context or categorization and make sure that the article is appropriate at the placing place. Honestly saying, I have not seen much of this application around.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Remarketing&lt;/strong&gt;: an online advertisement based on the browsing history of a user to target them with similar advertising product to drive them back the previous interest. This personalized application is a very effective tool in today online market in which thousands of sellers trying to attract each of their customers. Youtube, Facebook or Google are the biggest applicators.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Medical assistant&lt;/strong&gt;: although called &amp;ldquo;assistant&amp;rdquo;, the major task of this service is to transcript the discussion between doctors and patients.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text generation&lt;/strong&gt;: this is one of the applications of Decoding in NLP, in which the machine will generate a complete article from what it was learnt or summarize a paragraph. As you may know, there are many contradictions about this application, especially the emergence of fake news in recent years. With the completion of this technology, whether the fake news issue continues its expansion or is stopped is still a big question.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Information extraction&lt;/strong&gt;: extract dynamic required information that is sleeping in the database system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resume reviews&lt;/strong&gt;: use NLP to scan the applicants&amp;rsquo; resume to figure out potential candidate for the interview. This application sticks with Amazon big scandal. Amazon used to use this to scan the resume which led to the inequality between male and female with the result preferred male than female. This is due to 0the bias toward male in the training set of the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Voice command&lt;/strong&gt;: an emerging technology in recent years with the appearance of smart device such as Siri in Iphone, Alexa of Amazone, Google home or Cortana of Microsoft.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beside the major technological trends mentioned above, the application trend is heading toward diversity in languages and translation efficacy. Moreover, not only applying NLP alone, there are more applications combining NLP and Voice recognition or Computer Vision.&lt;/p&gt;
&lt;p&gt;NLP is a powerful Machine Learning area and its application is supporting human&#39;s life even more than we can expect. Therefore, NLP is one of the most used Machine Learning fields by Data Scienctist.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
