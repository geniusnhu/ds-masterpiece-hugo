<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science | Nhu Hoang</title>
    <link>https://geniusnhu.netlify.com/tags/data-science/</link>
      <atom:link href="https://geniusnhu.netlify.com/tags/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 Nhu HoangServed by [Netlify](https://geniusnhu.netlify.app/)</copyright><lastBuildDate>Sat, 01 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://geniusnhu.netlify.com/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_300x300_fit_lanczos_3.png</url>
      <title>Data Science</title>
      <link>https://geniusnhu.netlify.com/tags/data-science/</link>
    </image>
    
    <item>
      <title>Optimize Memory Tips in Python</title>
      <link>https://geniusnhu.netlify.com/project/2022-01-01-optimize-memory-tips-in-python/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/project/2022-01-01-optimize-memory-tips-in-python/</guid>
      <description>&lt;p&gt;Memory management in Python is not a simple issue to solve, it requires a decent understanding of Python objects and data structures. Unlike in C/C++, users have no control over memory management. It is taken over by Python itself. However, with some insights about how Python works and memory support modules, we can somehow seed some light on how to control this issue.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;how-much-memory-is-being-allocated&#34;&gt;How much memory is being allocated?&lt;/h2&gt;
&lt;p&gt;There are several ways to get the size of an object in Python. You can use &lt;code&gt;sys.getsizeof()&lt;/code&gt; to get the exact size of the object, &lt;code&gt;objgraph.show_refs()&lt;/code&gt; to visualize the structure of an object or &lt;code&gt;psutil.Process().memory_info().rss&lt;/code&gt; to get all memory is allocated at the moment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; import sys
&amp;gt;&amp;gt;&amp;gt; import objgraph
&amp;gt;&amp;gt;&amp;gt; import psutil
&amp;gt;&amp;gt;&amp;gt; import pandas as pd


&amp;gt;&amp;gt;&amp;gt; ob = np.ones((1024, 1024, 1024, 3), dtype=np.uint8)

### Check object &#39;ob&#39; size
&amp;gt;&amp;gt;&amp;gt; sys.getsizeof(ob) / (1024 * 1024)
3072.0001373291016

### Check current memory usage of whole process (include ob and installed packages, ...)
&amp;gt;&amp;gt;&amp;gt; psutil.Process().memory_info().rss / (1024 * 1024)
3234.19140625

### Check structure of &#39;ob&#39; (Useful for class object)
&amp;gt;&amp;gt;&amp;gt; objgraph.show_refs([ob], filename=&#39;sample-graph.png&#39;)

### Check memory for pandas.DataFrame
&amp;gt;&amp;gt;&amp;gt; from sklearn.datasets import load_boston
&amp;gt;&amp;gt;&amp;gt; data = load_boston()
&amp;gt;&amp;gt;&amp;gt; data = pd.DataFrame(data[&#39;data&#39;])
&amp;gt;&amp;gt;&amp;gt; print(data.info(verbose=False, memory_usage=&#39;deep&#39;))
&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 506 entries, 0 to 505
Columns: 13 entries, 0 to 12
dtypes: float64(13)
memory usage: 51.5 KB
  
### Check memory for pandas.Series
&amp;gt;&amp;gt;&amp;gt; data[0].memory_usage(deep=True)   # deep=True to include all the memory used by underlying parts that construct the pd.Series
4176

#### deep=True to include all the memory used by underlying parts that construct the pd.Series ###

&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;sample-graph.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt; Structure of ‘ob’ &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.python.org/3/library/tracemalloc.html&#34;&gt;tracemalloc&lt;/a&gt; is also another choice. It is included in the Python standard library and provides block-level traces of memory allocation, statistics for the overall memory behavior of a program.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; import tracemalloc
&amp;gt;&amp;gt;&amp;gt; import numpy as np

&amp;gt;&amp;gt;&amp;gt; def create_array(x, y):
&amp;gt;&amp;gt;&amp;gt;     x = x**2
&amp;gt;&amp;gt;&amp;gt;     y = y**3
&amp;gt;&amp;gt;&amp;gt;     return np.ones((x, y, 1024, 3), dtype=np.uint8)

&amp;gt;&amp;gt;&amp;gt; tracemalloc.start()
&amp;gt;&amp;gt;&amp;gt; ### Run application
&amp;gt;&amp;gt;&amp;gt; arr = create_array(30,10)
&amp;gt;&amp;gt;&amp;gt; ### take memory usage snapshot
&amp;gt;&amp;gt;&amp;gt; snapshot = tracemalloc.take_snapshot()
&amp;gt;&amp;gt;&amp;gt; top_stats = snapshot.statistics(&#39;lineno&#39;)

&amp;gt;&amp;gt;&amp;gt; ### Print top 10 files allocating the most memory
&amp;gt;&amp;gt;&amp;gt; print(&amp;quot;[ Top 10 ]&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; for stat in top_stats[:10]:
&amp;gt;&amp;gt;&amp;gt;     print(stat)


[ Top 10 ]
/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py:192: size=2637 MiB, count=2, average=1318 MiB
/usr/lib/python3.7/threading.py:289: size=216 B, count=6, average=36 B
/usr/lib/python3.7/codeop.py:141: size=189 B, count=2, average=94 B
/usr/local/lib/python3.7/dist-packages/debugpy/_vendored/pydevd/pydevd.py:1532: size=144 B, count=2, average=72 B
/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2820: size=112 B, count=3, average=37 B
/usr/lib/python3.7/queue.py:182: size=80 B, count=1, average=80 B
/usr/lib/python3.7/queue.py:107: size=80 B, count=1, average=80 B
/usr/lib/python3.7/threading.py:1264: size=72 B, count=1, average=72 B
    
## ==================================================================================
##### Explanation #####
## count: Number of memory blocks (int)
## size: Total size of memory blocks in bytes (int)
## traceback: Traceback where the memory block was allocated, Traceback instance

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most used file is the arr object which takes up 2 memory blocks with a total size of 2637 MiB. Other objects are minimal.&lt;/p&gt;
&lt;p&gt;Another important technique is to estimate how much memory is needed for the process to run. This can be guessed through monitoring the peak memory usage of the process. To measure peak memory, you can use the below code at the end of the process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
### For Linux (in KiB) and MacOS (in bytes)
from resource import getrusage, RUSAGE_SELF
print(getrusage(RUSAGE_SELF).ru_maxrss)
### For Windows
import psutil
print(psutil.Process().memory_info().peak_wset)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having the peak number and the amount of data put in the process, you can, by some means, judge the amount of memory to be consumed for your next process.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;optimize-memory&#34;&gt;Optimize memory&lt;/h1&gt;
&lt;h2 id=&#34;1-utilize-pytorch-dataloader&#34;&gt;1. Utilize Pytorch DataLoader&lt;/h2&gt;
&lt;p&gt;Training a large dataset is a bottleneck for your memory and you will never be able to train a complete model given the whole dataset never fits in your memory at the same time, especially for unstructured data such as image, text, voice,… However, with Pytorch DataLoader, you manage to set up various mini-batches for the whole dataset and each is loaded uninterruptedly into your model (the number of samples depends on your memory capability). You can see &lt;a href=&#34;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&#34;&gt;here&lt;/a&gt; the tutorial on using Pytorch DataLoader.&lt;/p&gt;
&lt;p&gt;However, if you want to train a Machine Learning model on tabular data without using Deep learning (hence, not using Pytorch) or you don’t have access to a Database and have to work solely on the memory, what will be the choice for memory optimization?&lt;/p&gt;
&lt;h2 id=&#34;2-optimized-data-type&#34;&gt;2. Optimized data type&lt;/h2&gt;
&lt;p&gt;By understanding how data is stored and manipulated and using the optimal data type for the tasks, it will save you huge space in memory and computation time. In Numpy, there are multiple types, including boolean (bool), integer (int), Unsigned integer (uint), float, complex, datetime64, timedelta64, object_, etc…&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
### Check numpy integer
&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; ii16 = np.iinfo(np.int16)
&amp;gt;&amp;gt;&amp;gt; ii16
iinfo(min=-32768, max=32767, dtype=int16)
### Access min value
&amp;gt;&amp;gt;&amp;gt; ii16.min
-32768

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I narrow them down to &lt;strong&gt;uint, int, and float&lt;/strong&gt; given these are the most common when training models, handling data in Python. Depending on different needs and objectives, using sufficient data types becomes vital know-how. To check type minimum and maximum values, you can use function &lt;code&gt;numpy.iinfo()&lt;/code&gt;, &lt;code&gt;and numpy.finfo()&lt;/code&gt; for float.&lt;/p&gt;
&lt;p&gt;Below is the summary information for each type.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;number_memory.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The CSV file size doubles if the data type is converted to &lt;code&gt;numpy.float64&lt;/code&gt;, which is the default type of numpy.array, compared to &lt;code&gt;numpy.float32&lt;/code&gt;. Therefore, &lt;strong&gt;float32&lt;/strong&gt; is one of the optimal ones to use (Pytorch datatype is also &lt;strong&gt;float32&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;As the default data type &lt;code&gt;numpy.float()&lt;/code&gt; is &lt;strong&gt;float64&lt;/strong&gt; and &lt;code&gt;numpy.int()&lt;/code&gt; is &lt;strong&gt;int64&lt;/strong&gt;, remember to define the dtype when creating numpy array will save a huge amount of memory space.&lt;/p&gt;
&lt;p&gt;When working with DataFrame, there will be another usual type which is &lt;strong&gt;“object”&lt;/strong&gt;. Converting from object to category for feature having various repetitions will help computation time faster.&lt;/p&gt;
&lt;p&gt;Below is an example function to optimize the pd.DataFrame data type for scalars and strings.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def data_optimize(df, object_option=False):
    &amp;quot;&amp;quot;&amp;quot;Reduce the size of the input dataframe
    Parameters
    ----------
    df: pd.DataFrame
        input DataFrame
    object_option : bool, default=False
        if true, try to convert object to category
    Returns
    -------
    df: pd.DataFrame
        data type optimized output dataframe
    &amp;quot;&amp;quot;&amp;quot;

    # loop columns in the dataframe to downcast the dtype
    for col in df.columns:
        # process the int columns
        if df[col].dtype == &#39;int&#39;:
            col_min = df[col].min()
            col_max = df[col].max()
            # if all are non-negative, change to uint
            if col_min &amp;gt;= 0:
                if col_max &amp;lt; np.iinfo(np.uint8).max:
                    df[col] = df[col].astype(np.uint8)
                elif col_max &amp;lt; np.iinfo(np.uint16).max:
                    df[col] = df[col].astype(np.uint16)
                elif col_max &amp;lt; np.iinfo(np.uint32).max:
                    df[col] = df[col].astype(np.uint32)
                else:
                    df[col] = df[col]
            else:
                # if it has negative values, downcast based on the min and max
                if col_max &amp;lt; np.iinfo(np.int8).max and col_min &amp;gt; np.iinfo(np.int8).min:
                    df[col] = df[col].astype(np.int8)
                elif col_max &amp;lt; np.iinfo(np.int16).max and col_min &amp;gt; np.iinfo(np.int16).min:
                    df[col] = df[col].astype(np.int16)
                elif col_max &amp;lt; np.iinfo(np.int32).max and col_min &amp;gt; np.iinfo(np.int32).min:
                    df[col] = df[col].astype(np.int32)
                else:
                    df[col] = df[col]
                    
        # process the float columns
        elif df[col].dtype == &#39;float&#39;:
            col_min = df[col].min()
            col_max = df[col].max()
            # downcast based on the min and max
            if col_min &amp;gt; np.finfo(np.float32).min and col_max &amp;lt; np.finfo(np.float32).max:
                df[col] = df[col].astype(np.float32)
            else:
                df[col] = df[col]

        if object_option:
            if df[col].dtype == &#39;object&#39;:
                if len(df[col].value_counts()) &amp;lt; 0.5 * df.shape[0]:
                    df[col] = df[col].astype(&#39;category&#39;)

    return df

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to easily and efficiently reduce &lt;code&gt;pd.DataFrame&lt;/code&gt; memory footprint is to import data with specific columns using &lt;code&gt;usercols&lt;/code&gt; parameters in &lt;code&gt;pd.read_csv()&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-avoid-using-global-variables-instead-utilize-local-objects&#34;&gt;3. Avoid using global variables, instead utilize local objects&lt;/h2&gt;
&lt;p&gt;Python is faster at retrieving a local variable than a global one. Moreover, declaring too many variables as global leads to Out of Memory issue as these remain in the memory till program execution is completed while local variables are deleted as soon as the function is over and release the memory space which it occupies. Read more at &lt;a href=&#34;https://towardsdatascience.com/the-real-life-skill-set-that-data-scientists-must-master-8746876d5b2e&#34;&gt;The real-life skill set that data scientists must master&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-use-yield-keyword&#34;&gt;4. Use yield keyword&lt;/h2&gt;
&lt;p&gt;Python yield returns a generator object, which converts the expression given into a generator function. To get the values of the object, it has to be iterated to read the values given to the yield. To read the generator’s values, you can use &lt;code&gt;list()&lt;/code&gt;, for loop, or &lt;code&gt;next()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; def say_hello():
&amp;gt;&amp;gt;&amp;gt;    yield &amp;quot;HELLO!&amp;quot;
&amp;gt;&amp;gt;&amp;gt; SENTENCE = say_hello()
&amp;gt;&amp;gt;&amp;gt; print(next(SENTENCE))
HELLO!

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, generators are for one-time use objects. If you access it the second time, it returns empty.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; def say_hello():
&amp;gt;&amp;gt;&amp;gt;    yield &amp;quot;HELLO!&amp;quot;
&amp;gt;&amp;gt;&amp;gt; SENTENCE = say_hello()
&amp;gt;&amp;gt;&amp;gt; print(next(SENTENCE))
HELLO!
&amp;gt;&amp;gt;&amp;gt; print(&amp;quot;calling the generator again: &amp;quot;, list(SENTENCE))
calling the generator again: []

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As there is no value returned unless the generator object is iterated, no memory is used when the Yield function is defined, while calling Return in a function leads to the allocation in memory.&lt;/p&gt;
&lt;p&gt;Hence, &lt;strong&gt;Yield&lt;/strong&gt; is suitable for large dataset, or when you don’t need to store all the output values but just one value for each iteration of the main function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; import sys
&amp;gt;&amp;gt;&amp;gt; my_generator_list = (i*2 for i in range(100000))
&amp;gt;&amp;gt;&amp;gt; print(f&amp;quot;My generator is {sys.getsizeof(my_generator_list)} bytes&amp;quot;)
My generator is 128 bytes
&amp;gt;&amp;gt;&amp;gt; timeit(my_generator_list)
10000000 loops, best of 5: 32 ns per loop
  
&amp;gt;&amp;gt;&amp;gt; my_list = [i*2 for i in range(1000000)]
&amp;gt;&amp;gt;&amp;gt; print(f&amp;quot;My list is {sys.getsizeof(my_list)} bytes&amp;quot;)
My list is 824472 bytes
&amp;gt;&amp;gt;&amp;gt; timeit(my_list)
10000000 loops, best of 5: 34.5 ns per loop

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the above code, the list comprehension is 6441 times heavier than the generator and runs slower than the other generator.&lt;/p&gt;
&lt;h2 id=&#34;5-built-in-optimizing-methods-of-python&#34;&gt;5. Built-in Optimizing methods of Python&lt;/h2&gt;
&lt;p&gt;Use Python Built-in Functions to improve code performance. Check the &lt;a href=&#34;https://docs.python.org/3/library/functions.html&#34;&gt;list&lt;/a&gt; of functions.&lt;/p&gt;
&lt;h3 id=&#34;utilize-__slots__-in-defining-class&#34;&gt;Utilize &lt;strong&gt;slots&lt;/strong&gt; in defining class&lt;/h3&gt;
&lt;p&gt;Python class objects’ attributes are stored in the form of a dictionary. Thus, defining thousands of objects is the same as allocating thousands of dictionaries to the memory space. And adding &lt;code&gt;__slots__&lt;/code&gt; (which reduces the wastage of space and speeds up the program by allocating space for a fixed amount of attributes.)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import pandas as pd
import objgraph

### class without __slots__
class PointWithDict():
    def __init__(self, iters):
        self.iters = iters
    def convert(self):
        s = [&amp;quot;xyz&amp;quot;]*self.iters
        s = &amp;quot;&amp;quot;.join(s)
        assert len(s) == 3*iters
        
w_dict = PointWithDict(10000)
objgraph.show_refs([w_dict], filename=&#39;PointWithDict_structure.png&#39;)

### class using __slots__
class PointWithSlots():
    __slots__ = &amp;quot;iters&amp;quot;
    def __init__(self, iters):
        self.iters = iters
    def convert(self):
        s = [&amp;quot;xyz&amp;quot;]*self.iters
        s = &amp;quot;&amp;quot;.join(s)
        assert len(s) == 3*iters
        
w_slots = PointWithSlots(10000)
objgraph.show_refs([w_slots],filename=&#39;PointWithSlots_structure.png&#39;)

### Check memory footprint
&amp;gt;&amp;gt;&amp;gt; print(sys.getsizeof(w_dict), sys.getsizeof(w_dict.__weakref__), sys.getsizeof(w_dict.__dict__))
64 16 120
&amp;gt;&amp;gt;&amp;gt; print(sys.getsizeof(ob))
56

&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;PointWithDict_structure.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt; Structure of Point module without __slots__  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;PointWithSlots_structure.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt; Structure of Point module with __slots__, there is no __dict__ anymore &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Regarding memory usage, given there is no longer &lt;code&gt;__dict__&lt;/code&gt; in a class object, the memory space reduces noticeably from (64+16+120)=200 to 56 bytes.&lt;/p&gt;
&lt;h3 id=&#34;use-join-instead-of--to-concatenate-string&#34;&gt;Use join() instead of ‘+’ to concatenate string&lt;/h3&gt;
&lt;p&gt;As strings are immutable, every time you add an element to a string by the “+” operator, a new string will be allocated in memory space. The longer the string, the more memory consumed, the less efficient the code becomes. Using &lt;code&gt;join()&lt;/code&gt; can improve speed &amp;gt;30% vs ‘+’ operator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
### Concatenate string using &#39;+&#39; operation
def add_string_with_plus(iters):
    s = &amp;quot;&amp;quot;
    for i in range(iters):
        s += &amp;quot;abc&amp;quot;
    assert len(s) == 3*iters
    
### Concatenate strings using join() function
def add_string_with_join(iters):
    l = []
    for i in range(iters):
        l.append(&amp;quot;abc&amp;quot;)
    s = &amp;quot;&amp;quot;.join(l)
    assert len(s) == 3*iters
    
### Compare speed
&amp;gt;&amp;gt;&amp;gt; timeit(add_string_with_plus(10000))
100 loops, best of 5: 3.74 ms per loop
&amp;gt;&amp;gt;&amp;gt; timeit(add_string_with_join(10000))
100 loops, best of 5: 2.3 ms per loop

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are other methods to improve speed and save memory, check details &lt;a href=&#34;https://wiki.python.org/moin/PythonSpeed&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;itertools&#34;&gt;itertools&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import itertools
import sys

def append_matrix_with_itertools(X, Y):
  
    &amp;quot;&amp;quot;&amp;quot; Loop matrix using itertools.product()
    &amp;quot;&amp;quot;&amp;quot;
    
    MTX = np.zeros((X, Y))
    for i, j in itertools.product(range(X), range(Y)):
        if (i%2==0) &amp;amp; (i%3==1):
            MTX[i, j] = i**2/10
            
    return MTX

def append_matrix_with_loop(X, Y):
  
    &amp;quot;&amp;quot;&amp;quot; Loop matrix using normal for loop
    &amp;quot;&amp;quot;&amp;quot;
    
     MTX = np.zeros((X, Y))
    for i in range(X):
        for j in range(Y):
            if (i%2==0) &amp;amp; (i%3==1):
                MTX[i, j] = i**2/10
    return MTX


&amp;gt;&amp;gt;&amp;gt; MTX_itertools = append_matrix_with_itertools(MTX.shape[0], MTX.shape[1])
&amp;gt;&amp;gt;&amp;gt; MTX_loop = append_matrix_with_loop(MTX.shape[0], MTX.shape[1])

### Matrix size
&amp;gt;&amp;gt;&amp;gt; print(sys.getsizeof(MTX_itertools)/ (1024 * 1024))
&amp;gt;&amp;gt;&amp;gt; print(sys.getsizeof(MTX_loop)/ (1024 * 1024))
7.6295013427734375
7.6295013427734375

### Run time
&amp;gt;&amp;gt;&amp;gt; timeit(append_matrix_with_itertools(1000, 1000))
1 loop, best of 5: 234 ms per loop
&amp;gt;&amp;gt;&amp;gt; timeit(append_matrix_with_loop(1000, 1000))
1 loop, best of 5: 347 ms per loop

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or flatten a list with &lt;code&gt;itertools.chain()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
### Concatenate string using &#39;+&#39; operation
def add_string_with_plus(iters):
    s = &amp;quot;&amp;quot;
    for i in range(iters):
        s += &amp;quot;abc&amp;quot;
    assert len(s) == 3*iters
    
### Concatenate strings using join() function
def add_string_with_join(iters):
    l = []
    for i in range(iters):
        l.append(&amp;quot;abc&amp;quot;)
    s = &amp;quot;&amp;quot;.join(l)
    assert len(s) == 3*iters
    
### Compare speed
&amp;gt;&amp;gt;&amp;gt; timeit(add_string_with_plus(10000))
100 loops, best of 5: 3.74 ms per loop
&amp;gt;&amp;gt;&amp;gt; timeit(add_string_with_join(10000))
100 loops, best of 5: 2.3 ms per loop

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out &lt;a href=&#34;https://docs.python.org/3/library/itertools.html#module-itertools&#34;&gt;itertools documentation&lt;/a&gt; for more methods. I recommend exploring:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.accumulate(iterable , func)&lt;/code&gt;: accumulate through iterable. func can be an operator.func or default Python functions such as max, min…&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.compress(iterable, selectors)&lt;/code&gt;: filters the iterable with another (the other object can be treated as a condition)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.filterfalse(predicate, iterable)&lt;/code&gt;: filter and drop values that satisfy the predicate. This is useful and fast for filtering a list object.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.repeat(object[, times])&lt;/code&gt;: repeat a object value N times. However, I prefer using list multiplication &lt;code&gt;[&#39;hi&#39;]*1000&lt;/code&gt; to repeat ‘hi’ 1000 times than using &lt;code&gt;itertools.repeat(&#39;hi&#39;, 1000)&lt;/code&gt; (12.2 µs per loop vs 162 µs per loop respectively)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.zip_longest(*iterables, fillvalue=None)&lt;/code&gt;: zip multiple iterables into tuples and fill None value with the value specified in fillvalue.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;6-import-statement-overhead&#34;&gt;6. Import Statement Overhead&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;import&lt;/code&gt; statement can be executed from anywhere. However, executing outside of a function will run much faster than the inside one, even though the package is declared as a global variable (doit2), but in return, takes up more memory space than the other one.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;figure.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt; Comparison of import execution position &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;7-data-chunk&#34;&gt;7. Data chunk&lt;/h2&gt;
&lt;p&gt;I’m confident to say that most of the time you don’t use all of your data all at once and loading them in 1 big batch may crash the memory. Therefore, chunking data or load in small chunks, process the chunk of data, and save the result is one of the most useful techniques to prevent memory error.&lt;/p&gt;
&lt;p&gt;pandas lets you do that with the help of &lt;code&gt;chunksize&lt;/code&gt; or &lt;code&gt;iterator&lt;/code&gt; parameters in &lt;code&gt;pandas.read_csv()&lt;/code&gt; and &lt;code&gt;pandas.read_sql()&lt;/code&gt;. sklearn also supports training in small chunks with &lt;code&gt;partial_fit()&lt;/code&gt; method for most models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; from sklearn.linear_model import SGDRegressor
&amp;gt;&amp;gt;&amp;gt; from sklearn.datasets import make_regression
&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; import pandas as pd

&amp;gt;&amp;gt;&amp;gt; ### Load original data
&amp;gt;&amp;gt;&amp;gt; original_data = pd.read_csv(&#39;sample.csv&#39;)
&amp;gt;&amp;gt;&amp;gt; print(f&#39;Shape of original data {original_data.shape:.f02}&#39;)
Shape of original data (100000, 21)


&amp;gt;&amp;gt;&amp;gt; ### Load in chunk
&amp;gt;&amp;gt;&amp;gt; chunksize = 1000
&amp;gt;&amp;gt;&amp;gt; reg = SGDRegressor()
&amp;gt;&amp;gt;&amp;gt; features_columns = [str(i) for i in range(20)]

&amp;gt;&amp;gt;&amp;gt; ### Fit each chunk
&amp;gt;&amp;gt;&amp;gt; for train_df in pd.read_csv(&amp;quot;sample.csv&amp;quot;, chunksize=chunksize, iterator=True):
&amp;gt;&amp;gt;&amp;gt;     X = train_df[features_columns]
&amp;gt;&amp;gt;&amp;gt;     Y = train_df[&amp;quot;target&amp;quot;]
&amp;gt;&amp;gt;&amp;gt;     reg.partial_fit(X, Y)

### The reg.partial_fit() method fit each chunk at a time and update weights accordingly after each the next chunk is loaded 

&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;take-aways-&#34;&gt;Take-aways 📖&lt;/h1&gt;
&lt;p&gt;Handling Memory Error in Python is a tricky task and the root cause might be never been detected if you go another way around.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, investigate which process/variable is the core reason leading to the memory overflown problem.
Second, apply the applicable memory optimization methods for that object, estimate the new memory footprint and examine if the new one solves the issue.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If not, try to optimize related processes (such as reducing whole process memory consumption to save more space for the core object).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try the above tips and if the trouble remains, consider building the process with chunk/batch operation with the support of an outside database service.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Monitor MakerDao status using Daistats statistics</title>
      <link>https://geniusnhu.netlify.com/project/2021-07-30-daistats-analytics/</link>
      <pubDate>Fri, 30 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/project/2021-07-30-daistats-analytics/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://oasis.app/&#34;&gt;MakerDAO&lt;/a&gt; is one of the 3 largest Decentralized Finance platforms in crypto currency market, which has been received a lot of attention since 2020. Maker is the place where you can deposit collateral and loan DAI (1 DAI = 1 USD).&lt;/p&gt;
&lt;p&gt;In Maker, there are several types of vaults (loan plan in normal practice) in which accepts different cryptocurrencies and tokens as collateral. These vaults also vary in Stability Fee and Minimum collateral Ratio, which ranges from 102% to as high as 175%.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;maker_LP.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Firgure 1: Maker landing page&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;vaults.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Firgure 2: Different vaults in Maker (as of July 2021)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Now, to the main topic of this post.&lt;/p&gt;
&lt;p&gt;Given DAI is generated as collateral is backed in Maker vault, and the total market cap of DAI is more than &lt;strong&gt;$5.5 Billion&lt;/strong&gt;, the movement of the DAI and the collateral in and out each vault tell a lot more stories than we expect. Therefore, several tracking websites and tools &lt;a href=&#34;https://github.com/makerdao/awesome-makerdao#analytics-and-metrics&#34;&gt;link&lt;/a&gt; for Maker, and one of the most useful one is &lt;a href=&#34;https://daistats.com/#/&#34;&gt;DaiStats&lt;/a&gt; . Personally, I prefer this page over others as it gives direct stats to each vault and provide other information such as amount of liquidation or auctions.&lt;/p&gt;
&lt;p&gt;However, there is no tracking of historical data for the change corresponding to the vault throughout time, hence unable to determine the trend in overall of Maker. Thus, I created my own tracking website to observe the movement of the platform in term of Borrow.. Source of data is crawled directly from &lt;a href=&#34;https://daistats.com/#/&#34;&gt;DaiStats&lt;/a&gt; on a daily basis.&lt;/p&gt;
&lt;p&gt;Here is the access link :&lt;/p&gt;
&lt;h2 id=&#34;maker-tracking-apphttpsmaker-dai-statusherokuappcom&#34;&gt;&lt;a href=&#34;https://maker-dai-status.herokuapp.com/&#34;&gt;Maker tracking app&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Source code : &lt;a href=&#34;https://github.com/geniusnhu/daistats&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The app uses chart from &lt;a href=&#34;https://plotly.com/&#34;&gt;Plotly&lt;/a&gt; and serves by &lt;a href=&#34;https://streamlit.io/&#34;&gt;streamlit&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improve deep neural network training speed and performance with Optimization</title>
      <link>https://geniusnhu.netlify.com/project/2020-06-14-optimization-and-neural-network-performance/</link>
      <pubDate>Sun, 14 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/project/2020-06-14-optimization-and-neural-network-performance/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://geniusnhu.netlify.app/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/&#34;&gt;Part 1: Initialization, Activation function and Batch Normalization/Gradient Clipping&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 2: Optimizer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Training a deep neural network is an extremely time-consuming task especially with complex problems. Using a faster optimizer for the network is an efficient way to speed up the training speed, rather than simply using the regular Gradient Descent optimizer. Below, I will discuss and show training results/speed of 5 popular Optimizer approaches: &lt;strong&gt;Gradient Descent with momentum and Nesterov Accelerated Gradient, AdaGrad, RMSProp, Adam, and Nadam optimizer&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One of the dangers of using inappropriate optimizers is that the model takes a long time to converge to a global minimum or it will be stuck at a local minimum, resulting in a worse model. Therefore, knowing which Optimizer suits mostly on the problem will save you tons of training hours.
The main purpose of tuning Optimizer is to speed up the training speed but it also helps to improve the model&amp;rsquo;s performance.&lt;/p&gt;
&lt;h2 id=&#34;1-gradient-descent&#34;&gt;1. Gradient Descent&lt;/h2&gt;
&lt;p&gt;Computing the gradient of the associated cost function with regard to each theta and getting the gradient vector pointing uphill, then going in the opposite direction with the vector direction (downhill) using the below equation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$\theta_{next step} = \theta - \eta*  \nabla_{\theta}J(\theta)$&lt;/p&gt;
&lt;p&gt;$\theta$: weight of the model&lt;/p&gt;
&lt;p&gt;$\eta$: learning rate&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, the speed of &lt;strong&gt;Gradient Descent&lt;/strong&gt; optimizer depends solely on the learning rate parameter ($\eta$). With a small learning rate, GD will take small and unchanged steps downward on a gentle surface, and a bit faster steps on a steep surface. Consequently, in a large neural network, it repeats millions of slow steps until it reaches the global minimum (or gets lost in the local minimum). Therefore, the runtime becomes extremely slow.&lt;/p&gt;
&lt;p&gt;Result of training with Fashion MNIST dataset using &lt;strong&gt;SGD&lt;/strong&gt;:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;SGD.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Firgure 1: Loss and accuracy of model using SGD with learning rate 0.001&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The loss declined gradually and will be closer and closer to global minimum after several more epochs.&lt;/p&gt;
&lt;p&gt;There are other versions of Gradient Descent such as &lt;strong&gt;Batch Gradient Descent&lt;/strong&gt; (running on a full dataset), &lt;strong&gt;Mini-batch Gradient Descent&lt;/strong&gt; (running on random subsets of a dataset), &lt;strong&gt;Stochastic Gradient Descent - SGD&lt;/strong&gt; (picking a random instance at each step), and all have pros and cons. &lt;strong&gt;Batch Gradient Descent&lt;/strong&gt; can reach the global minimum at a terribly slow pace. &lt;strong&gt;Mini-batch Gradient Descent&lt;/strong&gt; gets to the global minimum faster than BGD but it is easier to get stuck in the local minimum, and &lt;strong&gt;SGD&lt;/strong&gt; is usually harder to get to the global minimum compared to the other two.&lt;/p&gt;
&lt;h2 id=&#34;2-momentum-optimization&#34;&gt;2. Momentum Optimization&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s imagine, when a ball rolls from the summit, downward the sloping side to the foot of the hill, it will start slowly then increase the speed as the momentum picks up and eventually reaches a fast pace toward the minimum. This is how &lt;strong&gt;Momentum Optimization&lt;/strong&gt; works. This is enabled by adding a momentum vector m and update the theta parameter with this new weight from &lt;em&gt;momentum vector&lt;/em&gt; $m$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$m$ ← $\beta m - \eta * \nabla_{\theta}J(\theta)$&lt;/p&gt;
&lt;p&gt;$\theta_{next step}$ ← $\theta + m$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Gradient descent&lt;/em&gt; does not take into account the previous gradients. By adding the &lt;em&gt;momentum vector&lt;/em&gt;, it updates the weight $m$ after each iteration. The momentum $\beta$ is the parameter controls how fast the terminal velocity is, which is typically set at 0.9 but it should be tuned from 0.5 to 0.99. As a result, &lt;strong&gt;Momentum Optimizer&lt;/strong&gt; converges better and faster than &lt;em&gt;SGD&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement Momentum optimizer in Tensorflow
optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.99)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;SDG_momentum.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 2: Loss and accuracy of models using SGD compared to momentum optimizer&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Momentum&lt;/strong&gt; converges faster and eventually reaches a better result than &lt;em&gt;SGD&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;3-nesterov-accelerated-gradient&#34;&gt;3. Nesterov Accelerated Gradient&lt;/h2&gt;
&lt;p&gt;Another variation of &lt;em&gt;Momentum Optimizer&lt;/em&gt; is &lt;strong&gt;Nesterov Accelerated Gradient - NAG&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$m$ ← $\beta m - \eta * \nabla_{\theta}J(\theta + \beta m)$&lt;/p&gt;
&lt;p&gt;$\theta_{next step}$ ← $\theta + m$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The gradient of the cost function is measured at location $\theta + \beta m$ (instead of $\theta$ in the original momentum optimization). The reason behind this is that momentum optimization has already pointed toward the right direction, so we should use a slightly ahead location (an approximately next position of the $\theta$) to moderately accelerating the speed of convergence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement Nesterov Accelerated Gradient optimizer in Tensorflow
optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.9,nesterov=True
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;NAG_momentum.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 3: Loss and accuracy of models using momentum compared to Nesterov Accelerated Gradient optimizer&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;NAG&lt;/strong&gt; showed only a slightly better result than original &lt;em&gt;Momentum&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;4-adagrad&#34;&gt;4. AdaGrad&lt;/h2&gt;
&lt;p&gt;One of the &lt;em&gt;Adaptive learning rate methods&lt;/em&gt;, in which the algorithm goes faster down the steep slopes than the gentle slopes.
&lt;strong&gt;AdaGrad&lt;/strong&gt; performs well in a simple quadratic problem but not in training a neural network because it tends to slow down a bit too fast and stops before reaching the global minimum. Due to this drawback, I do not usually use &lt;strong&gt;AdaGrad&lt;/strong&gt; for Neural Network but instead apply &lt;strong&gt;RMSProp&lt;/strong&gt;, an alternative of &lt;strong&gt;AdaGrad&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;5-rmsprop---root-mean-square-prop&#34;&gt;5. RMSProp - Root Mean Square Prop&lt;/h2&gt;
&lt;p&gt;This is one of the most frequently used optimizers, which continues the idea of &lt;em&gt;Adagrad&lt;/em&gt; in trying to minimize the vertical movement and updating the model in a horizontal direction toward the global minimum.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Adagrad&lt;/em&gt; sums the gradients from the first iteration and that is why it usually never converges to the global minimum, while &lt;strong&gt;RMSProp&lt;/strong&gt; accumulates the gradients from the previous iterations:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$s$ ← $\beta s - (1-\beta) \nabla_{\theta}J(\theta)^2$&lt;/p&gt;
&lt;p&gt;$\theta_{nextstep}$ ← $\theta + \frac{\eta \nabla_{\theta}J(\theta)}{\sqrt{s + \epsilon}}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$\beta$: decay rate, typically set at 0.9&lt;/p&gt;
&lt;p&gt;$s$: exponential average square of past gradients&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement RMSProp optimizer in Tensorflow
optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;adagrad_rmsprop.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 4: Loss and accuracy of models using RMSProp compared to Adagrad optimizer&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;RMSProp&lt;/strong&gt; converges better than &lt;em&gt;Adagrad&lt;/em&gt; which is lost at a plateau.&lt;/p&gt;
&lt;h2 id=&#34;6-adam&#34;&gt;6. Adam&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Adam optimizer&lt;/strong&gt; is the combination of &lt;em&gt;Momentum&lt;/em&gt; and &lt;em&gt;RMSProp&lt;/em&gt; optimizers. In other words, it takes into account both the exponential decay average of past gradients and the exponential decay average of past squared gradients.&lt;/p&gt;
&lt;p&gt;With these characteristics, &lt;strong&gt;Adam&lt;/strong&gt; is suitable for handling sparse gradients on complex problems with complex data and a large number of features.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$m$ ← $\beta_1 m - (1-\beta_1) \nabla_{\theta}J(\theta)$&lt;/p&gt;
&lt;p&gt;$s$ ← $\beta_2 s - (1-\beta_2) \nabla_{\theta}J(\theta)$&lt;/p&gt;
&lt;p&gt;$\hat{m}$ ← $\frac{m}{1-\beta_1^T}$&lt;/p&gt;
&lt;p&gt;$\hat{s}$ ← $\frac{s}{1-\beta_2^T}$&lt;/p&gt;
&lt;p&gt;$\theta_{nextstep}$ ← $\theta + \frac{\eta \hat{m}}{\sqrt{\hat{s} + \epsilon}}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$\eta$: learning rate&lt;/p&gt;
&lt;p&gt;$s$: exponential average square of past gradients&lt;/p&gt;
&lt;p&gt;$m$: momentum vector&lt;/p&gt;
&lt;p&gt;$\beta_1$: momentum decay, typlically set at 0.9&lt;/p&gt;
&lt;p&gt;$\beta_2$: scaling decay, typlically set at 0.999&lt;/p&gt;
&lt;p&gt;$\epsilon$: smoothing term&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement Adam optimizer in Tensorflow
optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;adagrad_rmsprop_adam.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 5: Loss and accuracy of models using Adagrad, RMSProp, and Adam&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;7-nadam&#34;&gt;7. Nadam&lt;/h2&gt;
&lt;p&gt;Another variation of &lt;em&gt;Adam&lt;/em&gt; is &lt;strong&gt;Nadam&lt;/strong&gt; (using &lt;em&gt;Adam optimizer with Nesterov technique&lt;/em&gt;), resulting in a little faster training time than &lt;em&gt;Adam&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement Nadam optimizer in Tensorflow
optimizer=keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;rmsprop_adam_nadam.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 6: Loss and accuracy of models using RMSProp, Adam and Nadam&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Adagrad, RMSProp, Ada, Nadam, and Adamax&lt;/strong&gt; are &lt;em&gt;Adaptive learning rate algorithms&lt;/em&gt;, which require less tuning on hyperparameters. In case the performance of the model does not meet your expectation, you can try to change back to &lt;strong&gt;Momentum optimizer&lt;/strong&gt; or &lt;strong&gt;Nesterov Accelerated Gradient&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;final-words-&#34;&gt;Final words 🤘&lt;/h1&gt;
&lt;p&gt;In conclusion, most of the time, &lt;em&gt;Adaptive learning rate algorithms&lt;/em&gt; outperform &lt;em&gt;Gradient descent&lt;/em&gt; and its variants in terms of speed, especially in a deep neural network. However, &lt;em&gt;Adaptive learning rate algorithms&lt;/em&gt; do not ensure an absolute convergence to the global minimum.&lt;/p&gt;
&lt;p&gt;If your model is not too complex with a small number of features, and training time is not your priority, using &lt;strong&gt;Momentum&lt;/strong&gt;, &lt;strong&gt;Nesterov Accelerated Gradient&lt;/strong&gt; or &lt;strong&gt;SGD&lt;/strong&gt; is the optimal starting point, then tune the learning rate, activation functions, change Initialization technique to improve the model rather than using &lt;strong&gt;Adaptive learning rate Optimizers&lt;/strong&gt; because the later ones hinder the risk of not converging to the global minimum.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;summary.png&#34; alt=&#34;&#34; style=&#34;width:110%&#34;&gt;
  &lt;figcaption&gt;Figure 7: Summary model performance on training loss of different optimization techniques&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Regular SGD or regular Gradient Descent takes much more time to converge to the global minimum. Adagrad often stops too early before reaching the global minimum so in time it becomes the worse optimizer.&lt;/li&gt;
&lt;li&gt;With the Fashion MNIST dataset, Adam/Nadam eventually performs better than RMSProp and Momentum/Nesterov Accelerated Gradient. This depends on the model, usually, Nadam outperforms Adam but sometimes RMSProp gives the best performance.&lt;/li&gt;
&lt;li&gt;With my experience, I found out that Momentum, RMSProp, and Adam (or Nadam) should be the first try of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Optimizer&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Training speed&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Converge quality&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Note&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Gradient Descent / SGD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Medium for simple model&lt;br&gt;Slow for complex model&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Good&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Risk of converging to local minimum.&lt;br&gt;Can be controled by assigning the correct learning rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Momentum&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast for simple model&lt;br&gt;Medium for complex model&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Good&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for less complex NN with small number of features&lt;br&gt;Need to consider tuning the momentum hyperparameter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nesterov Accelerated&lt;br&gt;Gradient&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast for simple model&lt;br&gt;Medium for complex model&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Good&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for less complex NN with small number of features&lt;br&gt;Need to consider tuning the momentum hyperparameter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AdaGrad&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Usually miss global minimum&lt;br&gt;due to early stopping&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for simple quadratic problem, not NN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RMSProp&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Acceptable&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for complex NN&lt;br&gt;Need to tune Decay rate for better performance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adam&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Acceptable&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for sparse gradients on complex model&lt;br&gt;with a large number of features&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nadam&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Good&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for sparse gradients on complex model&lt;br&gt;with a large number of features&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;This article was originally published in &lt;a href=&#34;https://towardsdatascience.com/full-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&#34;&gt;Towards Data Science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Source code: &lt;a href=&#34;https://github.com/geniusnhu/DNN-Improvement/blob/master/Tuning_Optimizer.ipynb&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speed up training and improve performance in deep neural net</title>
      <link>https://geniusnhu.netlify.com/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/</guid>
      <description>&lt;p&gt;Training a large and deep neural network is a time and computation consuming task and was the main reason for the unpopularity of DNN 20 years ago. As several techniques have been found out to push up the training speed, Deep learning has come back to the light. So which technique to use, how and when to use which? Let&amp;rsquo;s discuss it here!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Performance summary is shown at the end of the post for Classification &amp;amp; Regression examples&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-applying-initialization&#34;&gt;1. Applying Initialization&lt;/h2&gt;
&lt;p&gt;Initialization is one of the first technique used to fasten the training time of Neuron Network (as well as improve performance). Let&amp;rsquo;s briefly explain its importance. In Artificial Neural Network (ANN), there are numerous connections between different neurons. One neuron in the current layer connects to several neurons in the next layer and is attached to various ones in the previous layer. If 2 neurons interact frequently than another pair, their connection (i.e the weights) will be stronger than the other one.&lt;/p&gt;
&lt;p&gt;However, one problem with the ANN is that if the weights aren&amp;rsquo;t specified from the beginning of training, the connection weights can be either too small or too large which makes them too tiny or too massive to use further in the network. In other words, the network will fall into &lt;strong&gt;Vanishing Gradients&lt;/strong&gt; or &lt;strong&gt;Exploding Gradients&lt;/strong&gt; problems.&lt;/p&gt;
&lt;p&gt;So if the weights are set at suitable random values from the beginning of the training, these problem can be avoided. This technique was proposed by &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf&#34;&gt;Glorot and Bengio&lt;/a&gt;, which then significantly lifted these unstable problems. This initialization strategy is called &lt;em&gt;Xavier initialization&lt;/em&gt; or &lt;em&gt;Glorot initialization&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this strategy, the connection weights between neurons are initialized randomly using the Normal distribution with $mean=0$ and variance $\sigma^2 = \frac{2}{fan_{in}+fan_{out}}$ , in which $fan_{in}$ is the number of input neurons and $fan_{out}$ is the number of output neurons.&lt;/p&gt;
&lt;p&gt;There are 2 other popular initialization techniques beside &lt;strong&gt;Glorot&lt;/strong&gt; (used in Keras as default): &lt;strong&gt;He&lt;/strong&gt; and &lt;strong&gt;LeCun&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s examine different initialization techniques&#39; effect on model performance and training time with &lt;code&gt;fashion MNIST&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(10, 10))
for row in range(5):
  for col in range(5):
    index = 5 * row + col
    plt.subplot(5, 5, index + 1)
    plt.imshow(X_train_full[index], cmap=&amp;quot;binary&amp;quot;, interpolation=&amp;quot;nearest&amp;quot;)
    plt.axis(&#39;off&#39;)
    plt.title(y_train_full[index], fontsize=12)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;fashion_set.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Here is the example of Fashion MNIST, in which the predictors are a set of values in the shape of [28,28] representing the image; and the target value is 10 types of cloth and shoes (denoted from 0 to 9)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;First, let&amp;rsquo;s start with the default setting of Keras on a network consisting of 5 hidden layers and 300, 100, 50, 50, 50 neurons each.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.random.set_seed(50)
np.random.seed(50)

model_default = keras.models.Sequential()
model_default.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_layers in (300, 100, 50, 50, 50):
  model_default.add(keras.layers.Dense(n_layers, activation =&#39;relu&#39;))
model_default.add(keras.layers.Dense(10, activation=&#39;softmax&#39;))
    
model_default.compile(loss=&amp;quot;sparse_categorical_crossentropy&amp;quot;,
                      optimizer=keras.optimizers.SGD(lr=1e-3),
                      metrics=[&amp;quot;accuracy&amp;quot;])

start_time = time.time()
history = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)
print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4185 - accuracy: 0.8526 - val_loss: 0.4256 - val_accuracy: 0.8518
--- 99.03307843208313 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The train set reached 85.26% accuracy and Val set reached 85.18% within 99.3 seconds. If &lt;code&gt;activation =&#39;relu&#39;&lt;/code&gt; is not set (i.e. no Activation function in the hidden layers), the accuracy is 85.32% and 84.95% respectively with 104.5 seconds needed to train on.&lt;/p&gt;
&lt;p&gt;Comparing this with weight initialization to all Zeros and all Ones:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Zeros initialization
Epoch 20/20
1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925
--- 69.43926930427551 seconds ---

# Ones initialization
Epoch 20/20
1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925
--- 67.2280786037445 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The performance in both cases is much worse and actually the model stopped improving from 5th epoch.&lt;/p&gt;
&lt;p&gt;Another Initialization that can be considered to use is &lt;code&gt;He Initialization&lt;/code&gt;,  enabling in Keras by adding &lt;code&gt;kernel_initializer=&amp;quot;he_normal&amp;quot;&lt;/code&gt; argument to the hidden layers.&lt;/p&gt;
&lt;p&gt;Result&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3780 - accuracy: 0.8672 - val_loss: 0.3924 - val_accuracy: 0.8637
--- 99.76096153259277 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The accuracy actually improved but the running time was half a second slower than &lt;strong&gt;Glorot Initialization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are also discussions about the performance of &lt;strong&gt;normal distribution&lt;/strong&gt; and &lt;strong&gt;uniform distribution&lt;/strong&gt; in initialization technique, but there is indeed no one shows better performance than the other one. The result of &lt;code&gt;init = keras.initializers.VarianceScaling(scale=2.,mode=&#39;fan_avg&#39;,distribution=&#39;uniform&#39;)&lt;/code&gt; does not improve for this data set (Train set accuracy: 87.05%, Val set: 86.27% and took 100.82 seconds to run)&lt;/p&gt;
&lt;h2 id=&#34;2-get-along-with-the-right-activation-function&#34;&gt;2. Get along with the right Activation function&lt;/h2&gt;
&lt;p&gt;Choosing an unfit activation function is one of the reasons leading to poor model performance. &lt;code&gt;sigmoid&lt;/code&gt; might be a good choice but I prefer to use &lt;strong&gt;SELU, ReLU, or its variants&lt;/strong&gt; instead.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s talk about &lt;strong&gt;ReLU&lt;/strong&gt; first. Simply saying, if the value is larger than 0, the function returns the value itself; else it returns 0. This activation is fast to compute but in return there will be a case that it stops outputting anything other than 0 (i.e neurons were died). This issue usually happens in case of a large learning rate.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;relu_and_lrelu.png&#34; alt=&#34;&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;ReLU, Leaky ReLU and SELU&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Some of the solutions for this problem is to use alternative versions of ReLU: &lt;strong&gt;LeakyReLU, Randomized LeakyReLU or Scaled ReLU (SELU)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;LeakyReLU&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if x&amp;gt;0:
  return x
else:
  return ax
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in which a is $\alpha$, the slope of the $x$ given $x&amp;lt;0$. $\alpha$ is usually set at 0.01, serving as a small leak (that&amp;rsquo;s why this technique is called LeakyReLU). Using $\alpha$ helps to stop the dying problem (i.e. slope=0).&lt;/p&gt;
&lt;p&gt;In case of &lt;strong&gt;Randomized LeakyReLU&lt;/strong&gt;, $\alpha$ is selected randomly given a range. This method can reduce the Overfitting issue but requires more running time due to extra computation.&lt;/p&gt;
&lt;p&gt;One of the outperformed activation function for DNN is &lt;strong&gt;Scaled ReLU (SELU)&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if x&amp;gt;0:
  return Lambda*x
else:
  return Lambda*(alpha*exp(x)-alpha)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this function, each layer outputs&#39; mean is 0 and standard deviation is 1. Note when using this activation function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; It must be used with &lt;code&gt;kernel_initializer=&amp;quot;lecun_normal&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The input features must be standardized&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The NN&amp;rsquo;s architecture must be sequential&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s try different Activation functions on the &lt;code&gt;fashion MNIST&lt;/code&gt; dataset.&lt;/p&gt;
&lt;p&gt;Result of &lt;strong&gt;LeakyReLU&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3791 - accuracy: 0.8670 - val_loss: 0.3910 - val_accuracy: 0.8615
--- 101.87710905075073 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result of &lt;strong&gt;Randomized LeakyReLU&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 6s 3ms/step - loss: 0.3779 - accuracy: 0.8667 - val_loss: 0.3918 - val_accuracy: 0.8630
--- 113.58738899230957 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result of &lt;strong&gt;SELU&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 19/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3526 - accuracy: 0.8763 - val_loss: 0.3755 - val_accuracy: 0.8647
--- 106.25733232498169 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;SELU&lt;/strong&gt; seems to achieve slightly better performance over ReLU and its variants but the speed is slower (as expected).&lt;/p&gt;
&lt;span class=&#34;markup-quote&#34;&gt;&lt;strong&gt;If the NN performs relatively well at a low learning rate, ReLU is an optimal choice given the fastest training time. In case of the deep NN, SELU is an excellent try.&lt;/strong&gt;&lt;/span&gt;
&lt;p&gt;Detailed explanation about these activations can be found in here: &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf&#34;&gt;ReLU&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1505.00853&#34;&gt;LeakyReLU, Randomized LeakyReLU&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1706.02515&#34;&gt;SELU&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-batch-normalization&#34;&gt;3. Batch Normalization&lt;/h2&gt;
&lt;p&gt;To ensure Vanishing/Exploding Gradients problems do not happen again during training (as Initialization and Activation function can help reduce these issues at the beginning of the training), &lt;strong&gt;Batch Normalization&lt;/strong&gt; is implemented.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Batch Normalization&lt;/strong&gt; zeros centers and normalizes each input, then scales and shifts the result using 1 parameter vector for scaling and 1 for shifting. This technique evaluates the $mean$ and $standard deviation$ of the input over the current mini-batch and repeats this calculation across all mini-batches of the training set. $\mu$ and $\sigma$ are estimated during training but only used after training.&lt;/p&gt;
&lt;p&gt;The vector of input means $\mu$ and vector of input standard devition $\sigma$ will become non-trainable parameters (i.e. untouchable by backpropagation) and be used to compute the moving averages at the end of the training. Subsequently, these final parameters will be used to normalize new data to make prediction.&lt;/p&gt;
&lt;p&gt;If using &lt;strong&gt;Batch Normalization&lt;/strong&gt;, the input data will not need to be standardized prior training.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.random.set_seed(50)
np.random.seed(50)

model_default = keras.models.Sequential()
model_default.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_layers in (300, 100, 50, 50, 50):
  model_default.add(keras.layers.BatchNormalization())
  model_default.add(keras.layers.Dense(n_layers, activation =&#39;relu&#39;, kernel_initializer=&amp;quot;he_normal&amp;quot;))
model_default.add(keras.layers.Dense(10, activation=&#39;softmax&#39;))
    
model_default.compile(loss=&amp;quot;sparse_categorical_crossentropy&amp;quot;,
                      optimizer=keras.optimizers.SGD(lr=1e-3),
                      metrics=[&amp;quot;accuracy&amp;quot;])

start_time = time.time()
history = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)
print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 8s 5ms/step - loss: 0.3799 - accuracy: 0.8645 - val_loss: 0.3571 - val_accuracy: 0.8685
--- 167.6186249256134 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, training is slower in &lt;strong&gt;Batch Normalization&lt;/strong&gt; given more computations during training but in contrast, in &lt;strong&gt;Batch Normalization&lt;/strong&gt;, the model convergences faster so fewer epoches are needed to reach the same performance.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Batch Normalization is strictly implemented in Recurrent NN
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;4-gradient-clipping&#34;&gt;4. Gradient Clipping&lt;/h2&gt;
&lt;p&gt;As &lt;strong&gt;Batch Normalization&lt;/strong&gt; is recommended not to use with Recurrent NN, &lt;strong&gt;Gradient Clipping&lt;/strong&gt; is the alternative choice for RNN.&lt;/p&gt;
&lt;p&gt;Details about &lt;a href=&#34;https://arxiv.org/abs/1211.5063&#34;&gt;Gradient Clipping&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;summary-of-the-result-of-classification-task-with-fashion-mnist-dataset&#34;&gt;Summary of the result of Classification task with Fashion MNIST dataset&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Initialization&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Activation fuction&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Train set accuracy&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Val set accuracy&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Running time (seconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Glorot - Zeros/Ones&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.08%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.25%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;69.43/67.22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Glorot&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;None&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;85.32%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;84.95%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;104.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Glorot - Normal Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;85.26%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;85.18%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;99.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.72%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.37%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;99.76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Uniform  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;87.05%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.27%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;100.82&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Leaky ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.7%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.15%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;101.87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Randomized LeakyReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.67%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.3%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;113.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LeCun&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;SELU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;87.63%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.47%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;106.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Batch normalization He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.45%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.85%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;167.618&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;summary-of-the-result-of-regression-task-with-california-housing-dataset&#34;&gt;Summary of the result of Regression task with California housing dataset&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Initialization&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Activation fuction&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Train set MSE&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Val set MSE&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Running time (seconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Glorot&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;None&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3985&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3899&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Glorot - Normal Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3779&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3819&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3517&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.35&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Leaky ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3517&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.35&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.48&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Randomized LeakyReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3517&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.35&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LeCun&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;SELU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3423&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.326&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Batch normalization He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.4365&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5728&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.64&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;MSE of Train and Validation set&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;MSE.png&#34; alt=&#34;&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;regression_BN.png&#34; alt=&#34;&#34; style=&#34;width:35%&#34;&gt;
  &lt;figcaption&gt;Fashion MNIST consists of image on 10 types of fashion&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    These performances are subject to change depending on the dataset and NN&amp;rsquo;s architecture
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;final-thoughts-on-this-part-&#34;&gt;Final thoughts on this part 🔆&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Glorot Initialization is the good starting point for most of the cases. He Initialization technique sometimes performs better than Glorot (slower in the above Classification example while faster in Regression example).&lt;/li&gt;
&lt;li&gt;ReLU or Leaky ReLU are great choices if running time is the priority.&lt;/li&gt;
&lt;li&gt;ReLU should be avoided if high Learning rate is used.&lt;/li&gt;
&lt;li&gt;SELU is the good choice for complex dataset and deep neural network but might be traded off by running time. However, if the NN&amp;rsquo;s architecture does not allow &lt;em&gt;self-normalization&lt;/em&gt;, use ELU instead of SELU.&lt;/li&gt;
&lt;li&gt;SELU and Batch Normalization cannot be applied in RNN. Gradient Clipping is the alternative strategy for Batch Normalization in RNN.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;5-transfer-learning&#34;&gt;5. Transfer Learning&lt;/h2&gt;
&lt;p&gt;Another important technique too improve the performance of DNN is &lt;strong&gt;Transfer Learning&lt;/strong&gt;, using pretrained layers to train similar new task. There is much to say about this technique and it will be covered in another post.&lt;/p&gt;
&lt;p&gt;Source code can be accessed &lt;a href=&#34;https://github.com/geniusnhu/DNN-Improvement/blob/master/Improve_DNN_performance.ipynb&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Reference:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Glorot, X., &amp;amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. PMLR&lt;/li&gt;
&lt;li&gt;He, K., Zhang, X., Ren,S., &amp;amp; Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)&lt;/li&gt;
&lt;li&gt;Geron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O&amp;rsquo;Reilly Media, Inc.,&lt;/li&gt;
&lt;li&gt;Xu, B., Wang, N., Chen, T., &amp;amp; Li, M. (2015). Empirical Evaluation of Rectified Activations in Convolutional Network. Retrieved from &lt;a href=&#34;https://arxiv.org/abs/1505.00853&#34;&gt;https://arxiv.org/abs/1505.00853&lt;/a&gt; on May 5, 2020.&lt;/li&gt;
&lt;li&gt;Klambauer, G., Unterthiner, T., Mayr, A., &amp;amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. Advances in Neural Information Processing Systems 30 (NIPS 2017)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Complete guide for Time series Visualization</title>
      <link>https://geniusnhu.netlify.com/project/2020-03-30-complete-guide-for-time-series-visualization/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/project/2020-03-30-complete-guide-for-time-series-visualization/</guid>
      <description>&lt;p&gt;When visualizing time series data, there are several things to be set in mind:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Although we use the same plotting technique as for non-time-series one, but it will not work with the same implication. &lt;strong&gt;Reshaped data&lt;/strong&gt; (aka lag, difference extraction, downsampling, upsampling, etc) is essential.&lt;/li&gt;
&lt;li&gt;It is informative to confirm the &lt;strong&gt;trend, seasonality, cyclic pattern&lt;/strong&gt; as well as &lt;strong&gt;correlation among the series itself (Self-correlation/Autocorrelation) and the series with other series&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Watch out for the &lt;strong&gt;Spurious correlation&lt;/strong&gt;: high correlation is always a trap rather than a prize for data scientist. Many remarks this as &lt;strong&gt;correlation-causation trap&lt;/strong&gt;
. If you observe a &lt;strong&gt;trending and/or seasonal time-series&lt;/strong&gt;, be careful with the correlation. Check if the data is a &lt;strong&gt;cummulative sum&lt;/strong&gt; or not. If it is, spurious correlation is more apt to appear.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The below example with plots will give more details on this.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-time-series-patterns&#34;&gt;1. Time series patterns&lt;/h2&gt;
&lt;p&gt;Time series can be describe as the combination of 3 terms: &lt;strong&gt;Trend&lt;/strong&gt;, &lt;strong&gt;Seasonality&lt;/strong&gt; and &lt;strong&gt;Cyclic&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trend&lt;/strong&gt; is the changeing direction of the series. &lt;strong&gt;Seasonality&lt;/strong&gt; occurs when there is a seasonal factor is seen in the series. &lt;strong&gt;Cyclic&lt;/strong&gt; is similar with Seasonality in term of the repeating cycle of a similar pattern but differs in term of the length nd frequency of the pattern.&lt;/p&gt;
&lt;p&gt;The below graph was plot simply with &lt;code&gt;plot&lt;/code&gt; function of &lt;code&gt;matplotlib&lt;/code&gt;, one of the most common way to observe the series&#39; trend, seasonality or cyclic.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;total_sales.png&#34; alt=&#34;&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Looking at the example figure, there is &lt;strong&gt;no trend&lt;/strong&gt; but there is a clear &lt;strong&gt;annual seasonlity&lt;/strong&gt; occured in December. &lt;strong&gt;No cyclic&lt;/strong&gt; as there is no pattern with frequency longer than 1 year.&lt;/p&gt;
&lt;h2 id=&#34;2-confirming-seasonality&#34;&gt;2. Confirming seasonality&lt;/h2&gt;
&lt;p&gt;There are several ways to confirm the seasonlity. Below, I list down vizualization approaches (which is prefered by non-technical people).&lt;/p&gt;
&lt;h3 id=&#34;seasonal-plot&#34;&gt;Seasonal plot:&lt;/h3&gt;
&lt;p&gt;This gives a better prove to spot seasonality, spike and drop. As seen in the below chart, there is a large jump in December, followed by a drop in January.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;seasonal_plot.png&#34; alt=&#34;&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Code can be found below (I am using the new Cyberpunk of Matplotlib, can be found &lt;a href=&#34;https://github.com/dhaitz/mplcyberpunk&#34;&gt;here&lt;/a&gt; with heptic neon color)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;colors = [&#39;#08F7FE&#39;,  # teal/cyan
          &#39;#FE53BB&#39;,  # pink
          &#39;#F5D300&#39;] # matrix green
plt.figure(figsize=(10,6))
w =data.groupby([&#39;Year&#39;,&#39;Month&#39;])[&#39;Weekly_Sales&#39;].sum().reset_index()
sns.lineplot(&amp;quot;Month&amp;quot;, &amp;quot;Weekly_Sales&amp;quot;, data=w, hue=&#39;Year&#39;, palette=colors,marker=&#39;o&#39;, legend=False)
mplcyberpunk.make_lines_glow()
plt.title(&#39;Seasonal plot: Total sales of Walmart 45 stores in 3 years&#39;,fontsize=20 )
plt.legend(title=&#39;Year&#39;, loc=&#39;upper left&#39;, labels=[&#39;2010&#39;, &#39;2011&#39;,&#39;2012&#39;],fontsize=&#39;x-large&#39;, title_fontsize=&#39;20&#39;)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;seasonal-subseries-plot&#34;&gt;Seasonal subseries plot&lt;/h3&gt;
&lt;p&gt;Next is an another way of showing the &lt;strong&gt;distribution&lt;/strong&gt; of time-series data in each month. Insteading of using histogram (which I considered difficult to understand the insight in time series), I generated &lt;em&gt;box plot&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Of note, the main purpose of this plot is to show the &lt;strong&gt;values changing from one month to another&lt;/strong&gt; as well as &lt;strong&gt;how the value distributed within each month&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;sub_seasonal.png&#34; alt=&#34;&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;Box plot&lt;/em&gt; is strongly recommended in case of &lt;strong&gt;confirming the mean, median of the seasonal period comparing to other periods&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;3-correlation&#34;&gt;3. Correlation&lt;/h2&gt;
&lt;p&gt;Alike other type of data, &lt;strong&gt;Scatter plot&lt;/strong&gt; stands as the first choice for &lt;strong&gt;identifying the correlation between different time series&lt;/strong&gt;. This is especially the case if one series can be used to explain another series. Below is the correlation of sales and its lag 1.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;scatter.png&#34; alt=&#34;&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_lag = data.copy()
data_lag[&#39;lag_1&#39;] = data[&#39;Weekly_Sales&#39;].shift(1) # Create lag 1 feature
data_lag.dropna(inplace=True) 

plt.style.use(&amp;quot;cyberpunk&amp;quot;)
plt.figure(figsize=(10,6))
sns.scatterplot(np.log(data_lag.Weekly_Sales), np.log(data_lag.lag_1), data =data_lag)
mplcyberpunk.make_lines_glow()
plt.title(&#39;Weekly sales vs its 1st lag&#39;,fontsize=20 );
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is apparant that the correlation between the original data and its 1&lt;sup&gt;st&lt;/sup&gt; lag is not too strong and there seems some outlier in the top left of the graph.&lt;/p&gt;
&lt;p&gt;It is also interesting to identify if this &lt;em&gt;correlation actually exists and can we use lag 1 to predict the original series&lt;/em&gt;. &lt;strong&gt;The correlation between the original difference and the 1&lt;sup&gt;st&lt;/sup&gt; lag difference&lt;/strong&gt; will give proof for hypothesis.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;scatter_diff.png&#34; alt=&#34;&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;The correlation between the original difference and the 1&lt;sup&gt;st&lt;/sup&gt; lag difference disappeared, indicating that lag1 does not appear to predict sales.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_lag[&#39;lag_1_diff&#39;] = data_lag[&#39;lag_1&#39;].diff() # Create lag 1 difference feature
data_lag[&#39;diff&#39;] = data_lag[&#39;Weekly_Sales&#39;].diff() # Create difference feature
data_lag.dropna(inplace=True) 

plt.style.use(&amp;quot;cyberpunk&amp;quot;)
plt.figure(figsize=(10,6))
sns.scatterplot(data_lag[&#39;diff&#39;], data_lag.lag_1_diff, data =data_lag)
mplcyberpunk.make_lines_glow()
plt.title(&#39;The correlation between original series difference with its 1st lag difference&#39;,fontsize=15);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;moving-average-and-original-series-plot&#34;&gt;Moving average and Original series plot&lt;/h3&gt;
&lt;figure&gt;
  &lt;img src=&#34;moving_average.png&#34; alt=&#34;&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):

    rolling_mean = series.rolling(window=window).mean()
    
    plt.figure(figsize=(15,5))
    plt.title(&amp;quot;Moving average\n window size = {}&amp;quot;.format(window))
    plt.plot(rolling_mean, &amp;quot;g&amp;quot;, label=&amp;quot;Rolling mean trend&amp;quot;)

    # Plot confidence intervals for smoothed values
    if plot_intervals:
        mae = mean_absolute_error(series[window:], rolling_mean[window:])
        deviation = np.std(series[window:] - rolling_mean[window:])
        lower_bond = rolling_mean - (mae + scale * deviation)
        upper_bond = rolling_mean + (mae + scale * deviation)
        plt.plot(upper_bond, &amp;quot;r--&amp;quot;, label=&amp;quot;Upper Bond / Lower Bond&amp;quot;)
        plt.plot(lower_bond, &amp;quot;r--&amp;quot;)
        
        # Having the intervals, find abnormal values
        if plot_anomalies:
            anomalies = pd.DataFrame(index=series.index, columns=series.columns)
            anomalies[series&amp;lt;lower_bond] = series[series&amp;lt;lower_bond]
            anomalies[series&amp;gt;upper_bond] = series[series&amp;gt;upper_bond]
            plt.plot(anomalies, &amp;quot;ro&amp;quot;, markersize=10)
        
    plt.plot(series[window:], label=&amp;quot;Actual values&amp;quot;)
    plt.legend(loc=&amp;quot;upper left&amp;quot;)
    plt.grid(True)
    
plotMovingAverage(series, window, plot_intervals=True, scale=1.96,
                  plot_anomalies=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;acf--pacf-plots-autocorrelation--partial-autocorrelation-plots&#34;&gt;ACF / PACF plots (Autocorrelation / Partial Autocorrelation plots)&lt;/h3&gt;
&lt;p&gt;First, talking about &lt;strong&gt;Autocorrelaltion&lt;/strong&gt;, by definition,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Autocorrelation implies how data points at different points in time are linearly related to one another.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;em&gt;blue area&lt;/em&gt; represents the &lt;em&gt;distance that is not significant than 0&lt;/em&gt; or the &lt;strong&gt;critical region&lt;/strong&gt;, in orther word, the correlation points that &lt;strong&gt;fall beyond this area are significantly different than 0&lt;/strong&gt;, and these the points needed our attention. This region is same for both ACF and PACF, which denoted as $ \pm 1.96\sqrt{n}$&lt;/p&gt;
&lt;p&gt;The details of ACF and PACF plot implication and how to use them for further forecast can be found &lt;a href=&#34;https://geniusnhu.netlify.com/publication/arima-autoregressive-intergreated-moving-average/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;ACF_PACF.png&#34; alt=&#34;ACF / PACF plots&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;ACF shows a significant negativve correlation at lag 3 and no positive correlation, indicating that the series has no correlation with its previous values. &lt;br /&gt; PACF reveals that lag 3, lag 6, lag 9, lag 18 and probably lag 19 are important to the original series&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ACF and PACF for time series data
series=train.dropna()
fig, ax = plt.subplots(2,1, figsize=(10,8))
fig = sm.graphics.tsa.plot_acf(series, lags=None, ax=ax[0])
fig = sm.graphics.tsa.plot_pacf(series, lags=None, ax=ax[1])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;actual-vs-predicted-values-plot&#34;&gt;Actual vs Predicted values plot&lt;/h3&gt;
&lt;figure&gt;
  &lt;img src=&#34;actual_predicted.png&#34; alt=&#34;Actual vs Predicted values plot&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;Actual vs Predicted values plot&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plotModelResults(model, X_train, X_test, y_train, y_test, plot_intervals=False, plot_anomalies=False):

    prediction = model.predict(X_test)
    
    plt.figure(figsize=(12, 8))
    plt.plot(prediction, &amp;quot;g&amp;quot;, label=&amp;quot;prediction&amp;quot;, linewidth=2.0, color=&amp;quot;blue&amp;quot;)
    plt.plot(y_test.values, label=&amp;quot;actual&amp;quot;, linewidth=2.0, color=&amp;quot;olive&amp;quot;)
    
    if plot_intervals:
        cv = cross_val_score(model, X_train, y_train, 
                                    cv=tscv, 
                                    scoring=&amp;quot;neg_mean_absolute_error&amp;quot;)
        mae = cv.mean() * (-1)
        deviation = cv.std()
        
        scale = 1
        lower = prediction - (mae + scale * deviation)
        upper = prediction + (mae + scale * deviation)
        
        plt.plot(lower, &amp;quot;r--&amp;quot;, label=&amp;quot;upper bond / lower bond&amp;quot;, alpha=0.5)
        plt.plot(upper, &amp;quot;r--&amp;quot;, alpha=0.5)
        
        if plot_anomalies:
            anomalies = np.array([np.NaN]*len(y_test))
            anomalies[y_test&amp;lt;lower] = y_test[y_test&amp;lt;lower]
            anomalies[y_test&amp;gt;upper] = y_test[y_test&amp;gt;upper]
            plt.plot(anomalies, &amp;quot;o&amp;quot;, markersize=10, label = &amp;quot;Anomalies&amp;quot;)
    
    error = mean_absolute_percentage_error(y_test,prediction)
    plt.title(&amp;quot;Mean absolute percentage error {0:.2f}%&amp;quot;.format(error))
    plt.legend(loc=&amp;quot;best&amp;quot;)
    plt.tight_layout()
    plt.grid(True);

plotModelResults(linear, X_train, X_test, y_train, y_test,
                 plot_intervals=True, plot_anomalies=True)    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;To be updated&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The beauty of Transformer in bringing more applications to life</title>
      <link>https://geniusnhu.netlify.com/2020/02/12/the-beauty-of-transformer-in-bringing-more-applications-to-life/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/2020/02/12/the-beauty-of-transformer-in-bringing-more-applications-to-life/</guid>
      <description>&lt;p&gt;My 2020 started awesomely with a Machine Learning Forum @Google Japan and followed with 2 Meetups about Natural Language Processing application. NLP has been the focus of the Machine Learning society for the last decade and it is reaching to its ultimate point with several outbreak of innovations and application.&lt;/p&gt;
&lt;p&gt;At this moment, 2020, NLP is heading towards &lt;strong&gt;speed and big data&lt;/strong&gt;, which means that the increasing of the speed and size of data is the key objective of future NLP innovations. At the moment, I am impressed with &lt;strong&gt;BERT - Bidirectional Encoder Representations from Transformers&lt;/strong&gt;, a powerful state-of-the-art NPL model introduced by Google in 2018; and *&lt;strong&gt;Google Meena&lt;/strong&gt;, a lift toward dealing with big-NLP database using the Transformer base introducted on Jan 28, 2020.&lt;/p&gt;
&lt;h3 id=&#34;1why-are-bert-and-transformer-being-called-the-revolution-of-the-nlp-world&#34;&gt;1.	Why are BERT and Transformer being called the revolution of the NLP world?&lt;/h3&gt;
&lt;p&gt;To understand &lt;strong&gt;Transformer model&lt;/strong&gt; inclduing BERT, we need to take a look of the progress from &lt;strong&gt;Seq2Seq (sequence to sequence)&lt;/strong&gt; and its evolution to attention and to BERT.&lt;/p&gt;
&lt;h4 id=&#34;11-seq2seq-model&#34;&gt;1.1 Seq2Seq model:&lt;/h4&gt;
&lt;p&gt;In NLP, the end that a machine is expected to understand is the meaning of the sentence, not only word by word. &lt;strong&gt;Seq2Seq&lt;/strong&gt; is a technique to train the machine in which it takes a sequence of an item and generates another sequence as output.&lt;/p&gt;
&lt;p&gt;Within the model, there contains &lt;strong&gt;Encoder&lt;/strong&gt; and &lt;strong&gt;Decoder&lt;/strong&gt;. The &lt;strong&gt;Encoder&lt;/strong&gt; receives the original text and convert the text into a &lt;strong&gt;Context vector&lt;/strong&gt; that the Machine can read. Then, the &lt;strong&gt;Decoder&lt;/strong&gt; does the job of generating a new sequence of items based on the &lt;strong&gt;Context vector&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In Seq2Seq, a sentence does not need to go through both Encoder and Decoder, it can stop at Encoder. Some example of Encoder only is the suggested word &lt;em&gt;&amp;ldquo;message&amp;rdquo;&lt;/em&gt; after you type &lt;em&gt;&amp;ldquo;Thank you for your&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt; or &lt;strong&gt;Context vector&lt;/strong&gt; is a vector of floats representing the input sequence. &lt;strong&gt;&amp;ldquo;Word Embedding&amp;rdquo;&lt;/strong&gt; is the algorithm used to transform text into vector, and the size of vector is usually 265, 512 or 1024 dimensions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Seq2Seq_visual.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;12-attention&#34;&gt;1.2 Attention&lt;/h4&gt;
&lt;p&gt;One of the disadvantages of Context in Seq2Seq is the &lt;strong&gt;dealing with long sentences and handling the sequence of output&lt;/strong&gt;. Context is generated by the Word embedding algorithm and the longer the sentence or the paragraph, the bigger the size of vector and the more memory consuming.&lt;/p&gt;
&lt;p&gt;Moreover, the context in Seq2Seq &lt;strong&gt;was not built to figure out the similarity between words&lt;/strong&gt; because it does not focus on the &lt;strong&gt;relevancy of the words in the sentence&lt;/strong&gt;. This leads to the issue that a sentence in English cannot be translated correctly to Japanese which has the reserve order in sentence structure.&lt;/p&gt;
&lt;p&gt;The concept of Attention was introduced in &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Bahdanau et al., 2014&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1508.04025&#34;&gt;Luong et al., 2015&lt;/a&gt; in which it takes into account the relevant parts in the sentence.&lt;/p&gt;
&lt;p&gt;Instead of passing the last hidden state to the &lt;strong&gt;Decoder&lt;/strong&gt;, the Attention model &lt;strong&gt;passes all the hidden states to the Decoder&lt;/strong&gt; with the summary process as below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Give each hidden state a score&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Softmax function&lt;/strong&gt; to multiply each hidden state. This brings about high Hidden state scores and low hidden state scores or in other word, it generates the probability of each hidden state associating with the input word.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Decoder&lt;/strong&gt; will sum up all the weighted softmax Hidden state vectors into a context vector and concatenate it with its original hidden state vector.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Attention_Visual.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The advantage of this model is the capability to &lt;strong&gt;choose the decoding word based on the probability of that word in associating with the original input without losing the sequential characteristics of the sentence&lt;/strong&gt;. This has high effectiveness in dealing with translating of common words such as &amp;ldquo;the&amp;rdquo;, &amp;ldquo;his&amp;rdquo;, &amp;ldquo;of&amp;rdquo;, etc. and the sequence of different languages.&lt;/p&gt;
&lt;h4 id=&#34;13-transformer&#34;&gt;1.3 Transformer:&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; is built on the foundation of &lt;strong&gt;Attention&lt;/strong&gt; model. Therefore, &lt;strong&gt;Transformer&lt;/strong&gt; can deal with the relevancy of the sentence rather than converting from word to word in &lt;strong&gt;Seq2Seq&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The biggest difference of &lt;strong&gt;Transformer&lt;/strong&gt; vs &lt;strong&gt;Seq2Seq&lt;/strong&gt; is that instead of generating 1 vector from &lt;strong&gt;Encoder&lt;/strong&gt;, &lt;strong&gt;Transformer&lt;/strong&gt; model uses 3 vectors in order to decide which other parts of the sentence are important (or unimportant) to that word.&lt;/p&gt;
&lt;p&gt;The below table show more details of the calculation.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Word&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Q vector&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;K vector&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;V vector&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Score&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Softmax&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Sum&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;First word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;K1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;V1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q1xK1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;S1=Q1xK1/8xV1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;love&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;K2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;V2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q1xK2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;S2=Q1xK2/8xV2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Z1 = S1+S2+S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;K3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;V3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q1xK3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;S3=Q1xK3/8xV3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Second word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;K1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;V1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q2xK1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;S1=Q2xK1/8xV1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;love&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;K2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;V2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q2xK2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;S2=Q2xK2/8xV2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Z2 = S1+S2+S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;K3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;V3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q2xK3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;S3=Q2xK3/8xV3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Third word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;K1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;V1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q3xK1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;S1=Q3xK1/8xV1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;love&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;K2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;V2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q3xK2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;S2=Q3xK2/8xV2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Z3 = S1+S2+S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;K3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;V3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Q3xK3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;S3=Q3xK3/8xV3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;One example of this model application is the suggestion of relevant words when typing sentence. Gmail can suggest &lt;strong&gt;&amp;ldquo;message&amp;rdquo;, &amp;ldquo;reply&amp;rdquo;, &amp;ldquo;call&amp;rdquo;&lt;/strong&gt; at the same time based on the typed sentence &lt;strong&gt;&amp;ldquo;Thank you for your&lt;/strong&gt;&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a brief introduction on the transition from Seq2Seq to Transformer and how the &lt;strong&gt;Transformer model&lt;/strong&gt; outstands &lt;strong&gt;Seq2Seq&lt;/strong&gt; at the moment.&lt;/p&gt;
&lt;h3 id=&#34;2-business-applications-of-nlp&#34;&gt;2. Business Applications of NLP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chatbot&lt;/strong&gt;: This is obviously the forefront application of NLP, which can be seen across all industries and companies. Given its popularity, there are several tools to support building a chatbot such as Google DialogFlow, Microsoft LUIS. These tools can be customized based on the user&amp;rsquo;s needs; however, they can only deal with simple requests.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Machine translation&lt;/strong&gt;: such as Google translate or pocket translator device.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Search engine&lt;/strong&gt;: 5 years ago, when you searched something on search engine, whether you type key words &amp;ldquo;to Tokyo&amp;rdquo; or the whole sentences &amp;ldquo;How to go to Tokyo&amp;rdquo;, the machine would generate quite similar results. However, with the evolution of BERT and Transformer, searching the whole sentence will throw you to a better search result.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monitoring of brand and product - Sentiment analysis:&lt;/strong&gt; This is the field that I used to analyze during my first job. I used sentiment analysis on big scale online platforms including online forums, social network, brand website and e-commerce sites to understand the reaction of consumers toward a campaign or a brand in order to react promptly toward negative trend related to the brand.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Place to display an advertisement&lt;/strong&gt;: display ads based on context or categorization and make sure that the article is appropriate at the placing place. Honestly saying, I have not seen much of this application around.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Remarketing&lt;/strong&gt;: an online advertisement based on the browsing history of a user to target them with similar advertising product to drive them back the previous interest. This personalized application is a very effective tool in today online market in which thousands of sellers trying to attract each of their customers. Youtube, Facebook or Google are the biggest applicators.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Medical assistant&lt;/strong&gt;: although called &amp;ldquo;assistant&amp;rdquo;, the major task of this service is to transcript the discussion between doctors and patients.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text generation&lt;/strong&gt;: this is one of the applications of Decoding in NLP, in which the machine will generate a complete article from what it was learnt or summarize a paragraph. As you may know, there are many contradictions about this application, especially the emergence of fake news in recent years. With the completion of this technology, whether the fake news issue continues its expansion or is stopped is still a big question.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Information extraction&lt;/strong&gt;: extract dynamic required information that is sleeping in the database system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resume reviews&lt;/strong&gt;: use NLP to scan the applicants&#39; resume to figure out potential candidate for the interview. This application sticks with Amazon big scandal. Amazon used to use this to scan the resume which led to the inequality between male and female with the result preferred male than female. This is due to 0the bias toward male in the training set of the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Voice command&lt;/strong&gt;: an emerging technology in recent years with the appearance of smart device such as Siri in Iphone, Alexa of Amazone, Google home or Cortana of Microsoft.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beside the major technological trends mentioned above, the application trend is heading toward diversity in languages and translation efficacy. Moreover, not only applying NLP alone, there are more applications combining NLP and Voice recognition or Computer Vision.&lt;/p&gt;
&lt;p&gt;NLP is a powerful Machine Learning area and its application is supporting human&amp;rsquo;s life even more than we can expect. Therefore, NLP is one of the most used Machine Learning fields by Data Scienctist.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ARIMA Autoregressive Integrated Moving Average model family</title>
      <link>https://geniusnhu.netlify.com/project/2020-01-26-arima-autoregressive-intergreated-moving-average/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/project/2020-01-26-arima-autoregressive-intergreated-moving-average/</guid>
      <description>&lt;h2 id=&#34;1concept-introduction&#34;&gt;1.	Concept Introduction&lt;/h2&gt;
&lt;p&gt;Auto Regressive Integrated Moving Average: &amp;lsquo;explains&amp;rsquo; a given time series based on its own past values. ARIMA is expressed as $ARIMA(p,d,q)$&lt;/p&gt;
&lt;p&gt;The evolution of ARIMA started with the model ARMA or Auto Regressive Moving Average. However, this model does not include the &lt;strong&gt;Integrated term&lt;/strong&gt;, or differencing order (I&amp;rsquo;ll talk about this later on) so this model can only be used with &lt;strong&gt;Stationary data&lt;/strong&gt;. For &lt;strong&gt;non-stationary data&lt;/strong&gt;, we will use ARIMA.&lt;/p&gt;
&lt;p&gt;There are 3 parts in the ARIMA model: &lt;strong&gt;Auto Regressive (AR)&lt;/strong&gt; $p$, &lt;strong&gt;Integrated (I)&lt;/strong&gt; $d$, &lt;strong&gt;Moving Average (MA)&lt;/strong&gt; $q$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrated&lt;/strong&gt; (Or stationary): A time series which needs to be differenced to become stationary is the &lt;em&gt;integrated&lt;/em&gt; version of stationary series. One of the characteristics of Stationary is that the effect of an observation dissipated as time goes on. Therefore, the best long-term predictions for data that has stationary is the historical mean of the series.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Auto Regressive&lt;/strong&gt;: is simply defined a the linear or non-linear model between current value of the series with its previous values (so called &lt;strong&gt;lags&lt;/strong&gt;), and there are unlimited number of lags in the model. The basic assumption of this model is that the current series value depends on its previous values. This is the long memory model because the effect slowly dissipates across time. p is preferred as the maximum lag of the data series.
The AR can be denoted as
$Y_{t}=\omega_{0}+\alpha_{1}Y_{t-1}+\alpha_{2}Y_{t-2}+&amp;hellip;+\xi$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Moving Average&lt;/strong&gt;: deal with &amp;lsquo;shock&amp;rsquo; or error in the model, or how abnormal your current value is compared to the previous values (has some residual effect).
The MA is denoted as
$Y_{t}=m_1\xi_{t-1}+\xi_t$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$p$, $d$, and $q$ are non-negative integers;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p$: The number of &lt;strong&gt;Autoregressive terms&lt;/strong&gt;. &lt;strong&gt;Autoregressive term&lt;/strong&gt; is the lag of the staionarized series in the forecasting equation.&lt;/li&gt;
&lt;li&gt;$d$: the degree of differencing (the number of times the data have had past values subtracted).&lt;/li&gt;
&lt;li&gt;$q$: the order of the moving-average terms (The size of the moving average window) or in order word, the lag of the forecast errors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A value of 0 can be used for a parameter, which indicates to not use that element of the model. When two out of the three parameters are zeros, the model may be referred to non-zero parameter. For example, $ARIMA (1,0,0)$ is $AR(1)$  (i.e. the ARIMA model is configured to perform the function if a AR model), $ARIMA(0,1,0)$ is $I(1)$, and $ARIMA(0,0,1)$ is $MA(1)$&lt;/p&gt;
&lt;h2 id=&#34;2-is-the-data-predictable&#34;&gt;2. Is the data predictable?&lt;/h2&gt;
&lt;p&gt;One of the key important thing to define before fitting ro forecast any sets of data is confirm whether the data is &lt;strong&gt;predictable&lt;/strong&gt; or it is just a &lt;strong&gt;&amp;ldquo;Random Walk&amp;rdquo;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Random walk&lt;/strong&gt; means the movement of the data is random and cannot be detected. The &lt;strong&gt;Random Walk&lt;/strong&gt; is denoted as $ARIMA(0,1,0)$. If the data is not stationary, Random walk is the simplest model to fit&lt;/p&gt;
&lt;p&gt;The forecasting equation for Random Walk is:&lt;/p&gt;
&lt;p&gt;$\hat{Y_{t}}-Y_{t-1}=\mu$&lt;/p&gt;
&lt;p&gt;In other word, &lt;strong&gt;Random walk&lt;/strong&gt; is the $AR(1)$ model with coefficient $\beta_1=0$.&lt;/p&gt;
&lt;p&gt;Therefore, to test this hypothesis, we use hypothesis testing with &lt;em&gt;null hypothesis&lt;/em&gt; $H_0 = 1$ vs. $H_1 \neq 1 $. The $AR(1)$ model is fitted to the data and we examine the coefficient. If the coefficient is statistically significantly different than 1, we can conclude that the data is predictable and vice versa.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work with some data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,8));
data.plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;ts_plot.png&#34; alt=&#34;Figure 1: Weekly Sales of suppermarket A from 2010 to 2019&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;Figure 1: Weekly Sales of suppermarket A from 2010 to 2019&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Distribution&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;ts_dist.png&#34; alt=&#34;Figure 2: Distribution of time series weekly sales&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;Figure 2: Distribution of time series weekly sales&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;These plots show a high probability that the data is not &lt;strong&gt;Stationary&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand, this data shows a seasonlity trend so instead of ARIMA, wI will use SARIMA, another seasonal-detected ARIMA model.&lt;/p&gt;
&lt;p&gt;SARIMA is denoted as $SARIMA(p,d,q)(P,D,Q)m$&lt;/p&gt;
&lt;h2 id=&#34;3-confirm-the-datas-stationarity&#34;&gt;3. Confirm the data&amp;rsquo;s Stationarity&lt;/h2&gt;
&lt;p&gt;It is essential to confirm the data to be stationary or not because this impacts directly to your model selection for the highest accuracy.&lt;/p&gt;
&lt;p&gt;There are several methods to examine the data. One of the most statistical accurate way is the &lt;strong&gt;Augmented Dicky-Fuller&lt;/strong&gt; method in which it tests the data with 2 hypothesis. The &lt;strong&gt;Null hypothesis&lt;/strong&gt; is not staionary and the &lt;strong&gt;alternative hypothese&lt;/strong&gt; is stationary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run test
series = data.values
result = adfuller(data)
print(&#39;ADF Statistic: %f&#39; % result[0])
print(&#39;p-value: %f&#39; % result[1])
print(&#39;Critical Values:&#39;)
for key, value in result[4].items():
	print(&#39;\t%s: %.3f&#39; % (key, value))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;ADF Statistic: -1.557214
p-value: 0.505043
Critical Values:
	1%: -3.492
	5%: -2.889
	10%: -2.581
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;p-value is higher than 0.05 so we fail to reject the Null hypothesis which means the data is stationary.&lt;/p&gt;
&lt;h2 id=&#34;4-differencing-the-data&#34;&gt;4. Differencing the data&lt;/h2&gt;
&lt;p&gt;Differencing is the methid to stationarize the time series data.&lt;/p&gt;
&lt;p&gt;There is quite a clear 3-month seasonality with this data so I&amp;rsquo;ll conduct 3 month seasonaliry differencing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Difference the orginal sales data
plt.figure(figsize=(15,8));
train_diff_seasonal = train - train.shift(3)
plt.plot(train_diff_seasonal)

# Conduct the test
series = train_diff_seasonal.dropna().values
result = adfuller(series)
print(&#39;ADF Statistic: %f&#39; % result[0])
print(&#39;p-value: %f&#39; % result[1])
print(&#39;Critical Values:&#39;)
for key, value in result[4].items():
	print(&#39;\t%s: %.3f&#39; % (key, value))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;ADF Statistic: -3.481334
p-value: 0.008480
Critical Values:
	1%: -3.529
	5%: -2.904
	10%: -2.590
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;ts_season_plot.png&#34; alt=&#34;Figure 3: Seasonal differencing with order of 3&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;Figure 3: Seasonal differencing with order of 3&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The data became stationary with p-value of the test is less than 0.05.
Let&amp;rsquo;s examine ACF and PACF of the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Split train, validation and test sets
train = data[:84]
validation = data[84:108]
test = data[108:]

# ACF and PACF for orginal data
series=train.dropna()
fig, ax = plt.subplots(2,1, figsize=(10,8))
fig = sm.graphics.tsa.plot_acf(series, lags=None, ax=ax[0])
fig = sm.graphics.tsa.plot_pacf(series, lags=None, ax=ax[1])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;ACF.png&#34; alt=&#34;Figure 4: ACF and PACF of orginal tiem series weekly sales&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;Figure 4: ACF and PACF of orginal tiem series weekly sales&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Some observation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With the p-value from the test is now significantly lower than 0.05, and the number of significantly peaks in ACF has dropped, the data has become stationary.
Let&amp;rsquo;s set the parameters for SARIMA&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p$ is most probably 3 as this is the last significant lag on the PACF.&lt;/li&gt;
&lt;li&gt;$d$ should equal 0 as we do not have differencing (only seasonal differencing and this will be reflected later on)&lt;/li&gt;
&lt;li&gt;$q$ should be around 3&lt;/li&gt;
&lt;li&gt;$P$ should be 2 as 3th, and 9th lags are somewhat significant on the PACF&lt;/li&gt;
&lt;li&gt;$D$ should be 1 as we performed seasonal differencing&lt;/li&gt;
&lt;li&gt;$Q$ is probably 2 as the 3th lag and 9th lag are significant in ACF plot while other 6th and 9th lags are not.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;It is not suggestable to use only ACF and PACF plots to decide the value within ARIMA model. The reason is that ACF and PACF are useful in case either $p$ or $q$ is positive. In a situation that both $p$ and $q$ are positive, these 2 plots will give no value.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The $ARIMA(p,d,0)$ is decided given the following conditions observed from ACF and PACF plots:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACF is exponentially decaying&lt;/li&gt;
&lt;li&gt;There is a significant spike at lag $p$ in the PACF, but none beyond lag $p$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For $ARIMA(0,d,q)$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PACF is exponentially decaying&lt;/li&gt;
&lt;li&gt;There is a significant spike at lag $q$ in the PACF, but none beyond lag $q$&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TIP&lt;/strong&gt; 
The ACF of stationary data should drop to zero quickly. 
For nonstationary data the value at lag 1 is positive and large.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another way to have an idea for which $p$ and $q$ values in $ARIMA$ model are opt to be used is through grid search with assigned parameter to identify the optimal comnbination based on score (aka AIC and BIC)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ps = range(3,5)
d= 0
qs = range(2,5)
Ps= range(1,4)
D=1
Qs=range(0,3)
s=6 # annual seasonality

parameters = product(ps,qs,Ps, Qs)
parameters_list = list(parameters)
result_table = optimizeSARIMA(parameters_list, d, D, s)

# set the parameters that give the lowest AIC
p, q, P, Q = result_table.parameters[0]

best_model=sm.tsa.statespace.SARIMAX(data, order=(p, d, q),
                                     seasonal_order=(P, D, Q, zs)).fit(disp=-1)
print(best_model.summary())

# Examine the residuals
# ACF and PACF for orginal data
plt.plot(best_model.resid)

fig, ax = plt.subplots(2,1, figsize=(10,8))
fig = sm.graphics.tsa.plot_acf(best_model.resid, lags=None, ax=ax[0])
fig = sm.graphics.tsa.plot_pacf(best_model.resid, lags=None, ax=ax[1])

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;residual.png&#34; alt=&#34;Figure 5: ACF and PACF plots of Residuals&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;Figure 5: ACF and PACF plots of Residuals&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Lag-1 of the residual in PACF still shows the sign of autocorrelation which implies that it needs more adjustment with the model.&lt;/p&gt;
&lt;p&gt;Below is the General process for forecasting using an ARIMA model (Source: &lt;a href=&#34;https://otexts.com/fpp2/arima-r.html#fig:arimaflowchart&#34;&gt;Hyndman, R.J., &amp;amp; Athanasopoulos, G.&lt;/a&gt; )&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;arimaflowchart.png&#34; alt=&#34;Figure 6: General process for forecasting using an ARIMA model&#34; style=&#34;width:60%&#34;&gt;
  &lt;figcaption&gt;Figure 6: General process for forecasting using an ARIMA model&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;5-model-evaluation&#34;&gt;5. Model evaluation&lt;/h2&gt;
&lt;p&gt;There are 2 common measures to evaluate the predicted values with the validation set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.	Mean Absolute Error (MAE):&lt;/strong&gt;
&amp;hellip;How far your predicted term to the real value on absolute term. One of the drawbacks of the MAE is because it shows the absolute value so there is no strong evidence and comparison on which the predicted value is actually lower or higher.&lt;/p&gt;
&lt;p&gt;$MAE=\frac{1}{n}\sum_{i = 1}^{n} |Y_{t}-\hat{Y_{t}}|$&lt;/p&gt;
&lt;p&gt;can be run with R&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(abs(Yp - Yv))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or in Python&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import metrics
metrics.mean_absolute_error(y_test, y_pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Mean absolute percentage error (MAPE):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The MAE score shows the absolute value and it is hardly to define whether that number is good or bad, close or far from expectation. This is when MAPE comes in.&lt;/p&gt;
&lt;p&gt;MAPE measures how far your predicted term to the real value on absolute percentage term.&lt;/p&gt;
&lt;p&gt;$MAPE=100\frac{1}{n}\sum_{i = 1}^{n} \frac{|Y_t-\hat{Y_t}|} {\hat{Y_{t}}}$&lt;/p&gt;
&lt;p&gt;Can compute as&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;100 x mean(abs(Yp - Yv) / Yv )
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Reference&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hyndman, R.J., &amp;amp; Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on March 31, 2020&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Practical flow of a Data Science Project</title>
      <link>https://geniusnhu.netlify.com/2020/01/26/practical-flow-of-a-data-science-project/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/2020/01/26/practical-flow-of-a-data-science-project/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Have you ever wondered why you, a talented Data Scientist, is not an adviser or a consultant for business, but just a normal employee spending time solving other&amp;rsquo;s requests?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is such a common complaint that I heard so many times from my friends, my fellows and especially from the junior. Years ago during the beginning of my career, I was also a victim of this trap. Now, I realized that this is a real &lt;strong&gt;bottleneck&lt;/strong&gt; of Data Scientist and I want to share my experience to others so that it will help other Data Scientist achieving a brighter career.&lt;/p&gt;
&lt;p&gt;Whenever I meet other Data Science fellows, most of the time was spent talking about RNN, NLP, Deep learning, or Machine learning algorithms. But when I called on a question of why they used RNN instead of Deep learning or how their model supports the business, they could either provide an unconvincing reason or stay on a lengthy explanation of concept, algorithm without a comprehensive business thinking.&lt;/p&gt;
&lt;p&gt;It is a routine for a Data Scientist to saturate with &lt;strong&gt;technical models&lt;/strong&gt; while understates the role of &lt;strong&gt;business mindset&lt;/strong&gt;. However, I totally do not deny the integral role of technical work of a data scientist, I only want to emphasize the importance of &lt;strong&gt;understanding the business concept at first before any other activities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, I list out below a standard flow for starting a data science projects with key points, which I have been applying throughout 4 years working for 2 multinational companies as a data analyst and data scientist.
This post was written based on my experience. Hence, take it as your reference and adjust to your own needs.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-first-and-foremost-importance---clarifying-business-question&#34;&gt;1. First and foremost importance - Clarifying Business question&lt;/h3&gt;
&lt;p&gt;During my years in analytics and data science, in addition to technological concept explanation, &lt;strong&gt;business question clarification&lt;/strong&gt; is one of the most difficult tasks when a Data scientist communicates with business partner.&lt;/p&gt;
&lt;p&gt;I am sure that you catch this phrase everywhere, in numerous articles, warning you to clarify business question in every situation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;But how?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Working and researching are not identical. In business, top people never stop expecting a Data Scientist to become a wise man who knows the answer for all of their questions. Therefore, &lt;strong&gt;digging the problem is our job and how to do that is our responsibility&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It must be very familiar with you when a sales manager asks you &lt;em&gt;&amp;ldquo;I want to know why sales declined?&amp;quot;&lt;/em&gt; or a marketing director demands &lt;em&gt;&amp;ldquo;How to increase the efficiency of the promotion activity for brand A on our website?&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When you hear these questions, can you imagine the right approach or the answer to the case? Or you will be very vague and keep asking yourself &lt;em&gt;&amp;ldquo;Is that they want me to do this.&amp;rdquo; or &amp;ldquo;I think they want to know that.&amp;quot;&lt;/em&gt;, and if you deliver the result based on this understanding, &lt;strong&gt;how much confidence you have on your result?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In reality, if you keep it this way, the one and only response you will get from them is:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;This is not what I need&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;OMG! How terrible it is when you spent so much effort into this, but no one values it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;This is because you did not truly understand problem so that you did not touch the right pain point!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, the director wants to know way to improve his marketing activity efficacy, so what does &lt;strong&gt;Efficacy&lt;/strong&gt; mean here? What kind of activity he points to? What are the real pain points? If these concerns are plainly clarified, the request will be interpreted as &lt;em&gt;&amp;ldquo;How to optimize the budget spending on online promotion in order to increase the purchase rate and new customers vs last year&amp;rdquo;&lt;/em&gt;. This will end up with increasing efficacy.&lt;/p&gt;
&lt;p&gt;One of the common advises is to ask &lt;strong&gt;&amp;ldquo;Why&amp;rdquo;&lt;/strong&gt; in order to dig into the real problems. However, this solution is not applicable all times because the business partners might not know why for all of your questions.&lt;/p&gt;
&lt;p&gt;What can you do more:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ask about the background of the question&lt;/strong&gt;, why and how they come up with the request after you receive it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Be sure that the request is your responsibility to answer&lt;/strong&gt;. If your company has several Data teams such as Data Scientist, Data Analyst and BI, make sure to understand the Role &amp;amp; Responsibility of each team and know when to jump in and when to leap out. However, don&amp;rsquo;t ever say &lt;em&gt;&amp;ldquo;This is not my job. Ask BI&amp;rdquo;&lt;/em&gt;. Instead, show them that you know everything about company and data &lt;em&gt;&amp;ldquo;With your request, the BI team has already got the data that can help with your question, I suggest you to meet BI and ask for sales and churn rate data of the last 3 years&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Engage with other teams in the company&lt;/strong&gt; to frequently get updates about other things happened within your company. Moreover, it is extremely important to always raise up questions such as &lt;em&gt;&amp;ldquo;What are the latest company strategy, calendar, current key projects and recent performance?&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;ldquo;Do I understand the vision and objectives of the projects that are critical to my company?&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Think of initiatives and what you can do more&lt;/strong&gt; within your expertise to bring the projects to the next level.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Be a thinker, not a doer!&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-identify-the-approach-for-the-problem&#34;&gt;2. Identify the approach for the problem&lt;/h3&gt;
&lt;p&gt;This part is to place the methodology for the analysis&lt;/p&gt;
&lt;p&gt;This step requires a broad knowledge on either &lt;strong&gt;statistical models&lt;/strong&gt; or &lt;strong&gt;machine learning&lt;/strong&gt; approached. In some companies, especially non-tech savvy ones, a data scientist is in charge of both analytics and data science work stream.&lt;/p&gt;
&lt;p&gt;With a mixed role of Analytics and Data Science, the approaches for the problem will also diversified with various concepts and models. For example: Linear Regression cannot be used to segment customers or Descriptive analysis cannot predict customer churn.&lt;/p&gt;
&lt;p&gt;At first, choosing methodology seems to be effortless but indeed always drives you crazy. If the Sales Director asked Data Science team to &lt;strong&gt;forecast the sales for next year based on the amount of budget spending while putting online appearance as company&amp;rsquo;s focus&lt;/strong&gt;, then which approach/model should be used? If the business wanted the &lt;strong&gt;forecast based on the market movement with the outlook of maintaining the current company&amp;rsquo;s leadership position&lt;/strong&gt;, which approach is correct?&lt;/p&gt;
&lt;p&gt;What you can do more:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is fundamental to &lt;strong&gt;understand the discrepancy between Descriptive analysis and Predictive analysis&lt;/strong&gt; (many people are still ambiguous between these 2 concepts). An example of descriptive analysis is the relationship between factors; while prescriptive analysis deals with figuring out the future outcomes of that relationship. &lt;em&gt;Descriptive analysis delivers historical insights and prescriptive analysis foresees future values&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identify the specific type of data&lt;/strong&gt; to assist the problem approach: The target variable and other variables are continuous, categorical or binary.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Understand key problem approaches&lt;/strong&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Binary&lt;/em&gt; (2 possible answers) or &lt;em&gt;Multi-clas&lt;/em&gt;s (more than 2 possible answers) classification;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Regression relationship&lt;/em&gt; (relationship between 2 or more factors) or &lt;em&gt;Regression prediction&lt;/em&gt; (predict future value using regression model);&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Clustering&lt;/em&gt; (cluster unlabeled observation into groups of similar characteristics) or &lt;em&gt;Segmentation&lt;/em&gt; (divide observations into specific groups);&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Trend detection&lt;/em&gt; (historical movement) or &lt;em&gt;Time-series forecasting&lt;/em&gt; (project future value of that movement).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-acquire-the-appropriate-data&#34;&gt;3. Acquire the appropriate data&lt;/h3&gt;
&lt;p&gt;After identifying the business questions and the approach above, set up data requirement and extract the appropriate data from the data warehouse are the next thing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data selection&lt;/strong&gt; sounds to be straightforward but indeed complicated. To solve the business questions, which kind of data is in need of. For example, is it necessary to have customer&amp;rsquo;s birthday information for the task of predicting churn probability?&lt;/p&gt;
&lt;p&gt;Ingest sufficient data will save you tons of effort afterward. Bear in mind the unspoken truth: &lt;em&gt;Garbage in is Garbage out&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Two major problems that usually occur during Data Collection&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The unavailability of data&lt;/li&gt;
&lt;li&gt;The bias of training data&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;31-first-lets-look-at-the-unavailability-of-data&#34;&gt;3.1 First, let&amp;rsquo;s look at the unavailability of data&lt;/h4&gt;
&lt;p&gt;This problem is very common globally in which data is unable to capture at the moment of collection due to the limitation of current digital connection. For instance, it is merely impossible to acquire the &lt;em&gt;time spent in cooking at home&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As a common sense, when the data is nonexistent, you will instantly think of way to get the data. However, you have to consider the consequences of unavailability data including cost, time, resources and if the data is indeed not too important to your model, all the effort you put into it will be down the drain.&lt;/p&gt;
&lt;p&gt;Therefore, the solution for this case is to &lt;strong&gt;defer inaccessible data&lt;/strong&gt; and in case the model requires this data for a better result, you will have more resources and confident to invest in obtaining it in the future.&lt;/p&gt;
&lt;p&gt;What you can do more if you need more data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bring along a data request summary&lt;/strong&gt; when you visit the database owner if you need to talk to other parties for this. The summary form should include background of you project, data requirement, your request. This will help smoothing the discussion and the business partner will give the adequate solution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Change the process/method of collecting data&lt;/strong&gt; to acquire the right information needed. Work with database owner or IT team or propose to your up-line a system revision plan for approval.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prepare budget and contact an outside data owner&lt;/strong&gt; if the additional data is vital to improve the model and inaccessible for you.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;32-second-the-bias-of-data&#34;&gt;3.2 Second, the bias of data&lt;/h4&gt;
&lt;p&gt;This problem is serious especially &lt;strong&gt;when the training set gets bias from the beginning, the model will learnt accordingly to that bias and results into an inaccuracy prediction&lt;/strong&gt; when comparing to the real world.&lt;/p&gt;
&lt;p&gt;One of the most famous flaws of bias in data is the Amazon recruiting AI tool that showed bias against women. The tool reviewed candidate&amp;rsquo;s resumes in order to pick the top talents within them. The tool showed an obvious bias against women because its training data is &lt;strong&gt;not gender-neutral&lt;/strong&gt; from the beginning.&lt;/p&gt;
&lt;p&gt;Therefore, at first hand, be careful with data and its natural distribution are critical responsibility of every Data Scientist.&lt;/p&gt;
&lt;p&gt;What you can do more to eliminate the bias:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ensure the statistical distribution of data and its representatives over population&lt;/strong&gt;. For example, if the population is made up of 56% of male and 43% of female and 1% of others, the data distribution must be in similar ratio.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Verify the split of train, validate and test sets&lt;/strong&gt; in prediction models to establish a similar allocation of variable and categories.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Choose the learning model fitting the problem and reduce the skewness&lt;/strong&gt;. Some models can reduce the bias in data including clustering or dimension reduction.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monitor the performance in real data&lt;/strong&gt;. Frequently run statistical test on real data to pick out uncommon case. If the test result shows a statically significant in churn rate among male than female, dig it out. Is it the sudden shift or result of bias?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;After getting all the data you need, the next step is thing that a Data scientist usually does:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;DS_Flow.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The order can be flexible and this is the standard progress that I usually do in my project and my job. Sometimes, after tuning and the accuracy does not meet my expectation, I need to go back to the feature engineering step to find other way to deal with features.&lt;/p&gt;
&lt;p&gt;These are the key bottlenecks beside the technical skills that I want to head up for Data Scientist who want to become more than just a Data insight extractor.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
