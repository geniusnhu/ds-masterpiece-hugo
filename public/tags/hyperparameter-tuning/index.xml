<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hyperparameter tuning | Nhu Hoang</title>
    <link>https://geniusnhu.netlify.com/tags/hyperparameter-tuning/</link>
      <atom:link href="https://geniusnhu.netlify.com/tags/hyperparameter-tuning/index.xml" rel="self" type="application/rss+xml" />
    <description>Hyperparameter tuning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 Nhu HoangServed by [Netlify](https://geniusnhu.netlify.app/)</copyright><lastBuildDate>Sun, 14 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://geniusnhu.netlify.com/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_300x300_fit_lanczos_3.png</url>
      <title>Hyperparameter tuning</title>
      <link>https://geniusnhu.netlify.com/tags/hyperparameter-tuning/</link>
    </image>
    
    <item>
      <title>Improve deep neural network training speed and performance with Optimization</title>
      <link>https://geniusnhu.netlify.com/project/2020-06-14-optimization-and-neural-network-performance/</link>
      <pubDate>Sun, 14 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/project/2020-06-14-optimization-and-neural-network-performance/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://geniusnhu.netlify.app/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/&#34;&gt;Part 1: Initialization, Activation function and Batch Normalization/Gradient Clipping&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 2: Optimizer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Training a deep neural network is an extremely time-consuming task especially with complex problems. Using a faster optimizer for the network is an efficient way to speed up the training speed, rather than simply using the regular Gradient Descent optimizer. Below, I will discuss and show training results/speed of 5 popular Optimizer approaches: &lt;strong&gt;Gradient Descent with momentum and Nesterov Accelerated Gradient, AdaGrad, RMSProp, Adam, and Nadam optimizer&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One of the dangers of using inappropriate optimizers is that the model takes a long time to converge to a global minimum or it will be stuck at a local minimum, resulting in a worse model. Therefore, knowing which Optimizer suits mostly on the problem will save you tons of training hours.
The main purpose of tuning Optimizer is to speed up the training speed but it also helps to improve the model&amp;rsquo;s performance.&lt;/p&gt;
&lt;h2 id=&#34;1-gradient-descent&#34;&gt;1. Gradient Descent&lt;/h2&gt;
&lt;p&gt;Computing the gradient of the associated cost function with regard to each theta and getting the gradient vector pointing uphill, then going in the opposite direction with the vector direction (downhill) using the below equation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$\theta_{next step} = \theta - \eta*  \nabla_{\theta}J(\theta)$&lt;/p&gt;
&lt;p&gt;$\theta$: weight of the model&lt;/p&gt;
&lt;p&gt;$\eta$: learning rate&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, the speed of &lt;strong&gt;Gradient Descent&lt;/strong&gt; optimizer depends solely on the learning rate parameter ($\eta$). With a small learning rate, GD will take small and unchanged steps downward on a gentle surface, and a bit faster steps on a steep surface. Consequently, in a large neural network, it repeats millions of slow steps until it reaches the global minimum (or gets lost in the local minimum). Therefore, the runtime becomes extremely slow.&lt;/p&gt;
&lt;p&gt;Result of training with Fashion MNIST dataset using &lt;strong&gt;SGD&lt;/strong&gt;:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;SGD.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Firgure 1: Loss and accuracy of model using SGD with learning rate 0.001&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The loss declined gradually and will be closer and closer to global minimum after several more epochs.&lt;/p&gt;
&lt;p&gt;There are other versions of Gradient Descent such as &lt;strong&gt;Batch Gradient Descent&lt;/strong&gt; (running on a full dataset), &lt;strong&gt;Mini-batch Gradient Descent&lt;/strong&gt; (running on random subsets of a dataset), &lt;strong&gt;Stochastic Gradient Descent - SGD&lt;/strong&gt; (picking a random instance at each step), and all have pros and cons. &lt;strong&gt;Batch Gradient Descent&lt;/strong&gt; can reach the global minimum at a terribly slow pace. &lt;strong&gt;Mini-batch Gradient Descent&lt;/strong&gt; gets to the global minimum faster than BGD but it is easier to get stuck in the local minimum, and &lt;strong&gt;SGD&lt;/strong&gt; is usually harder to get to the global minimum compared to the other two.&lt;/p&gt;
&lt;h2 id=&#34;2-momentum-optimization&#34;&gt;2. Momentum Optimization&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s imagine, when a ball rolls from the summit, downward the sloping side to the foot of the hill, it will start slowly then increase the speed as the momentum picks up and eventually reaches a fast pace toward the minimum. This is how &lt;strong&gt;Momentum Optimization&lt;/strong&gt; works. This is enabled by adding a momentum vector m and update the theta parameter with this new weight from &lt;em&gt;momentum vector&lt;/em&gt; $m$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$m$ ← $\beta m - \eta * \nabla_{\theta}J(\theta)$&lt;/p&gt;
&lt;p&gt;$\theta_{next step}$ ← $\theta + m$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Gradient descent&lt;/em&gt; does not take into account the previous gradients. By adding the &lt;em&gt;momentum vector&lt;/em&gt;, it updates the weight $m$ after each iteration. The momentum $\beta$ is the parameter controls how fast the terminal velocity is, which is typically set at 0.9 but it should be tuned from 0.5 to 0.99. As a result, &lt;strong&gt;Momentum Optimizer&lt;/strong&gt; converges better and faster than &lt;em&gt;SGD&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement Momentum optimizer in Tensorflow
optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.99)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;SDG_momentum.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 2: Loss and accuracy of models using SGD compared to momentum optimizer&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Momentum&lt;/strong&gt; converges faster and eventually reaches a better result than &lt;em&gt;SGD&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;3-nesterov-accelerated-gradient&#34;&gt;3. Nesterov Accelerated Gradient&lt;/h2&gt;
&lt;p&gt;Another variation of &lt;em&gt;Momentum Optimizer&lt;/em&gt; is &lt;strong&gt;Nesterov Accelerated Gradient - NAG&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$m$ ← $\beta m - \eta * \nabla_{\theta}J(\theta + \beta m)$&lt;/p&gt;
&lt;p&gt;$\theta_{next step}$ ← $\theta + m$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The gradient of the cost function is measured at location $\theta + \beta m$ (instead of $\theta$ in the original momentum optimization). The reason behind this is that momentum optimization has already pointed toward the right direction, so we should use a slightly ahead location (an approximately next position of the $\theta$) to moderately accelerating the speed of convergence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement Nesterov Accelerated Gradient optimizer in Tensorflow
optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.9,nesterov=True
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;NAG_momentum.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 3: Loss and accuracy of models using momentum compared to Nesterov Accelerated Gradient optimizer&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;NAG&lt;/strong&gt; showed only a slightly better result than original &lt;em&gt;Momentum&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;4-adagrad&#34;&gt;4. AdaGrad&lt;/h2&gt;
&lt;p&gt;One of the &lt;em&gt;Adaptive learning rate methods&lt;/em&gt;, in which the algorithm goes faster down the steep slopes than the gentle slopes.
&lt;strong&gt;AdaGrad&lt;/strong&gt; performs well in a simple quadratic problem but not in training a neural network because it tends to slow down a bit too fast and stops before reaching the global minimum. Due to this drawback, I do not usually use &lt;strong&gt;AdaGrad&lt;/strong&gt; for Neural Network but instead apply &lt;strong&gt;RMSProp&lt;/strong&gt;, an alternative of &lt;strong&gt;AdaGrad&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;5-rmsprop---root-mean-square-prop&#34;&gt;5. RMSProp - Root Mean Square Prop&lt;/h2&gt;
&lt;p&gt;This is one of the most frequently used optimizers, which continues the idea of &lt;em&gt;Adagrad&lt;/em&gt; in trying to minimize the vertical movement and updating the model in a horizontal direction toward the global minimum.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Adagrad&lt;/em&gt; sums the gradients from the first iteration and that is why it usually never converges to the global minimum, while &lt;strong&gt;RMSProp&lt;/strong&gt; accumulates the gradients from the previous iterations:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$s$ ← $\beta s - (1-\beta) \nabla_{\theta}J(\theta)^2$&lt;/p&gt;
&lt;p&gt;$\theta_{nextstep}$ ← $\theta + \frac{\eta \nabla_{\theta}J(\theta)}{\sqrt{s + \epsilon}}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$\beta$: decay rate, typically set at 0.9&lt;/p&gt;
&lt;p&gt;$s$: exponential average square of past gradients&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement RMSProp optimizer in Tensorflow
optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;adagrad_rmsprop.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 4: Loss and accuracy of models using RMSProp compared to Adagrad optimizer&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;RMSProp&lt;/strong&gt; converges better than &lt;em&gt;Adagrad&lt;/em&gt; which is lost at a plateau.&lt;/p&gt;
&lt;h2 id=&#34;6-adam&#34;&gt;6. Adam&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Adam optimizer&lt;/strong&gt; is the combination of &lt;em&gt;Momentum&lt;/em&gt; and &lt;em&gt;RMSProp&lt;/em&gt; optimizers. In other words, it takes into account both the exponential decay average of past gradients and the exponential decay average of past squared gradients.&lt;/p&gt;
&lt;p&gt;With these characteristics, &lt;strong&gt;Adam&lt;/strong&gt; is suitable for handling sparse gradients on complex problems with complex data and a large number of features.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$m$ ← $\beta_1 m - (1-\beta_1) \nabla_{\theta}J(\theta)$&lt;/p&gt;
&lt;p&gt;$s$ ← $\beta_2 s - (1-\beta_2) \nabla_{\theta}J(\theta)$&lt;/p&gt;
&lt;p&gt;$\hat{m}$ ← $\frac{m}{1-\beta_1^T}$&lt;/p&gt;
&lt;p&gt;$\hat{s}$ ← $\frac{s}{1-\beta_2^T}$&lt;/p&gt;
&lt;p&gt;$\theta_{nextstep}$ ← $\theta + \frac{\eta \hat{m}}{\sqrt{\hat{s} + \epsilon}}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$\eta$: learning rate&lt;/p&gt;
&lt;p&gt;$s$: exponential average square of past gradients&lt;/p&gt;
&lt;p&gt;$m$: momentum vector&lt;/p&gt;
&lt;p&gt;$\beta_1$: momentum decay, typlically set at 0.9&lt;/p&gt;
&lt;p&gt;$\beta_2$: scaling decay, typlically set at 0.999&lt;/p&gt;
&lt;p&gt;$\epsilon$: smoothing term&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement Adam optimizer in Tensorflow
optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;adagrad_rmsprop_adam.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 5: Loss and accuracy of models using Adagrad, RMSProp, and Adam&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;7-nadam&#34;&gt;7. Nadam&lt;/h2&gt;
&lt;p&gt;Another variation of &lt;em&gt;Adam&lt;/em&gt; is &lt;strong&gt;Nadam&lt;/strong&gt; (using &lt;em&gt;Adam optimizer with Nesterov technique&lt;/em&gt;), resulting in a little faster training time than &lt;em&gt;Adam&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Implement Nadam optimizer in Tensorflow
optimizer=keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;rmsprop_adam_nadam.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Figure 6: Loss and accuracy of models using RMSProp, Adam and Nadam&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Adagrad, RMSProp, Ada, Nadam, and Adamax&lt;/strong&gt; are &lt;em&gt;Adaptive learning rate algorithms&lt;/em&gt;, which require less tuning on hyperparameters. In case the performance of the model does not meet your expectation, you can try to change back to &lt;strong&gt;Momentum optimizer&lt;/strong&gt; or &lt;strong&gt;Nesterov Accelerated Gradient&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;final-words-&#34;&gt;Final words 🤘&lt;/h1&gt;
&lt;p&gt;In conclusion, most of the time, &lt;em&gt;Adaptive learning rate algorithms&lt;/em&gt; outperform &lt;em&gt;Gradient descent&lt;/em&gt; and its variants in terms of speed, especially in a deep neural network. However, &lt;em&gt;Adaptive learning rate algorithms&lt;/em&gt; do not ensure an absolute convergence to the global minimum.&lt;/p&gt;
&lt;p&gt;If your model is not too complex with a small number of features, and training time is not your priority, using &lt;strong&gt;Momentum&lt;/strong&gt;, &lt;strong&gt;Nesterov Accelerated Gradient&lt;/strong&gt; or &lt;strong&gt;SGD&lt;/strong&gt; is the optimal starting point, then tune the learning rate, activation functions, change Initialization technique to improve the model rather than using &lt;strong&gt;Adaptive learning rate Optimizers&lt;/strong&gt; because the later ones hinder the risk of not converging to the global minimum.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;summary.png&#34; alt=&#34;&#34; style=&#34;width:110%&#34;&gt;
  &lt;figcaption&gt;Figure 7: Summary model performance on training loss of different optimization techniques&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Regular SGD or regular Gradient Descent takes much more time to converge to the global minimum. Adagrad often stops too early before reaching the global minimum so in time it becomes the worse optimizer.&lt;/li&gt;
&lt;li&gt;With the Fashion MNIST dataset, Adam/Nadam eventually performs better than RMSProp and Momentum/Nesterov Accelerated Gradient. This depends on the model, usually, Nadam outperforms Adam but sometimes RMSProp gives the best performance.&lt;/li&gt;
&lt;li&gt;With my experience, I found out that Momentum, RMSProp, and Adam (or Nadam) should be the first try of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Optimizer&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Training speed&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Converge quality&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Note&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Gradient Descent / SGD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Medium for simple model&lt;br&gt;Slow for complex model&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Good&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Risk of converging to local minimum.&lt;br&gt;Can be controled by assigning the correct learning rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Momentum&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast for simple model&lt;br&gt;Medium for complex model&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Good&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for less complex NN with small number of features&lt;br&gt;Need to consider tuning the momentum hyperparameter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nesterov Accelerated&lt;br&gt;Gradient&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast for simple model&lt;br&gt;Medium for complex model&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Good&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for less complex NN with small number of features&lt;br&gt;Need to consider tuning the momentum hyperparameter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AdaGrad&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Usually miss global minimum&lt;br&gt;due to early stopping&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for simple quadratic problem, not NN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RMSProp&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Acceptable&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for complex NN&lt;br&gt;Need to tune Decay rate for better performance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adam&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Acceptable&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for sparse gradients on complex model&lt;br&gt;with a large number of features&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nadam&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Fast&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Good&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Suitable for sparse gradients on complex model&lt;br&gt;with a large number of features&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;This article was originally published in &lt;a href=&#34;https://towardsdatascience.com/full-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&#34;&gt;Towards Data Science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Source code: &lt;a href=&#34;https://github.com/geniusnhu/DNN-Improvement/blob/master/Tuning_Optimizer.ipynb&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speed up training and improve performance in deep neural net</title>
      <link>https://geniusnhu.netlify.com/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/</guid>
      <description>&lt;p&gt;Training a large and deep neural network is a time and computation consuming task and was the main reason for the unpopularity of DNN 20 years ago. As several techniques have been found out to push up the training speed, Deep learning has come back to the light. So which technique to use, how and when to use which? Let&amp;rsquo;s discuss it here!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Performance summary is shown at the end of the post for Classification &amp;amp; Regression examples&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-applying-initialization&#34;&gt;1. Applying Initialization&lt;/h2&gt;
&lt;p&gt;Initialization is one of the first technique used to fasten the training time of Neuron Network (as well as improve performance). Let&amp;rsquo;s briefly explain its importance. In Artificial Neural Network (ANN), there are numerous connections between different neurons. One neuron in the current layer connects to several neurons in the next layer and is attached to various ones in the previous layer. If 2 neurons interact frequently than another pair, their connection (i.e the weights) will be stronger than the other one.&lt;/p&gt;
&lt;p&gt;However, one problem with the ANN is that if the weights aren&amp;rsquo;t specified from the beginning of training, the connection weights can be either too small or too large which makes them too tiny or too massive to use further in the network. In other words, the network will fall into &lt;strong&gt;Vanishing Gradients&lt;/strong&gt; or &lt;strong&gt;Exploding Gradients&lt;/strong&gt; problems.&lt;/p&gt;
&lt;p&gt;So if the weights are set at suitable random values from the beginning of the training, these problem can be avoided. This technique was proposed by &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf&#34;&gt;Glorot and Bengio&lt;/a&gt;, which then significantly lifted these unstable problems. This initialization strategy is called &lt;em&gt;Xavier initialization&lt;/em&gt; or &lt;em&gt;Glorot initialization&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this strategy, the connection weights between neurons are initialized randomly using the Normal distribution with $mean=0$ and variance $\sigma^2 = \frac{2}{fan_{in}+fan_{out}}$ , in which $fan_{in}$ is the number of input neurons and $fan_{out}$ is the number of output neurons.&lt;/p&gt;
&lt;p&gt;There are 2 other popular initialization techniques beside &lt;strong&gt;Glorot&lt;/strong&gt; (used in Keras as default): &lt;strong&gt;He&lt;/strong&gt; and &lt;strong&gt;LeCun&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s examine different initialization techniques&#39; effect on model performance and training time with &lt;code&gt;fashion MNIST&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(10, 10))
for row in range(5):
  for col in range(5):
    index = 5 * row + col
    plt.subplot(5, 5, index + 1)
    plt.imshow(X_train_full[index], cmap=&amp;quot;binary&amp;quot;, interpolation=&amp;quot;nearest&amp;quot;)
    plt.axis(&#39;off&#39;)
    plt.title(y_train_full[index], fontsize=12)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;fashion_set.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;Here is the example of Fashion MNIST, in which the predictors are a set of values in the shape of [28,28] representing the image; and the target value is 10 types of cloth and shoes (denoted from 0 to 9)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;First, let&amp;rsquo;s start with the default setting of Keras on a network consisting of 5 hidden layers and 300, 100, 50, 50, 50 neurons each.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.random.set_seed(50)
np.random.seed(50)

model_default = keras.models.Sequential()
model_default.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_layers in (300, 100, 50, 50, 50):
  model_default.add(keras.layers.Dense(n_layers, activation =&#39;relu&#39;))
model_default.add(keras.layers.Dense(10, activation=&#39;softmax&#39;))
    
model_default.compile(loss=&amp;quot;sparse_categorical_crossentropy&amp;quot;,
                      optimizer=keras.optimizers.SGD(lr=1e-3),
                      metrics=[&amp;quot;accuracy&amp;quot;])

start_time = time.time()
history = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)
print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4185 - accuracy: 0.8526 - val_loss: 0.4256 - val_accuracy: 0.8518
--- 99.03307843208313 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The train set reached 85.26% accuracy and Val set reached 85.18% within 99.3 seconds. If &lt;code&gt;activation =&#39;relu&#39;&lt;/code&gt; is not set (i.e. no Activation function in the hidden layers), the accuracy is 85.32% and 84.95% respectively with 104.5 seconds needed to train on.&lt;/p&gt;
&lt;p&gt;Comparing this with weight initialization to all Zeros and all Ones:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Zeros initialization
Epoch 20/20
1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925
--- 69.43926930427551 seconds ---

# Ones initialization
Epoch 20/20
1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925
--- 67.2280786037445 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The performance in both cases is much worse and actually the model stopped improving from 5th epoch.&lt;/p&gt;
&lt;p&gt;Another Initialization that can be considered to use is &lt;code&gt;He Initialization&lt;/code&gt;,  enabling in Keras by adding &lt;code&gt;kernel_initializer=&amp;quot;he_normal&amp;quot;&lt;/code&gt; argument to the hidden layers.&lt;/p&gt;
&lt;p&gt;Result&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3780 - accuracy: 0.8672 - val_loss: 0.3924 - val_accuracy: 0.8637
--- 99.76096153259277 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The accuracy actually improved but the running time was half a second slower than &lt;strong&gt;Glorot Initialization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are also discussions about the performance of &lt;strong&gt;normal distribution&lt;/strong&gt; and &lt;strong&gt;uniform distribution&lt;/strong&gt; in initialization technique, but there is indeed no one shows better performance than the other one. The result of &lt;code&gt;init = keras.initializers.VarianceScaling(scale=2.,mode=&#39;fan_avg&#39;,distribution=&#39;uniform&#39;)&lt;/code&gt; does not improve for this data set (Train set accuracy: 87.05%, Val set: 86.27% and took 100.82 seconds to run)&lt;/p&gt;
&lt;h2 id=&#34;2-get-along-with-the-right-activation-function&#34;&gt;2. Get along with the right Activation function&lt;/h2&gt;
&lt;p&gt;Choosing an unfit activation function is one of the reasons leading to poor model performance. &lt;code&gt;sigmoid&lt;/code&gt; might be a good choice but I prefer to use &lt;strong&gt;SELU, ReLU, or its variants&lt;/strong&gt; instead.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s talk about &lt;strong&gt;ReLU&lt;/strong&gt; first. Simply saying, if the value is larger than 0, the function returns the value itself; else it returns 0. This activation is fast to compute but in return there will be a case that it stops outputting anything other than 0 (i.e neurons were died). This issue usually happens in case of a large learning rate.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;relu_and_lrelu.png&#34; alt=&#34;&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;ReLU, Leaky ReLU and SELU&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Some of the solutions for this problem is to use alternative versions of ReLU: &lt;strong&gt;LeakyReLU, Randomized LeakyReLU or Scaled ReLU (SELU)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;LeakyReLU&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if x&amp;gt;0:
  return x
else:
  return ax
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in which a is $\alpha$, the slope of the $x$ given $x&amp;lt;0$. $\alpha$ is usually set at 0.01, serving as a small leak (that&amp;rsquo;s why this technique is called LeakyReLU). Using $\alpha$ helps to stop the dying problem (i.e. slope=0).&lt;/p&gt;
&lt;p&gt;In case of &lt;strong&gt;Randomized LeakyReLU&lt;/strong&gt;, $\alpha$ is selected randomly given a range. This method can reduce the Overfitting issue but requires more running time due to extra computation.&lt;/p&gt;
&lt;p&gt;One of the outperformed activation function for DNN is &lt;strong&gt;Scaled ReLU (SELU)&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if x&amp;gt;0:
  return Lambda*x
else:
  return Lambda*(alpha*exp(x)-alpha)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this function, each layer outputs&#39; mean is 0 and standard deviation is 1. Note when using this activation function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; It must be used with &lt;code&gt;kernel_initializer=&amp;quot;lecun_normal&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The input features must be standardized&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; The NN&amp;rsquo;s architecture must be sequential&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s try different Activation functions on the &lt;code&gt;fashion MNIST&lt;/code&gt; dataset.&lt;/p&gt;
&lt;p&gt;Result of &lt;strong&gt;LeakyReLU&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3791 - accuracy: 0.8670 - val_loss: 0.3910 - val_accuracy: 0.8615
--- 101.87710905075073 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result of &lt;strong&gt;Randomized LeakyReLU&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 6s 3ms/step - loss: 0.3779 - accuracy: 0.8667 - val_loss: 0.3918 - val_accuracy: 0.8630
--- 113.58738899230957 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result of &lt;strong&gt;SELU&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 19/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3526 - accuracy: 0.8763 - val_loss: 0.3755 - val_accuracy: 0.8647
--- 106.25733232498169 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;SELU&lt;/strong&gt; seems to achieve slightly better performance over ReLU and its variants but the speed is slower (as expected).&lt;/p&gt;
&lt;span class=&#34;markup-quote&#34;&gt;&lt;strong&gt;If the NN performs relatively well at a low learning rate, ReLU is an optimal choice given the fastest training time. In case of the deep NN, SELU is an excellent try.&lt;/strong&gt;&lt;/span&gt;
&lt;p&gt;Detailed explanation about these activations can be found in here: &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf&#34;&gt;ReLU&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1505.00853&#34;&gt;LeakyReLU, Randomized LeakyReLU&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1706.02515&#34;&gt;SELU&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-batch-normalization&#34;&gt;3. Batch Normalization&lt;/h2&gt;
&lt;p&gt;To ensure Vanishing/Exploding Gradients problems do not happen again during training (as Initialization and Activation function can help reduce these issues at the beginning of the training), &lt;strong&gt;Batch Normalization&lt;/strong&gt; is implemented.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Batch Normalization&lt;/strong&gt; zeros centers and normalizes each input, then scales and shifts the result using 1 parameter vector for scaling and 1 for shifting. This technique evaluates the $mean$ and $standard deviation$ of the input over the current mini-batch and repeats this calculation across all mini-batches of the training set. $\mu$ and $\sigma$ are estimated during training but only used after training.&lt;/p&gt;
&lt;p&gt;The vector of input means $\mu$ and vector of input standard devition $\sigma$ will become non-trainable parameters (i.e. untouchable by backpropagation) and be used to compute the moving averages at the end of the training. Subsequently, these final parameters will be used to normalize new data to make prediction.&lt;/p&gt;
&lt;p&gt;If using &lt;strong&gt;Batch Normalization&lt;/strong&gt;, the input data will not need to be standardized prior training.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.random.set_seed(50)
np.random.seed(50)

model_default = keras.models.Sequential()
model_default.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_layers in (300, 100, 50, 50, 50):
  model_default.add(keras.layers.BatchNormalization())
  model_default.add(keras.layers.Dense(n_layers, activation =&#39;relu&#39;, kernel_initializer=&amp;quot;he_normal&amp;quot;))
model_default.add(keras.layers.Dense(10, activation=&#39;softmax&#39;))
    
model_default.compile(loss=&amp;quot;sparse_categorical_crossentropy&amp;quot;,
                      optimizer=keras.optimizers.SGD(lr=1e-3),
                      metrics=[&amp;quot;accuracy&amp;quot;])

start_time = time.time()
history = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)
print(&amp;quot;--- %s seconds ---&amp;quot; % (time.time() - start_time))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 8s 5ms/step - loss: 0.3799 - accuracy: 0.8645 - val_loss: 0.3571 - val_accuracy: 0.8685
--- 167.6186249256134 seconds ---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, training is slower in &lt;strong&gt;Batch Normalization&lt;/strong&gt; given more computations during training but in contrast, in &lt;strong&gt;Batch Normalization&lt;/strong&gt;, the model convergences faster so fewer epoches are needed to reach the same performance.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Batch Normalization is strictly implemented in Recurrent NN
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;4-gradient-clipping&#34;&gt;4. Gradient Clipping&lt;/h2&gt;
&lt;p&gt;As &lt;strong&gt;Batch Normalization&lt;/strong&gt; is recommended not to use with Recurrent NN, &lt;strong&gt;Gradient Clipping&lt;/strong&gt; is the alternative choice for RNN.&lt;/p&gt;
&lt;p&gt;Details about &lt;a href=&#34;https://arxiv.org/abs/1211.5063&#34;&gt;Gradient Clipping&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;summary-of-the-result-of-classification-task-with-fashion-mnist-dataset&#34;&gt;Summary of the result of Classification task with Fashion MNIST dataset&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Initialization&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Activation fuction&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Train set accuracy&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Val set accuracy&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Running time (seconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Glorot - Zeros/Ones&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.08%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.25%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;69.43/67.22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Glorot&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;None&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;85.32%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;84.95%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;104.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Glorot - Normal Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;85.26%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;85.18%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;99.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.72%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.37%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;99.76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Uniform  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;87.05%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.27%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;100.82&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Leaky ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.7%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.15%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;101.87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Randomized LeakyReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.67%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.3%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;113.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LeCun&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;SELU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;87.63%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.47%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;106.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Batch normalization He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.45%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;86.85%&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;167.618&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;summary-of-the-result-of-regression-task-with-california-housing-dataset&#34;&gt;Summary of the result of Regression task with California housing dataset&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Initialization&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Activation fuction&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Train set MSE&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Val set MSE&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Running time (seconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Glorot&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;None&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3985&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3899&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Glorot - Normal Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3779&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3819&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3517&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.35&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Leaky ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3517&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.35&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.48&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Randomized LeakyReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3517&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.35&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LeCun&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;SELU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.3423&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.326&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9.38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Batch normalization He - Normal  Dist&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.4365&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.5728&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.64&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;MSE of Train and Validation set&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;MSE.png&#34; alt=&#34;&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;regression_BN.png&#34; alt=&#34;&#34; style=&#34;width:35%&#34;&gt;
  &lt;figcaption&gt;Fashion MNIST consists of image on 10 types of fashion&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    These performances are subject to change depending on the dataset and NN&amp;rsquo;s architecture
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;final-thoughts-on-this-part-&#34;&gt;Final thoughts on this part 🔆&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Glorot Initialization is the good starting point for most of the cases. He Initialization technique sometimes performs better than Glorot (slower in the above Classification example while faster in Regression example).&lt;/li&gt;
&lt;li&gt;ReLU or Leaky ReLU are great choices if running time is the priority.&lt;/li&gt;
&lt;li&gt;ReLU should be avoided if high Learning rate is used.&lt;/li&gt;
&lt;li&gt;SELU is the good choice for complex dataset and deep neural network but might be traded off by running time. However, if the NN&amp;rsquo;s architecture does not allow &lt;em&gt;self-normalization&lt;/em&gt;, use ELU instead of SELU.&lt;/li&gt;
&lt;li&gt;SELU and Batch Normalization cannot be applied in RNN. Gradient Clipping is the alternative strategy for Batch Normalization in RNN.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;5-transfer-learning&#34;&gt;5. Transfer Learning&lt;/h2&gt;
&lt;p&gt;Another important technique too improve the performance of DNN is &lt;strong&gt;Transfer Learning&lt;/strong&gt;, using pretrained layers to train similar new task. There is much to say about this technique and it will be covered in another post.&lt;/p&gt;
&lt;p&gt;Source code can be accessed &lt;a href=&#34;https://github.com/geniusnhu/DNN-Improvement/blob/master/Improve_DNN_performance.ipynb&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Reference:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Glorot, X., &amp;amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. PMLR&lt;/li&gt;
&lt;li&gt;He, K., Zhang, X., Ren,S., &amp;amp; Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)&lt;/li&gt;
&lt;li&gt;Geron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O&amp;rsquo;Reilly Media, Inc.,&lt;/li&gt;
&lt;li&gt;Xu, B., Wang, N., Chen, T., &amp;amp; Li, M. (2015). Empirical Evaluation of Rectified Activations in Convolutional Network. Retrieved from &lt;a href=&#34;https://arxiv.org/abs/1505.00853&#34;&gt;https://arxiv.org/abs/1505.00853&lt;/a&gt; on May 5, 2020.&lt;/li&gt;
&lt;li&gt;Klambauer, G., Unterthiner, T., Mayr, A., &amp;amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. Advances in Neural Information Processing Systems 30 (NIPS 2017)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
