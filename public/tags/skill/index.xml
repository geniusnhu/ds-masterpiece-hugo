<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Skill | Nhu Hoang</title>
    <link>https://geniusnhu.netlify.com/tags/skill/</link>
      <atom:link href="https://geniusnhu.netlify.com/tags/skill/index.xml" rel="self" type="application/rss+xml" />
    <description>Skill</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 Nhu HoangServed by [Netlify](https://geniusnhu.netlify.app/)</copyright><lastBuildDate>Sat, 01 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://geniusnhu.netlify.com/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_300x300_fit_lanczos_3.png</url>
      <title>Skill</title>
      <link>https://geniusnhu.netlify.com/tags/skill/</link>
    </image>
    
    <item>
      <title>Optimize Memory Tips in Python</title>
      <link>https://geniusnhu.netlify.com/project/2022-01-01-optimize-memory-tips-in-python/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/project/2022-01-01-optimize-memory-tips-in-python/</guid>
      <description>&lt;p&gt;Memory management in Python is not a simple issue to solve, it requires a decent understanding of Python objects and data structures. Unlike in C/C++, users have no control over memory management. It is taken over by Python itself. However, with some insights about how Python works and memory support modules, we can somehow seed some light on how to control this issue.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;how-much-memory-is-being-allocated&#34;&gt;How much memory is being allocated?&lt;/h2&gt;
&lt;p&gt;There are several ways to get the size of an object in Python. You can use &lt;code&gt;sys.getsizeof()&lt;/code&gt; to get the exact size of the object, &lt;code&gt;objgraph.show_refs()&lt;/code&gt; to visualize the structure of an object or &lt;code&gt;psutil.Process().memory_info().rss&lt;/code&gt; to get all memory is allocated at the moment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; import sys
&amp;gt;&amp;gt;&amp;gt; import objgraph
&amp;gt;&amp;gt;&amp;gt; import psutil
&amp;gt;&amp;gt;&amp;gt; import pandas as pd


&amp;gt;&amp;gt;&amp;gt; ob = np.ones((1024, 1024, 1024, 3), dtype=np.uint8)

### Check object &#39;ob&#39; size
&amp;gt;&amp;gt;&amp;gt; sys.getsizeof(ob) / (1024 * 1024)
3072.0001373291016

### Check current memory usage of whole process (include ob and installed packages, ...)
&amp;gt;&amp;gt;&amp;gt; psutil.Process().memory_info().rss / (1024 * 1024)
3234.19140625

### Check structure of &#39;ob&#39; (Useful for class object)
&amp;gt;&amp;gt;&amp;gt; objgraph.show_refs([ob], filename=&#39;sample-graph.png&#39;)

### Check memory for pandas.DataFrame
&amp;gt;&amp;gt;&amp;gt; from sklearn.datasets import load_boston
&amp;gt;&amp;gt;&amp;gt; data = load_boston()
&amp;gt;&amp;gt;&amp;gt; data = pd.DataFrame(data[&#39;data&#39;])
&amp;gt;&amp;gt;&amp;gt; print(data.info(verbose=False, memory_usage=&#39;deep&#39;))
&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 506 entries, 0 to 505
Columns: 13 entries, 0 to 12
dtypes: float64(13)
memory usage: 51.5 KB
  
### Check memory for pandas.Series
&amp;gt;&amp;gt;&amp;gt; data[0].memory_usage(deep=True)   # deep=True to include all the memory used by underlying parts that construct the pd.Series
4176

#### deep=True to include all the memory used by underlying parts that construct the pd.Series ###

&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;sample-graph.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt; Structure of ‘ob’ &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.python.org/3/library/tracemalloc.html&#34;&gt;tracemalloc&lt;/a&gt; is also another choice. It is included in the Python standard library and provides block-level traces of memory allocation, statistics for the overall memory behavior of a program.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; import tracemalloc
&amp;gt;&amp;gt;&amp;gt; import numpy as np

&amp;gt;&amp;gt;&amp;gt; def create_array(x, y):
&amp;gt;&amp;gt;&amp;gt;     x = x**2
&amp;gt;&amp;gt;&amp;gt;     y = y**3
&amp;gt;&amp;gt;&amp;gt;     return np.ones((x, y, 1024, 3), dtype=np.uint8)

&amp;gt;&amp;gt;&amp;gt; tracemalloc.start()
&amp;gt;&amp;gt;&amp;gt; ### Run application
&amp;gt;&amp;gt;&amp;gt; arr = create_array(30,10)
&amp;gt;&amp;gt;&amp;gt; ### take memory usage snapshot
&amp;gt;&amp;gt;&amp;gt; snapshot = tracemalloc.take_snapshot()
&amp;gt;&amp;gt;&amp;gt; top_stats = snapshot.statistics(&#39;lineno&#39;)

&amp;gt;&amp;gt;&amp;gt; ### Print top 10 files allocating the most memory
&amp;gt;&amp;gt;&amp;gt; print(&amp;quot;[ Top 10 ]&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; for stat in top_stats[:10]:
&amp;gt;&amp;gt;&amp;gt;     print(stat)


[ Top 10 ]
/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py:192: size=2637 MiB, count=2, average=1318 MiB
/usr/lib/python3.7/threading.py:289: size=216 B, count=6, average=36 B
/usr/lib/python3.7/codeop.py:141: size=189 B, count=2, average=94 B
/usr/local/lib/python3.7/dist-packages/debugpy/_vendored/pydevd/pydevd.py:1532: size=144 B, count=2, average=72 B
/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2820: size=112 B, count=3, average=37 B
/usr/lib/python3.7/queue.py:182: size=80 B, count=1, average=80 B
/usr/lib/python3.7/queue.py:107: size=80 B, count=1, average=80 B
/usr/lib/python3.7/threading.py:1264: size=72 B, count=1, average=72 B
    
## ==================================================================================
##### Explanation #####
## count: Number of memory blocks (int)
## size: Total size of memory blocks in bytes (int)
## traceback: Traceback where the memory block was allocated, Traceback instance

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most used file is the arr object which takes up 2 memory blocks with a total size of 2637 MiB. Other objects are minimal.&lt;/p&gt;
&lt;p&gt;Another important technique is to estimate how much memory is needed for the process to run. This can be guessed through monitoring the peak memory usage of the process. To measure peak memory, you can use the below code at the end of the process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
### For Linux (in KiB) and MacOS (in bytes)
from resource import getrusage, RUSAGE_SELF
print(getrusage(RUSAGE_SELF).ru_maxrss)
### For Windows
import psutil
print(psutil.Process().memory_info().peak_wset)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having the peak number and the amount of data put in the process, you can, by some means, judge the amount of memory to be consumed for your next process.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;optimize-memory&#34;&gt;Optimize memory&lt;/h1&gt;
&lt;h2 id=&#34;1-utilize-pytorch-dataloader&#34;&gt;1. Utilize Pytorch DataLoader&lt;/h2&gt;
&lt;p&gt;Training a large dataset is a bottleneck for your memory and you will never be able to train a complete model given the whole dataset never fits in your memory at the same time, especially for unstructured data such as image, text, voice,… However, with Pytorch DataLoader, you manage to set up various mini-batches for the whole dataset and each is loaded uninterruptedly into your model (the number of samples depends on your memory capability). You can see &lt;a href=&#34;https://pytorch.org/tutorials/beginner/data_loading_tutorial.html&#34;&gt;here&lt;/a&gt; the tutorial on using Pytorch DataLoader.&lt;/p&gt;
&lt;p&gt;However, if you want to train a Machine Learning model on tabular data without using Deep learning (hence, not using Pytorch) or you don’t have access to a Database and have to work solely on the memory, what will be the choice for memory optimization?&lt;/p&gt;
&lt;h2 id=&#34;2-optimized-data-type&#34;&gt;2. Optimized data type&lt;/h2&gt;
&lt;p&gt;By understanding how data is stored and manipulated and using the optimal data type for the tasks, it will save you huge space in memory and computation time. In Numpy, there are multiple types, including boolean (bool), integer (int), Unsigned integer (uint), float, complex, datetime64, timedelta64, object_, etc…&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
### Check numpy integer
&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; ii16 = np.iinfo(np.int16)
&amp;gt;&amp;gt;&amp;gt; ii16
iinfo(min=-32768, max=32767, dtype=int16)
### Access min value
&amp;gt;&amp;gt;&amp;gt; ii16.min
-32768

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I narrow them down to &lt;strong&gt;uint, int, and float&lt;/strong&gt; given these are the most common when training models, handling data in Python. Depending on different needs and objectives, using sufficient data types becomes vital know-how. To check type minimum and maximum values, you can use function &lt;code&gt;numpy.iinfo()&lt;/code&gt;, &lt;code&gt;and numpy.finfo()&lt;/code&gt; for float.&lt;/p&gt;
&lt;p&gt;Below is the summary information for each type.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;number_memory.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt;  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The CSV file size doubles if the data type is converted to &lt;code&gt;numpy.float64&lt;/code&gt;, which is the default type of numpy.array, compared to &lt;code&gt;numpy.float32&lt;/code&gt;. Therefore, &lt;strong&gt;float32&lt;/strong&gt; is one of the optimal ones to use (Pytorch datatype is also &lt;strong&gt;float32&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;As the default data type &lt;code&gt;numpy.float()&lt;/code&gt; is &lt;strong&gt;float64&lt;/strong&gt; and &lt;code&gt;numpy.int()&lt;/code&gt; is &lt;strong&gt;int64&lt;/strong&gt;, remember to define the dtype when creating numpy array will save a huge amount of memory space.&lt;/p&gt;
&lt;p&gt;When working with DataFrame, there will be another usual type which is &lt;strong&gt;“object”&lt;/strong&gt;. Converting from object to category for feature having various repetitions will help computation time faster.&lt;/p&gt;
&lt;p&gt;Below is an example function to optimize the pd.DataFrame data type for scalars and strings.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def data_optimize(df, object_option=False):
    &amp;quot;&amp;quot;&amp;quot;Reduce the size of the input dataframe
    Parameters
    ----------
    df: pd.DataFrame
        input DataFrame
    object_option : bool, default=False
        if true, try to convert object to category
    Returns
    -------
    df: pd.DataFrame
        data type optimized output dataframe
    &amp;quot;&amp;quot;&amp;quot;

    # loop columns in the dataframe to downcast the dtype
    for col in df.columns:
        # process the int columns
        if df[col].dtype == &#39;int&#39;:
            col_min = df[col].min()
            col_max = df[col].max()
            # if all are non-negative, change to uint
            if col_min &amp;gt;= 0:
                if col_max &amp;lt; np.iinfo(np.uint8).max:
                    df[col] = df[col].astype(np.uint8)
                elif col_max &amp;lt; np.iinfo(np.uint16).max:
                    df[col] = df[col].astype(np.uint16)
                elif col_max &amp;lt; np.iinfo(np.uint32).max:
                    df[col] = df[col].astype(np.uint32)
                else:
                    df[col] = df[col]
            else:
                # if it has negative values, downcast based on the min and max
                if col_max &amp;lt; np.iinfo(np.int8).max and col_min &amp;gt; np.iinfo(np.int8).min:
                    df[col] = df[col].astype(np.int8)
                elif col_max &amp;lt; np.iinfo(np.int16).max and col_min &amp;gt; np.iinfo(np.int16).min:
                    df[col] = df[col].astype(np.int16)
                elif col_max &amp;lt; np.iinfo(np.int32).max and col_min &amp;gt; np.iinfo(np.int32).min:
                    df[col] = df[col].astype(np.int32)
                else:
                    df[col] = df[col]
                    
        # process the float columns
        elif df[col].dtype == &#39;float&#39;:
            col_min = df[col].min()
            col_max = df[col].max()
            # downcast based on the min and max
            if col_min &amp;gt; np.finfo(np.float32).min and col_max &amp;lt; np.finfo(np.float32).max:
                df[col] = df[col].astype(np.float32)
            else:
                df[col] = df[col]

        if object_option:
            if df[col].dtype == &#39;object&#39;:
                if len(df[col].value_counts()) &amp;lt; 0.5 * df.shape[0]:
                    df[col] = df[col].astype(&#39;category&#39;)

    return df

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to easily and efficiently reduce &lt;code&gt;pd.DataFrame&lt;/code&gt; memory footprint is to import data with specific columns using &lt;code&gt;usercols&lt;/code&gt; parameters in &lt;code&gt;pd.read_csv()&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-avoid-using-global-variables-instead-utilize-local-objects&#34;&gt;3. Avoid using global variables, instead utilize local objects&lt;/h2&gt;
&lt;p&gt;Python is faster at retrieving a local variable than a global one. Moreover, declaring too many variables as global leads to Out of Memory issue as these remain in the memory till program execution is completed while local variables are deleted as soon as the function is over and release the memory space which it occupies. Read more at &lt;a href=&#34;https://towardsdatascience.com/the-real-life-skill-set-that-data-scientists-must-master-8746876d5b2e&#34;&gt;The real-life skill set that data scientists must master&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-use-yield-keyword&#34;&gt;4. Use yield keyword&lt;/h2&gt;
&lt;p&gt;Python yield returns a generator object, which converts the expression given into a generator function. To get the values of the object, it has to be iterated to read the values given to the yield. To read the generator’s values, you can use &lt;code&gt;list()&lt;/code&gt;, for loop, or &lt;code&gt;next()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; def say_hello():
&amp;gt;&amp;gt;&amp;gt;    yield &amp;quot;HELLO!&amp;quot;
&amp;gt;&amp;gt;&amp;gt; SENTENCE = say_hello()
&amp;gt;&amp;gt;&amp;gt; print(next(SENTENCE))
HELLO!

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, generators are for one-time use objects. If you access it the second time, it returns empty.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; def say_hello():
&amp;gt;&amp;gt;&amp;gt;    yield &amp;quot;HELLO!&amp;quot;
&amp;gt;&amp;gt;&amp;gt; SENTENCE = say_hello()
&amp;gt;&amp;gt;&amp;gt; print(next(SENTENCE))
HELLO!
&amp;gt;&amp;gt;&amp;gt; print(&amp;quot;calling the generator again: &amp;quot;, list(SENTENCE))
calling the generator again: []

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As there is no value returned unless the generator object is iterated, no memory is used when the Yield function is defined, while calling Return in a function leads to the allocation in memory.&lt;/p&gt;
&lt;p&gt;Hence, &lt;strong&gt;Yield&lt;/strong&gt; is suitable for large dataset, or when you don’t need to store all the output values but just one value for each iteration of the main function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; import sys
&amp;gt;&amp;gt;&amp;gt; my_generator_list = (i*2 for i in range(100000))
&amp;gt;&amp;gt;&amp;gt; print(f&amp;quot;My generator is {sys.getsizeof(my_generator_list)} bytes&amp;quot;)
My generator is 128 bytes
&amp;gt;&amp;gt;&amp;gt; timeit(my_generator_list)
10000000 loops, best of 5: 32 ns per loop
  
&amp;gt;&amp;gt;&amp;gt; my_list = [i*2 for i in range(1000000)]
&amp;gt;&amp;gt;&amp;gt; print(f&amp;quot;My list is {sys.getsizeof(my_list)} bytes&amp;quot;)
My list is 824472 bytes
&amp;gt;&amp;gt;&amp;gt; timeit(my_list)
10000000 loops, best of 5: 34.5 ns per loop

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the above code, the list comprehension is 6441 times heavier than the generator and runs slower than the other generator.&lt;/p&gt;
&lt;h2 id=&#34;5-built-in-optimizing-methods-of-python&#34;&gt;5. Built-in Optimizing methods of Python&lt;/h2&gt;
&lt;p&gt;Use Python Built-in Functions to improve code performance. Check the &lt;a href=&#34;https://docs.python.org/3/library/functions.html&#34;&gt;list&lt;/a&gt; of functions.&lt;/p&gt;
&lt;h3 id=&#34;utilize-__slots__-in-defining-class&#34;&gt;Utilize &lt;strong&gt;slots&lt;/strong&gt; in defining class&lt;/h3&gt;
&lt;p&gt;Python class objects’ attributes are stored in the form of a dictionary. Thus, defining thousands of objects is the same as allocating thousands of dictionaries to the memory space. And adding &lt;code&gt;__slots__&lt;/code&gt; (which reduces the wastage of space and speeds up the program by allocating space for a fixed amount of attributes.)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import numpy as np
import pandas as pd
import objgraph

### class without __slots__
class PointWithDict():
    def __init__(self, iters):
        self.iters = iters
    def convert(self):
        s = [&amp;quot;xyz&amp;quot;]*self.iters
        s = &amp;quot;&amp;quot;.join(s)
        assert len(s) == 3*iters
        
w_dict = PointWithDict(10000)
objgraph.show_refs([w_dict], filename=&#39;PointWithDict_structure.png&#39;)

### class using __slots__
class PointWithSlots():
    __slots__ = &amp;quot;iters&amp;quot;
    def __init__(self, iters):
        self.iters = iters
    def convert(self):
        s = [&amp;quot;xyz&amp;quot;]*self.iters
        s = &amp;quot;&amp;quot;.join(s)
        assert len(s) == 3*iters
        
w_slots = PointWithSlots(10000)
objgraph.show_refs([w_slots],filename=&#39;PointWithSlots_structure.png&#39;)

### Check memory footprint
&amp;gt;&amp;gt;&amp;gt; print(sys.getsizeof(w_dict), sys.getsizeof(w_dict.__weakref__), sys.getsizeof(w_dict.__dict__))
64 16 120
&amp;gt;&amp;gt;&amp;gt; print(sys.getsizeof(ob))
56

&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
  &lt;img src=&#34;PointWithDict_structure.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt; Structure of Point module without __slots__  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;PointWithSlots_structure.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt; Structure of Point module with __slots__, there is no __dict__ anymore &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Regarding memory usage, given there is no longer &lt;code&gt;__dict__&lt;/code&gt; in a class object, the memory space reduces noticeably from (64+16+120)=200 to 56 bytes.&lt;/p&gt;
&lt;h3 id=&#34;use-join-instead-of--to-concatenate-string&#34;&gt;Use join() instead of ‘+’ to concatenate string&lt;/h3&gt;
&lt;p&gt;As strings are immutable, every time you add an element to a string by the “+” operator, a new string will be allocated in memory space. The longer the string, the more memory consumed, the less efficient the code becomes. Using &lt;code&gt;join()&lt;/code&gt; can improve speed &amp;gt;30% vs ‘+’ operator.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
### Concatenate string using &#39;+&#39; operation
def add_string_with_plus(iters):
    s = &amp;quot;&amp;quot;
    for i in range(iters):
        s += &amp;quot;abc&amp;quot;
    assert len(s) == 3*iters
    
### Concatenate strings using join() function
def add_string_with_join(iters):
    l = []
    for i in range(iters):
        l.append(&amp;quot;abc&amp;quot;)
    s = &amp;quot;&amp;quot;.join(l)
    assert len(s) == 3*iters
    
### Compare speed
&amp;gt;&amp;gt;&amp;gt; timeit(add_string_with_plus(10000))
100 loops, best of 5: 3.74 ms per loop
&amp;gt;&amp;gt;&amp;gt; timeit(add_string_with_join(10000))
100 loops, best of 5: 2.3 ms per loop

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are other methods to improve speed and save memory, check details &lt;a href=&#34;https://wiki.python.org/moin/PythonSpeed&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;itertools&#34;&gt;itertools&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import itertools
import sys

def append_matrix_with_itertools(X, Y):
  
    &amp;quot;&amp;quot;&amp;quot; Loop matrix using itertools.product()
    &amp;quot;&amp;quot;&amp;quot;
    
    MTX = np.zeros((X, Y))
    for i, j in itertools.product(range(X), range(Y)):
        if (i%2==0) &amp;amp; (i%3==1):
            MTX[i, j] = i**2/10
            
    return MTX

def append_matrix_with_loop(X, Y):
  
    &amp;quot;&amp;quot;&amp;quot; Loop matrix using normal for loop
    &amp;quot;&amp;quot;&amp;quot;
    
     MTX = np.zeros((X, Y))
    for i in range(X):
        for j in range(Y):
            if (i%2==0) &amp;amp; (i%3==1):
                MTX[i, j] = i**2/10
    return MTX


&amp;gt;&amp;gt;&amp;gt; MTX_itertools = append_matrix_with_itertools(MTX.shape[0], MTX.shape[1])
&amp;gt;&amp;gt;&amp;gt; MTX_loop = append_matrix_with_loop(MTX.shape[0], MTX.shape[1])

### Matrix size
&amp;gt;&amp;gt;&amp;gt; print(sys.getsizeof(MTX_itertools)/ (1024 * 1024))
&amp;gt;&amp;gt;&amp;gt; print(sys.getsizeof(MTX_loop)/ (1024 * 1024))
7.6295013427734375
7.6295013427734375

### Run time
&amp;gt;&amp;gt;&amp;gt; timeit(append_matrix_with_itertools(1000, 1000))
1 loop, best of 5: 234 ms per loop
&amp;gt;&amp;gt;&amp;gt; timeit(append_matrix_with_loop(1000, 1000))
1 loop, best of 5: 347 ms per loop

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or flatten a list with &lt;code&gt;itertools.chain()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
### Concatenate string using &#39;+&#39; operation
def add_string_with_plus(iters):
    s = &amp;quot;&amp;quot;
    for i in range(iters):
        s += &amp;quot;abc&amp;quot;
    assert len(s) == 3*iters
    
### Concatenate strings using join() function
def add_string_with_join(iters):
    l = []
    for i in range(iters):
        l.append(&amp;quot;abc&amp;quot;)
    s = &amp;quot;&amp;quot;.join(l)
    assert len(s) == 3*iters
    
### Compare speed
&amp;gt;&amp;gt;&amp;gt; timeit(add_string_with_plus(10000))
100 loops, best of 5: 3.74 ms per loop
&amp;gt;&amp;gt;&amp;gt; timeit(add_string_with_join(10000))
100 loops, best of 5: 2.3 ms per loop

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out &lt;a href=&#34;https://docs.python.org/3/library/itertools.html#module-itertools&#34;&gt;itertools documentation&lt;/a&gt; for more methods. I recommend exploring:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.accumulate(iterable , func)&lt;/code&gt;: accumulate through iterable. func can be an operator.func or default Python functions such as max, min…&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.compress(iterable, selectors)&lt;/code&gt;: filters the iterable with another (the other object can be treated as a condition)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.filterfalse(predicate, iterable)&lt;/code&gt;: filter and drop values that satisfy the predicate. This is useful and fast for filtering a list object.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.repeat(object[, times])&lt;/code&gt;: repeat a object value N times. However, I prefer using list multiplication &lt;code&gt;[&#39;hi&#39;]*1000&lt;/code&gt; to repeat ‘hi’ 1000 times than using &lt;code&gt;itertools.repeat(&#39;hi&#39;, 1000)&lt;/code&gt; (12.2 µs per loop vs 162 µs per loop respectively)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;itertools.zip_longest(*iterables, fillvalue=None)&lt;/code&gt;: zip multiple iterables into tuples and fill None value with the value specified in fillvalue.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;6-import-statement-overhead&#34;&gt;6. Import Statement Overhead&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;import&lt;/code&gt; statement can be executed from anywhere. However, executing outside of a function will run much faster than the inside one, even though the package is declared as a global variable (doit2), but in return, takes up more memory space than the other one.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;figure.png&#34; alt=&#34;&#34; style=&#34;width:80%&#34;&gt;
  &lt;figcaption&gt; Comparison of import execution position &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;7-data-chunk&#34;&gt;7. Data chunk&lt;/h2&gt;
&lt;p&gt;I’m confident to say that most of the time you don’t use all of your data all at once and loading them in 1 big batch may crash the memory. Therefore, chunking data or load in small chunks, process the chunk of data, and save the result is one of the most useful techniques to prevent memory error.&lt;/p&gt;
&lt;p&gt;pandas lets you do that with the help of &lt;code&gt;chunksize&lt;/code&gt; or &lt;code&gt;iterator&lt;/code&gt; parameters in &lt;code&gt;pandas.read_csv()&lt;/code&gt; and &lt;code&gt;pandas.read_sql()&lt;/code&gt;. sklearn also supports training in small chunks with &lt;code&gt;partial_fit()&lt;/code&gt; method for most models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&amp;gt;&amp;gt;&amp;gt; from sklearn.linear_model import SGDRegressor
&amp;gt;&amp;gt;&amp;gt; from sklearn.datasets import make_regression
&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; import pandas as pd

&amp;gt;&amp;gt;&amp;gt; ### Load original data
&amp;gt;&amp;gt;&amp;gt; original_data = pd.read_csv(&#39;sample.csv&#39;)
&amp;gt;&amp;gt;&amp;gt; print(f&#39;Shape of original data {original_data.shape:.f02}&#39;)
Shape of original data (100000, 21)


&amp;gt;&amp;gt;&amp;gt; ### Load in chunk
&amp;gt;&amp;gt;&amp;gt; chunksize = 1000
&amp;gt;&amp;gt;&amp;gt; reg = SGDRegressor()
&amp;gt;&amp;gt;&amp;gt; features_columns = [str(i) for i in range(20)]

&amp;gt;&amp;gt;&amp;gt; ### Fit each chunk
&amp;gt;&amp;gt;&amp;gt; for train_df in pd.read_csv(&amp;quot;sample.csv&amp;quot;, chunksize=chunksize, iterator=True):
&amp;gt;&amp;gt;&amp;gt;     X = train_df[features_columns]
&amp;gt;&amp;gt;&amp;gt;     Y = train_df[&amp;quot;target&amp;quot;]
&amp;gt;&amp;gt;&amp;gt;     reg.partial_fit(X, Y)

### The reg.partial_fit() method fit each chunk at a time and update weights accordingly after each the next chunk is loaded 

&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;take-aways-&#34;&gt;Take-aways 📖&lt;/h1&gt;
&lt;p&gt;Handling Memory Error in Python is a tricky task and the root cause might be never been detected if you go another way around.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, investigate which process/variable is the core reason leading to the memory overflown problem.
Second, apply the applicable memory optimization methods for that object, estimate the new memory footprint and examine if the new one solves the issue.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If not, try to optimize related processes (such as reducing whole process memory consumption to save more space for the core object).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try the above tips and if the trouble remains, consider building the process with chunk/batch operation with the support of an outside database service.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Quick deep dive at Data Scientist Skill set</title>
      <link>https://geniusnhu.netlify.com/2020/01/17/quick-deep-dive-at-data-scientist-skill-set/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://geniusnhu.netlify.com/2020/01/17/quick-deep-dive-at-data-scientist-skill-set/</guid>
      <description>&lt;p&gt;One year ago, when I truly and seriously considered improving my skill in Data Science, two questions were always lingering in my mind:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frankly saying, what are the skills that a Data Scientist needs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What skill will support me in the future and How do I improve myself in the right direction at the most efficacy?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moreover, some friends, acquaintances of mine reached out to me for a thought of what they should learn to develop their career as a Data Scientist.&lt;/p&gt;
&lt;p&gt;Actually, I can share some of my experiences with them, but as you&amp;rsquo;ve already known, this field evolves unpreceedingly fast, technology and new required skills change on the yearly basis at the slowest.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;What you learn today will be the old of tomorrow!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Luckily, when I was known of the Kaggle dataset on Survey of Data Science and Machine learning, this data will definitely give me some insights for my questions.&lt;/p&gt;
&lt;p&gt;The Data source is &lt;a href=&#34;https://www.kaggle.com/c/kaggle-survey-2019&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this post, I will summary my key findings from the survey. The complete analysis can be found through this &lt;a href=&#34;https://www.kaggle.com/geninhu/direction-toward-a-great-data-scientist/data&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This analysis uses R language with tidyverse package for the best insight visualization.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Before looking at the result, below are the data cleaning and preparation for the analysis.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Feature selection:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This dataset includes the written answer of the respondents if they picked &lt;code&gt;&amp;quot;Other&amp;quot;&lt;/code&gt; choice for some questions. However, the written answers are stored in another &lt;em&gt;csv&lt;/em&gt; file so that all the variables containing &lt;code&gt;&amp;quot;Other&amp;quot;&lt;/code&gt; will not a valuable variable for the analysis, therefore they will be excluded from the dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For this analysis, the &lt;code&gt;&amp;quot;Duration&amp;quot;&lt;/code&gt; variable is not the factor that I want to explore so it will be excluded as well.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Other data cleaning:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Shorten some typical column names for easier understanding.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set level for values in the variables that require level later on, such as &lt;em&gt;Company size, Compensation/Salary, year of experience with machine learning&lt;/em&gt;&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Create functions for plotting (Example is as below)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_df_ds &amp;lt;- function (df_draw_ds, columnName1, columnName2) {
    names(df_draw_ds)[columnName1] &amp;lt;- paste(&amp;quot;value&amp;quot;)
    names(df_draw_ds)[columnName2] &amp;lt;- paste(&amp;quot;value2&amp;quot;)
    df_draw_ds &amp;lt;- df_draw_ds %&amp;gt;% 
    select (value, value2) %&amp;gt;%
    group_by(value, value2) %&amp;gt;%
    filter(value != &amp;quot;Not employed&amp;quot;,value != &amp;quot;Other&amp;quot;) %&amp;gt;% 
    summarise(count=n()) %&amp;gt;% 
    mutate(perc= prop.table(count))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Now, let&amp;rsquo;s explore the result.&lt;/p&gt;
&lt;h3 id=&#34;role-and-responsibilities&#34;&gt;&lt;strong&gt;Role and Responsibilities:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A Data scientist is responsible for many tasks but the Top 3 includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Analyzing data&lt;/strong&gt; to influence product or to support business decisions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explore new areas of Machine learning&lt;/strong&gt; through builidng prototypes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Experiment to &lt;strong&gt;improve company existing Machine learning models&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/responsibility.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;programming-skills&#34;&gt;&lt;strong&gt;Programming skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Python, SQL and R&lt;/strong&gt; are all the programming languages that are essential for Data Scientist as they stand in the Top 3 respectively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/PL.png&#34; width=&#34;300&#34; height=&#34;450&#34; &gt;&lt;/p&gt;
&lt;h3 id=&#34;machine-learning-skills&#34;&gt;&lt;strong&gt;Machine Learning skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;It is not a surprise if all Data Scientists use Machine Learning in their daily job but I was amazed that almost all of them use both Natural Language Processing and Computer Vision. These 2 fields has no longer been specified areas to some groups of users but expanded to much wider application and require Data Scientist to own these skills.&lt;/p&gt;
&lt;h3 id=&#34;machine-learning-algorithm-skills&#34;&gt;&lt;strong&gt;Machine Learning algorithm skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt; and &lt;strong&gt;Tree-based algorithms&lt;/strong&gt; are the models used by Data Scientist. With the long history in statistics and analytics, these models are still being favor in analyzing data. Another advantage of these traditional models is that they are easy to explain to business partner. Other models such as Neuron network, or Deep learning is hard to explain the result.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/ML.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another interesting thing is that 51% of Data scientists is applying &lt;strong&gt;Boosting method&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;python-skills&#34;&gt;&lt;strong&gt;Python skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Scikit-learn, Keras, Xgboost, TensorFlow and RandomForest libraries&lt;/strong&gt; are the top used frameworks given their benefit and convenience.&lt;/p&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/package.png&#34; width=&#34;300&#34; height=&#34;450&#34;&gt;
&lt;h3 id=&#34;data-scientist-credential&#34;&gt;&lt;strong&gt;Data Scientist Credential:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Foundation skills&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Writing code&lt;/strong&gt; is an important skill of a good Data scientist. Being an excellent coder is not required but preferable to have, especially big companies have the tendency to hire Data Scientist Engineer with coding skill.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is essential to &lt;strong&gt;build up your business skills&lt;/strong&gt; in addition to technical skills. Many data scientists, especially the junior may excel at machine learning and algorithm but usually cry out for business point of view. Their recommendations are getting off the track from what the company is doing. That is the worst thing you want to face in the world of Data science when no one values your idea..&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Specialized skills&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Build expertise in &lt;strong&gt;Python, SQL and/or R&lt;/strong&gt; on various online/offline platforms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fluency in using Local development environments and Intergrated development Environment, including but not limited to Jupyter, RStudio, Pycharm. Try to &lt;strong&gt;learn on job&lt;/strong&gt; or &lt;strong&gt;learn on practice&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Strong in foundation and rich in practical experience&lt;/strong&gt;. Develop side projects within your interested fields with proper models showcase. The model can be either traditional Linear regression, Decision Tree/Random Forest, or cutting-edge XGBoost, Recurrent Neural Network.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enrich your knowledge and application in convenient frameworks&lt;/strong&gt; such as Scikit-learn, Keras, Tensorflow, Xgboost. Working on your own project/cooperating project is a good choice to get practical experience if you have not obtained the chance in a company.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Computer Visio&lt;/strong&gt;n and &lt;strong&gt;NLP&lt;/strong&gt; are the booming areas in Data science so it is beneficial to prepare yourself with these skills.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although &lt;strong&gt;AutoML&lt;/strong&gt; is a newly emerging field, it will probably become one of the important tools and skills for Data Scientist, including Automated hyperparameter tuning, Data augmentation, Feature engineering/selection and Auto ML pipelines.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Skills in working with &lt;strong&gt;Big data&lt;/strong&gt; and Big data products e.g Google Bigquerry, Databricks, Redshift are the must to own.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is the same with usage of &lt;strong&gt;cloud computing skills&lt;/strong&gt;. In big company, it is a common to work on cloud so if you do not know how to conduct machine learning on cloud, it will become your minus comparing to other candidates.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And one last important thing: &lt;strong&gt;Always believe in yourself, your choice and never give up&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;end-notes&#34;&gt;End notes&lt;/h4&gt;
&lt;p&gt;As the big survey focusing on the Data science / Machine Learning areas, it appeared as a great source for me to gain valuable information.&lt;/p&gt;
&lt;p&gt;Beside understanding which typical skills requiring to advance in the Data Science field, I want to tbuild a model to predict the salary range and I will update on that in upcoming days.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
