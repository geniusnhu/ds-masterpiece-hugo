<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nhu Hoang</title>
    <link>/authors/admin/</link>
      <atom:link href="/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Nhu Hoang</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Nhu Hoang</copyright><lastBuildDate>Wed, 12 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Nhu Hoang</title>
      <link>/authors/admin/</link>
    </image>
    
    <item>
      <title>The beauty of Transformer in bringing more applications to life</title>
      <link>/publication/the-beauty-of-transformer-in-bringing-more-applications-to-life/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/publication/the-beauty-of-transformer-in-bringing-more-applications-to-life/</guid>
      <description>&lt;p&gt;My 2020 started awesomely with a Machine Learning Forum @Google Japan and followed with 2 Meetups about Natural Language Processing application. NLP has been the focus of the Machine Learning society for the last decade and it is reaching to its ultimate point with several outbreak of innovations and application.&lt;/p&gt;
&lt;p&gt;At this moment, 2020, NLP is heading towards &lt;strong&gt;speed and big data&lt;/strong&gt;, which means that the increasing of the speed and size of data is the key objective of future NLP innovations. At the moment, I am impressed with &lt;strong&gt;BERT - Bidirectional Encoder Representations from Transformers&lt;/strong&gt;, a powerful state-of-the-art NPL model introduced by Google in 2018; and *&lt;strong&gt;Google Meena&lt;/strong&gt;, a lift toward dealing with big-NLP database using the Transformer base introducted on Jan 28, 2020.&lt;/p&gt;
&lt;h3 id=&#34;1why-are-bert-and-transformer-being-called-the-revolution-of-the-nlp-world&#34;&gt;1.	Why are BERT and Transformer being called the revolution of the NLP world?&lt;/h3&gt;
&lt;p&gt;To understand &lt;strong&gt;Transformer model&lt;/strong&gt; inclduing BERT, we need to take a look of the progress from &lt;strong&gt;Seq2Seq (sequence to sequence)&lt;/strong&gt; and its evolution to attention and to BERT.&lt;/p&gt;
&lt;h4 id=&#34;11-seq2seq-model&#34;&gt;1.1 Seq2Seq model:&lt;/h4&gt;
&lt;p&gt;In NLP, the end that a machine is expected to understand is the meaning of the sentence, not only word by word. &lt;strong&gt;Seq2Seq&lt;/strong&gt; is a technique to train the machine in which it takes a sequence of an item and generates another sequence as output.&lt;/p&gt;
&lt;p&gt;Within the model, there contains &lt;strong&gt;Encoder&lt;/strong&gt; and &lt;strong&gt;Decoder&lt;/strong&gt;. The &lt;strong&gt;Encoder&lt;/strong&gt; receives the original text and convert the text into a &lt;strong&gt;Context vector&lt;/strong&gt; that the Machine can read. Then, the &lt;strong&gt;Decoder&lt;/strong&gt; does the job of generating a new sequence of items based on the &lt;strong&gt;Context vector&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In Seq2Seq, a sentence does not need to go through both Encoder and Decoder, it can stop at Encoder. Some example of Encoder only is the suggested word &lt;em&gt;&amp;ldquo;message&amp;rdquo;&lt;/em&gt; after you type &lt;em&gt;&amp;ldquo;Thank you for your&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt; or &lt;strong&gt;Context vector&lt;/strong&gt; is a vector of floats representing the input sequence. &lt;strong&gt;&amp;ldquo;Word Embedding&amp;rdquo;&lt;/strong&gt; is the algorithm used to transform text into vector, and the size of vector is usually 265, 512 or 1024 dimensions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Seq2Seq_visual.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;h4 id=&#34;12-attention&#34;&gt;1.2 Attention&lt;/h4&gt;
&lt;p&gt;One of the disadvantages of Context in Seq2Seq is the &lt;strong&gt;dealing with long sentences and handling the sequence of output&lt;/strong&gt;. Context is generated by the Word embedding algorithm and the longer the sentence or the paragraph, the bigger the size of vector and the more memory consuming.&lt;/p&gt;
&lt;p&gt;Moreover, the context in Seq2Seq &lt;strong&gt;was not built to figure out the similarity between words&lt;/strong&gt; because it does not focus on the &lt;strong&gt;relevancy of the words in the sentence&lt;/strong&gt;. This leads to the issue that a sentence in English cannot be translated correctly to Japanese which has the reserve order in sentence structure.&lt;/p&gt;
&lt;p&gt;The concept of Attention was introduced in &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Bahdanau et al., 2014&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1508.04025&#34;&gt;Luong et al., 2015&lt;/a&gt; in which it takes into account the relevant parts in the sentence.&lt;/p&gt;
&lt;p&gt;Instead of passing the last hidden state to the &lt;strong&gt;Decoder&lt;/strong&gt;, the Attention model &lt;strong&gt;passes all the hidden states to the Decoder&lt;/strong&gt; with the summary process as below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Give each hidden state a score&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Softmax function&lt;/strong&gt; to multiply each hidden state. This brings about high Hidden state scores and low hidden state scores or in other word, it generates the probability of each hidden state associating with the input word.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Decoder&lt;/strong&gt; will sum up all the weighted softmax Hidden state vectors into a context vector and concatenate it with its original hidden state vector.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Attention_Visual.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The advantage of this model is the capability to &lt;strong&gt;choose the decoding word based on the probability of that word in associating with the original input without losing the sequential characteristics of the sentence&lt;/strong&gt;. This has high effectiveness in dealing with translating of common words such as &amp;ldquo;the&amp;rdquo;, &amp;ldquo;his&amp;rdquo;, &amp;ldquo;of&amp;rdquo;, etc. and the sequence of different languages.&lt;/p&gt;
&lt;h4 id=&#34;13-transformer&#34;&gt;1.3 Transformer:&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; is built on the foundation of &lt;strong&gt;Attention&lt;/strong&gt; model. Therefore, &lt;strong&gt;Transformer&lt;/strong&gt; can deal with the relevancy of the sentence rather than converting from word to word in &lt;strong&gt;Seq2Seq&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The biggest difference of &lt;strong&gt;Transformer&lt;/strong&gt; vs &lt;strong&gt;Seq2Seq&lt;/strong&gt; is that instead of generating 1 vector from &lt;strong&gt;Encoder&lt;/strong&gt;, &lt;strong&gt;Transformer&lt;/strong&gt; model uses 3 vectors in order to decide which other parts of the sentence are important (or unimportant) to that word.&lt;/p&gt;
&lt;p&gt;The below table show more details of the calculation.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Word&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Q vector&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;K vector&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;V vector&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Softmax&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Sum&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;First word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q1xK1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S1=Q1xK1/8xV1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;love&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q1xK2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S2=Q1xK2/8xV2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Z1 = S1+S2+S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q1xK3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S3=Q1xK3/8xV3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Second word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q2xK1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S1=Q2xK1/8xV1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;love&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q2xK2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S2=Q2xK2/8xV2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Z2 = S1+S2+S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q2xK3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S3=Q2xK3/8xV3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Third word&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;I&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q3xK1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S1=Q3xK1/8xV1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;love&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q3xK2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S2=Q3xK2/8xV2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Z3 = S1+S2+S3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;K3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;V3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Q3xK3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;S3=Q3xK3/8xV3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;One example of this model application is the suggestion of relevant words when typing sentence. Gmail can suggest &lt;strong&gt;&amp;ldquo;message&amp;rdquo;, &amp;ldquo;reply&amp;rdquo;, &amp;ldquo;call&amp;rdquo;&lt;/strong&gt; at the same time based on the typed sentence &lt;strong&gt;&amp;ldquo;Thank you for your&lt;/strong&gt;&amp;quot;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a brief introduction on the transition from Seq2Seq to Transformer and how the &lt;strong&gt;Transformer model&lt;/strong&gt; outstands &lt;strong&gt;Seq2Seq&lt;/strong&gt; at the moment.&lt;/p&gt;
&lt;h3 id=&#34;2-business-applications-of-nlp&#34;&gt;2. Business Applications of NLP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chatbot&lt;/strong&gt;: This is obviously the forefront application of NLP, which can be seen across all industries and companies. Given its popularity, there are several tools to support building a chatbot such as Google DialogFlow, Microsoft LUIS. These tools can be customized based on the user&#39;s needs; however, they can only deal with simple requests.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Machine translation&lt;/strong&gt;: such as Google translate or pocket translator device.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Search engine&lt;/strong&gt;: 5 years ago, when you searched something on search engine, whether you type key words &amp;ldquo;to Tokyo&amp;rdquo; or the whole sentences &amp;ldquo;How to go to Tokyo&amp;rdquo;, the machine would generate quite similar results. However, with the evolution of BERT and Transformer, searching the whole sentence will throw you to a better search result.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monitoring of brand and product - Sentiment analysis:&lt;/strong&gt; This is the field that I used to analyze during my first job. I used sentiment analysis on big scale online platforms including online forums, social network, brand website and e-commerce sites to understand the reaction of consumers toward a campaign or a brand in order to react promptly toward negative trend related to the brand.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Place to display an advertisement&lt;/strong&gt;: display ads based on context or categorization and make sure that the article is appropriate at the placing place. Honestly saying, I have not seen much of this application around.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Remarketing&lt;/strong&gt;: an online advertisement based on the browsing history of a user to target them with similar advertising product to drive them back the previous interest. This personalized application is a very effective tool in today online market in which thousands of sellers trying to attract each of their customers. Youtube, Facebook or Google are the biggest applicators.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Medical assistant&lt;/strong&gt;: although called &amp;ldquo;assistant&amp;rdquo;, the major task of this service is to transcript the discussion between doctors and patients.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text generation&lt;/strong&gt;: this is one of the applications of Decoding in NLP, in which the machine will generate a complete article from what it was learnt or summarize a paragraph. As you may know, there are many contradictions about this application, especially the emergence of fake news in recent years. With the completion of this technology, whether the fake news issue continues its expansion or is stopped is still a big question.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Information extraction&lt;/strong&gt;: extract dynamic required information that is sleeping in the database system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resume reviews&lt;/strong&gt;: use NLP to scan the applicants&amp;rsquo; resume to figure out potential candidate for the interview. This application sticks with Amazon big scandal. Amazon used to use this to scan the resume which led to the inequality between male and female with the result preferred male than female. This is due to 0the bias toward male in the training set of the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Voice command&lt;/strong&gt;: an emerging technology in recent years with the appearance of smart device such as Siri in Iphone, Alexa of Amazone, Google home or Cortana of Microsoft.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beside the major technological trends mentioned above, the application trend is heading toward diversity in languages and translation efficacy. Moreover, not only applying NLP alone, there are more applications combining NLP and Voice recognition or Computer Vision.&lt;/p&gt;
&lt;p&gt;NLP is a powerful Machine Learning area and its application is supporting human&#39;s life even more than we can expect. Therefore, NLP is one of the most used Machine Learning fields by Data Scienctist.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comprehensive understanding on Time Series forecasting</title>
      <link>/2020/02/06/comprehensive-understanding-on-time-series-forecasting/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/2020/02/06/comprehensive-understanding-on-time-series-forecasting/</guid>
      <description>&lt;h2 id=&#34;1-what-is-time-series&#34;&gt;1. What is Time Series&lt;/h2&gt;
&lt;p&gt;Time series is a sequence of value corresponding with time. Retail sales data, Daily temperature, production, demand, natural reserves are time series data because the later values depend on their historical values.&lt;/p&gt;
&lt;h2 id=&#34;2-what-makes-up-time-series&#34;&gt;2. What makes up Time Series&lt;/h2&gt;
&lt;p&gt;There are 4 components in Time Series: Level, Trend, Seasonality and Noise.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Level&lt;/strong&gt;: the average value of the time series&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trend&lt;/strong&gt;: The movement of the series values from 1 period to another period&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seasonality&lt;/strong&gt;: The short-term cyclical behavior of the series that can be observed several times&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Noise&lt;/strong&gt;: the random variation that results from the measurement of error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is not always that we will be able to distinguish the first 3 elements from Noise because they are usually invisble which need some techniques to be noticeable&lt;/p&gt;
&lt;p&gt;To observe and identify the existence of these components, we can consider.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plot the Time series (this is the best way to detect the characteristics of the series)&lt;/li&gt;
&lt;li&gt;Zoom in a specify shorter period of time&lt;/li&gt;
&lt;li&gt;Change scale of the series to observe the trend more clearly&lt;/li&gt;
&lt;li&gt;Suppress seasonality: aggregate the time series to a bigger time scale (from hourly scale to daily scale, from monthly scale to yearly scale, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I used 3-year weekly sales of a Retail store as an illustration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(&amp;quot;Date&amp;quot;, &amp;quot;Weekly_Sales&amp;quot;, data=Wal_sales)
plt.hlines(y=Wal_sales.Weekly_Sales.mean(), xmin=0, xmax=len(Wal_sales), linestyles=&#39;dashed&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;TS plot.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this data, there are obviously 2 peaks which denotes quite a clear &lt;strong&gt;seasonality at the end of year&lt;/strong&gt; (probably Christmas and New year period). There might be other sesonality but it is hard to observe it from the plot. &lt;strong&gt;Auto correlation&lt;/strong&gt; can be used to confirm the seasonality.&lt;/p&gt;
&lt;h2 id=&#34;3-autocorrelation&#34;&gt;3. Autocorrelation&lt;/h2&gt;
&lt;p&gt;Autocorrelation describes the &lt;strong&gt;connection between the value of time series  and its neighbors&lt;/strong&gt;. Thus, to compute Autocorrelation, we calculate the correlation of the series with its &lt;strong&gt;lagged versions&lt;/strong&gt;. Lag-n version is produced from the original dataset by moving the series values forward n period. For example, lag-1 is moved forward 1 period, Lag-10 series is moved forward 10 periods.&lt;/p&gt;
&lt;p&gt;By observing the correlation of the series and its lags, we can confirm the seasonality of the series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_cor = sales.groupby(&amp;quot;Date&amp;quot;)[&amp;quot;Weekly_Sales&amp;quot;].sum()
auto_cor = pd.DataFrame(auto_cor)
auto_cor.columns = [&amp;quot;y&amp;quot;]

# Adding the lag of the target variable from 1 steps back up to 52 (due to a seasonality at the end of the year)
for i in range(1, 53):
    auto_cor[&amp;quot;lag_{}&amp;quot;.format(i)] = auto_cor.y.shift(i)

# Compute autocorrelation of the series and its lags
lag_corr = auto_cor.corr()
lag_corr = lag_corr.iloc[1:,0]
lag_corr.columns = [&amp;quot;corr&amp;quot;]
order = lag_corr.abs().sort_values(ascending = False)
lag_corr = lag_corr[order.index]

# Plot the Autocorrelation
plt.figure(figsize=(12, 6))
lag_corr.plot(kind=&#39;bar&#39;)
plt.grid(True, axis=&#39;y&#39;)
plt.title(&amp;quot;Autocorrelation&amp;quot;)
plt.hlines(y=0, xmin=0, xmax=len(lag_corr), linestyles=&#39;dashed&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Autocorrelation.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Judging from the Autocorrelation plot above, there is a strong positive autocorrelation in lag-52 as well as lag-51 as we expected when observing the time series plot. This implies a &lt;strong&gt;cyclical annual pattern at the end of the year&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The second strong correlation is lag-1, which connotes as the second week of february, or Valentine period.&lt;/li&gt;
&lt;li&gt;The autocorrelation reveals both &lt;strong&gt;Positive&lt;/strong&gt; and &lt;strong&gt;Negative&lt;/strong&gt; autocorrelation, which implies that the series does not move in the same direction but ups and downs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Autocorrelation can plotted easily through using &lt;em&gt;autocorrelation_plot&lt;/em&gt; function from &lt;em&gt;pandas.plotting&lt;/em&gt; in Python or &lt;em&gt;acf&lt;/em&gt; function from &lt;em&gt;tseries&lt;/em&gt; package in R.&lt;/p&gt;
&lt;h2 id=&#34;4-forecasting-time-series&#34;&gt;4. Forecasting Time series&lt;/h2&gt;
&lt;p&gt;There are several methods to forecast Time series.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Model-based method through multiple linear regression&lt;/strong&gt; to explore the correlation of the series with other features. Alike other cross-sessional data, model-based method compute the dependence of the time series to other features, but does not take into account the dependence between time series values within different periods.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data-driven method in which learns the pattern from the data itself&lt;/strong&gt; and estimate the next value of the time series in correspondence with its previous values. The data-driven method is important in time series given in the time series context, the values in adjoining period tend to be correlated with each other. Such correlation is denoted as &lt;strong&gt;Autocorrelation&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combining method by forecasting the future values of the series as well as the future value of residual that generated from the first forecasting model&lt;/strong&gt;, and then combine the result of 2 forecast together. The residual forecast acts as the correct for the first forecast.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ensembles method&lt;/strong&gt; by averaging multiple methods to get the result&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;forecasting-using-data-driven-method&#34;&gt;Forecasting using Data-driven method:&lt;/h3&gt;
&lt;p&gt;ARIMA model is the most frequent choice to compute data-driven forecasting. You can find detail for ARIMA model in this &lt;a href=&#34;https://geniusnhu.netlify.com/publication/arima-autoregressive-intergreated-moving-average/&#34;&gt;post&lt;/a&gt;. 
Here I will apply the ARIMA to the data.&lt;/p&gt;
&lt;p&gt;It is useful to use *:auto_arima**function from &lt;strong&gt;pmdarima&lt;/strong&gt; in Python or &lt;strong&gt;auto.arima&lt;/strong&gt; function from &lt;strong&gt;forecast&lt;/strong&gt; packgage in R.&lt;/p&gt;
&lt;p&gt;There is one thing to note is that from the Autocorrelation above, there is a clear seasonality at lag 52 so we will need to include this into the ARIMA model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stepwise_model = pm.auto_arima(Wal_sales.iloc[:,1].values, start_p=1, start_q=1,
                               max_p=20, max_q=20, m=52,
                               start_P=0, seasonal=True,
                               d=1, D=1, trace=True,
                               error_action=&#39;ignore&#39;,  
                               suppress_warnings=True, 
                               stepwise=True)
print(stepwise_model.aic())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Performing stepwise search to minimize aic
Fit ARIMA: (1, 1, 1)x(0, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 0)x(0, 1, 0, 52) (constant=True); AIC=2898.191, BIC=2903.190, Time=0.423 seconds
Fit ARIMA: (1, 1, 0)x(1, 1, 0, 52) (constant=True); AIC=2883.839, BIC=2893.839, Time=5.555 seconds
Fit ARIMA: (0, 1, 1)x(0, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 0)x(0, 1, 0, 52) (constant=False); AIC=2907.540, BIC=2910.039, Time=0.371 seconds
Fit ARIMA: (1, 1, 0)x(0, 1, 0, 52) (constant=True); AIC=2893.265, BIC=2900.764, Time=0.807 seconds
Fit ARIMA: (1, 1, 0)x(2, 1, 0, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (1, 1, 0)x(1, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (1, 1, 0)x(0, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (1, 1, 0)x(2, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 0)x(1, 1, 0, 52) (constant=True); AIC=2890.759, BIC=2898.258, Time=7.666 seconds
Fit ARIMA: (2, 1, 0)x(1, 1, 0, 52) (constant=True); AIC=2884.464, BIC=2896.963, Time=7.595 seconds
Fit ARIMA: (1, 1, 1)x(1, 1, 0, 52) (constant=True); AIC=2884.895, BIC=2897.394, Time=20.608 seconds
Fit ARIMA: (0, 1, 1)x(1, 1, 0, 52) (constant=True); AIC=2883.040, BIC=2893.039, Time=6.410 seconds
Fit ARIMA: (0, 1, 1)x(0, 1, 0, 52) (constant=True); AIC=2893.770, BIC=2901.269, Time=5.440 seconds
Fit ARIMA: (0, 1, 1)x(2, 1, 0, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 1)x(1, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 1)x(2, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 2)x(1, 1, 0, 52) (constant=True); AIC=2887.816, BIC=2900.315, Time=7.108 seconds
Fit ARIMA: (1, 1, 2)x(1, 1, 0, 52) (constant=True); AIC=2889.929, BIC=2904.928, Time=17.358 seconds
Total fit time: 79.418 seconds
2883.039997060003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This fuction chose the lowest AIC-score model and embed it for further model usage.&lt;/p&gt;
&lt;p&gt;Split train-test set, train the model and make prediction.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Split train and test
train = Wal_sales.iloc[:106,1].values
test = Wal_sales.iloc[106:,1].values
# Train the model
stepwise_model.fit(train)

# Predict test set
pred = stepwise_model.predict(n_periods=37)

# Reframe the data
test_pred = Wal_sales.iloc[106:,:2]
test_pred[&amp;quot;Predict_sales&amp;quot;] = np.array(pred,dtype=&amp;quot;float&amp;quot;)

# Visualize the prediction
plt.figure(figsize=(12,8))
plt.plot( &#39;Date&#39;, &#39;Weekly_Sales&#39;, data=Wal_sales, markersize=12, color=&#39;olive&#39;, linewidth=3)
plt.plot( &#39;Date&#39;, &#39;Predict_sales&#39;, data=test_pred, marker=&#39;&#39;, color=&#39;blue&#39;, linewidth=3)
plt.title(&amp;quot;Predicted sales vs Actual sales&amp;quot;)
plt.legend()

print(&amp;quot;MAPE score: &amp;quot;, mean_absolute_percentage_error(test, pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;ARIMA_forecast.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In overal, the model seem to work moderately on the data but there is still room to improve further.&lt;/li&gt;
&lt;li&gt;The MAPE (mean absolute percentage error) score is 5.7%, which is not too high, not too low.&lt;/li&gt;
&lt;li&gt;The ARIMA model seems to perform well in early predicted value and gets worse in later predicted values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;one-question-emerged-does-this-model-truly-capture-the-values-of-the-time-series-data&#34;&gt;One question emerged, does this model truly capture the values of the time series data?&lt;/h4&gt;
&lt;p&gt;It is helpful to take a look at the &lt;strong&gt;Residual&lt;/strong&gt; of the model (or the &lt;strong&gt;diference between predicted values and actual values&lt;/strong&gt;). Examining the residuals of the forecasting model is suggested to evaluate whether the specified model has adequately captured the information of the data. This can be done through exploring the correlation of one period&#39;s residual with other periods&amp;rsquo; ones.&lt;/p&gt;
&lt;p&gt;The Residuals of a &lt;strong&gt;good time series forecasting model&lt;/strong&gt; have the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Residuals are &lt;strong&gt;uncorrelated&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Residuals have &lt;strong&gt;zero or nearly-zero mean&lt;/strong&gt; (which means the model is unbiased in any directions)&lt;/li&gt;
&lt;li&gt;Residuals should have &lt;strong&gt;normal distribution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Residuals should have &lt;strong&gt;constant variance&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the result is lack of any of the above attributes, the forecasting model can be further improved.&lt;/p&gt;
&lt;p&gt;Let&#39;s compute the Residuals Autocorrelation and judge the result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute Residual
train_pred = stepwise_model.predict(n_periods=106)
r_train = train - train_pred
r_test = test - pred
residual = pd.DataFrame(np.concatenate((r_train,r_test)), columns={&amp;quot;y&amp;quot;})


# Generate lag of Residuals from 1 step to 52 steps
# Adding the lag of the target variable from 1 steps back up to 52 
for i in range(1, 53):
    residual[&amp;quot;lag_{}&amp;quot;.format(i)] = residual.y.shift(i)

# Compute correlation of the Residual series and its lags
lag_corr = residual.corr()
lag_corr = lag_corr.iloc[1:,0]
lag_corr.columns = [&amp;quot;corr&amp;quot;]
order = lag_corr.abs().sort_values(ascending = False)
lag_corr = lag_corr[order.index]

# Plot the Residual Autocorrelation
plt.figure(figsize=(12, 6))
lag_corr.plot(kind=&#39;bar&#39;)
plt.grid(True, axis=&#39;y&#39;)
plt.title(&amp;quot;Autocorrelation&amp;quot;)
plt.hlines(y=0, xmin=0, xmax=len(lag_corr), linestyles=&#39;dashed&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;residual_autocorrelation.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Other criteria:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Residual mean and Distribution
print(&amp;quot;Residual mean: &amp;quot;,residual.iloc[:,0].mean())
plt.hist(residual.iloc[:,0], bins=20)
plt.title(&amp;quot;Residual Distribution&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Residual mean:  -6308833.905274585
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;residual_distribution.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Residual variance
plt.plot(residual.iloc[:,0])
plt.title(&amp;quot;Residual&amp;quot;)
plt.hlines(y=0, xmin=0, xmax=len(residual), linestyles=&#39;dashed&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;residual_variance.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&#39;s judge the Autocorrelation of Residual based on the following criteria:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Residuals are uncorrelated: the Residual series is still observed some correlations with its lags.&lt;/li&gt;
&lt;li&gt;Residuals have zero or nearly-zero mean (which means the model is unbiased in any directions): the mean is -6308833.905274585. So this criteria is not met.&lt;/li&gt;
&lt;li&gt;Residuals should have normal distribution: Not quite a normal distribution&lt;/li&gt;
&lt;li&gt;Residuals should have constant variance: No as consistent with mean does not equal to 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, the forecasting model has a lot of rooms to improve further by finding the way to capture the correlation in the Residuals, adding the values that is currently staying in residuals to the prediction.&lt;/p&gt;
&lt;h2 id=&#34;5-data-partitioning&#34;&gt;5. Data partitioning&lt;/h2&gt;
&lt;p&gt;One of the biggest characteristics of Time series distinguishing it with normal cross-sessional data is the &lt;strong&gt;dependence of the future values with their historical values&lt;/strong&gt;. Therefore, the Data partitioning for Time series cannot be done randomly but instead, trim the series into 2 periods, the earlier to train set and the later to validation set.&lt;/p&gt;
&lt;p&gt;The below code will help split the tran-test sets with respect to time series structure.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Split train and test sets in correspondence with Time series data
def ts_train_test_split(X, y, test_size):
    test_index = int(len(X)*(1-test_size))
    
    X_train = X.iloc[:test_index]
    y_train = y.iloc[:test_index]
    X_test = X.iloc[test_index:]
    y_test = y.iloc[test_index:]
    
    return X_train, X_test, y_train, y_test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sales does not only correlated with its own past but also might be affected by other factors such as special occasions (i.e Holiday in this dataset), weekday and weekend, etc&amp;hellip; The method-driven models will be presented in the next article with feature extraction, feature selection.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ARIMA Autoregressive Integrated Moving Average model</title>
      <link>/publication/arima-autoregressive-intergreated-moving-average/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/arima-autoregressive-intergreated-moving-average/</guid>
      <description>&lt;h2 id=&#34;1concept-introduction&#34;&gt;1.	Concept Introduction&lt;/h2&gt;
&lt;p&gt;Auto Regressive Integrated Moving Average: &amp;lsquo;explains&amp;rsquo; a given time series based on its own past values. ARIMA is expressed as $ARIMA(p,d,q)$&lt;/p&gt;
&lt;p&gt;There are 3 parts in the ARIMA model: &lt;strong&gt;Auto Regressive (AR)&lt;/strong&gt; $p$, &lt;strong&gt;Integrated (I)&lt;/strong&gt; $d$, &lt;strong&gt;Moving Average (MA)&lt;/strong&gt; $q$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Integrated&lt;/strong&gt; (Or stationary): how is your data depend on each other across time. One of the characteristics of Stationary is that the effect of an observation dissipated as time goes on. Therefore, the best long-term predictions for data that has stationary is the historical mean of the series.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;hellip;To get stationary to your data, we need Differencing (or the change from one time period to another)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Auto Regressive&lt;/strong&gt;: deals with previous values of model, or called lags, and there are unlimited number of lags in the model. The basic assumption of this model is that the current series value depends on its previous values. This is the long memory model because the effect slowly dissipates across time. p is preferred as the maximum lag of the data series.
&amp;hellip; The AR can be denoted as
$Y_{t}=\omega_{0}+\alpha_{1}Y_{t-1}+\alpha_{2}Y_{t-2}+&amp;hellip;+\xi$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Moving Average&lt;/strong&gt;: deal with &amp;lsquo;shock&amp;rsquo; or error in the model, or how abnormal your current value is compared to the previous values (has some residual effect). This is short memory model because the effect quickly disappears completely.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;p, d, and q are non-negative integers;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p$: the order (number of time lags) of the autoregressive model, also called the lag order.&lt;/li&gt;
&lt;li&gt;$d$: the degree of differencing (the number of times the data have had past values subtracted)&lt;/li&gt;
&lt;li&gt;$q$: the order of the moving-average model (The size of the moving average window)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A value of 0 can be used for a parameter, which indicates to not use that element of the model. When two out of the three parameters are zeros, the model may be referred to non-zero parameter. For example, $ARIMA (1,0,0)$ is $AR(1)$  (i.e. the ARIMA model is configured to perform the function if a AR model), $ARIMA(0,1,0)$ is $I(1)$, and $ARIMA(0,0,1)$ is $MA(1)$&lt;/p&gt;
&lt;h2 id=&#34;2-model-evaluation&#34;&gt;2. Model evaluation&lt;/h2&gt;
&lt;p&gt;There are 2 common measures to evaluate the predicted values with the validation set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.	Mean Absolute Error (MAE):&lt;/strong&gt;
&amp;hellip;How far your predicted term to the real value on absolute term. One of the drawbacks of the MAE is because it shows the absolute value so there is no strong evidence and comparison on which the predicted value is actually lower or higher.&lt;/p&gt;
&lt;p&gt;$MAE=\frac{1}{n}\sum_{i = 1}^{n} |Y_{t}-\hat{Y_{t}}|$&lt;/p&gt;
&lt;p&gt;can be run with R&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mean(abs(Yp - Yv))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or in Python&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn import metrics
metrics.mean_absolute_error(y_test, y_pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Mean absolute percentage error (MAPE):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&amp;hellip; The MAE score shows the absolute value and it is hardly to define whether that number is good or bad, close or far from expectation. This is when MAPE comes in.
&amp;hellip;MAPE measures how far your predicted term to the real value on absolute percentage term.&lt;/p&gt;
&lt;p&gt;$MAPE=100\frac{1}{n}\sum_{i = 1}^{n} \frac{|Y_t-\hat{Y_t}|} {\hat{Y_{t}}}$&lt;/p&gt;
&lt;p&gt;Can compute as&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;100 x mean(abs(Yp - Yv) / Yv )
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Practical flow of a Data Science Project</title>
      <link>/publication/practical-flow-of-a-data-science-project/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/practical-flow-of-a-data-science-project/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Have you ever wondered why you, a talented Data Scientist, is not an adviser or a consultant for business, but just a normal employee spending time solving other&#39;s requests?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is such a common complaint that I heard so many times from my friends, my fellows and especially from the junior. Years ago during the beginning of my career, I was also a victim of this trap. Now, I realized that this is a real &lt;strong&gt;bottleneck&lt;/strong&gt; of Data Scientist and I want to share my experience to others so that it will help other Data Scientist achieving a brighter career.&lt;/p&gt;
&lt;p&gt;Whenever I meet other Data Science fellows, most of the time was spent talking about RNN, NLP, Deep learning, or Machine learning algorithms. But when I called on a question of why they used RNN instead of Deep learning or how their model supports the business, they could either provide an unconvincing reason or stay on a lengthy explanation of concept, algorithm without a comprehensive business thinking.&lt;/p&gt;
&lt;p&gt;It is a routine for a Data Scientist to saturate with &lt;strong&gt;technical models&lt;/strong&gt; while understates the role of &lt;strong&gt;business mindset&lt;/strong&gt;. However, I totally do not deny the integral role of technical work of a data scientist, I only want to emphasize the importance of &lt;strong&gt;understanding the business concept at first before any other activities&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, I list out below a standard flow for starting a data science projects with key points, which I have been applying throughout 4 years working for 2 multinational companies as a data analyst and data scientist.
This post was written based on my experience. Hence, take it as your reference and adjust to your own needs.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-first-and-foremost-importance---clarifying-business-question&#34;&gt;1. First and foremost importance - Clarifying Business question&lt;/h3&gt;
&lt;p&gt;During my years in analytics and data science, in addition to technological concept explanation, &lt;strong&gt;business question clarification&lt;/strong&gt; is one of the most difficult tasks when a Data scientist communicates with business partner.&lt;/p&gt;
&lt;p&gt;I am sure that you catch this phrase everywhere, in numerous articles, warning you to clarify business question in every situation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;But how?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Working and researching are not identical. In business, top people never stop expecting a Data Scientist to become a wise man who knows the answer for all of their questions. Therefore, &lt;strong&gt;digging the problem is our job and how to do that is our responsibility&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It must be very familiar with you when a sales manager asks you &lt;em&gt;&amp;ldquo;I want to know why sales declined?&amp;quot;&lt;/em&gt; or a marketing director demands &lt;em&gt;&amp;ldquo;How to increase the efficiency of the promotion activity for brand A on our website?&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When you hear these questions, can you imagine the right approach or the answer to the case? Or you will be very vague and keep asking yourself &lt;em&gt;&amp;ldquo;Is that they want me to do this.&amp;rdquo; or &amp;ldquo;I think they want to know that.&amp;quot;&lt;/em&gt;, and if you deliver the result based on this understanding, &lt;strong&gt;how much confidence you have on your result?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In reality, if you keep it this way, the one and only response you will get from them is:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;ldquo;This is not what I need&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;OMG! How terrible it is when you spent so much effort into this, but no one values it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;This is because you did not truly understand problem so that you did not touch the right pain point!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, the director wants to know way to improve his marketing activity efficacy, so what does &lt;strong&gt;Efficacy&lt;/strong&gt; mean here? What kind of activity he points to? What are the real pain points? If these concerns are plainly clarified, the request will be interpreted as &lt;em&gt;&amp;ldquo;How to optimize the budget spending on online promotion in order to increase the purchase rate and new customers vs last year&amp;rdquo;&lt;/em&gt;. This will end up with increasing efficacy.&lt;/p&gt;
&lt;p&gt;One of the common advises is to ask &lt;strong&gt;&amp;ldquo;Why&amp;rdquo;&lt;/strong&gt; in order to dig into the real problems. However, this solution is not applicable all times because the business partners might not know why for all of your questions.&lt;/p&gt;
&lt;p&gt;What can you do more:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ask about the background of the question&lt;/strong&gt;, why and how they come up with the request after you receive it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Be sure that the request is your responsibility to answer&lt;/strong&gt;. If your company has several Data teams such as Data Scientist, Data Analyst and BI, make sure to understand the Role &amp;amp; Responsibility of each team and know when to jump in and when to leap out. However, don&#39;t ever say &lt;em&gt;&amp;ldquo;This is not my job. Ask BI&amp;rdquo;&lt;/em&gt;. Instead, show them that you know everything about company and data &lt;em&gt;&amp;ldquo;With your request, the BI team has already got the data that can help with your question, I suggest you to meet BI and ask for sales and churn rate data of the last 3 years&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Engage with other teams in the company&lt;/strong&gt; to frequently get updates about other things happened within your company. Moreover, it is extremely important to always raise up questions such as &lt;em&gt;&amp;ldquo;What are the latest company strategy, calendar, current key projects and recent performance?&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;ldquo;Do I understand the vision and objectives of the projects that are critical to my company?&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Think of initiatives and what you can do more&lt;/strong&gt; within your expertise to bring the projects to the next level.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Be a thinker, not a doer!&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-identify-the-approach-for-the-problem&#34;&gt;2. Identify the approach for the problem&lt;/h3&gt;
&lt;p&gt;This part is to place the methodology for the analysis&lt;/p&gt;
&lt;p&gt;This step requires a broad knowledge on either &lt;strong&gt;statistical models&lt;/strong&gt; or &lt;strong&gt;machine learning&lt;/strong&gt; approached. In some companies, especially non-tech savvy ones, a data scientist is in charge of both analytics and data science work stream.&lt;/p&gt;
&lt;p&gt;With a mixed role of Analytics and Data Science, the approaches for the problem will also diversified with various concepts and models. For example: Linear Regression cannot be used to segment customers or Descriptive analysis cannot predict customer churn.&lt;/p&gt;
&lt;p&gt;At first, choosing methodology seems to be effortless but indeed always drives you crazy. If the Sales Director asked Data Science team to &lt;strong&gt;forecast the sales for next year based on the amount of budget spending while putting online appearance as company&#39;s focus&lt;/strong&gt;, then which approach/model should be used? If the business wanted the &lt;strong&gt;forecast based on the market movement with the outlook of maintaining the current company&#39;s leadership position&lt;/strong&gt;, which approach is correct?&lt;/p&gt;
&lt;p&gt;What you can do more:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is fundamental to &lt;strong&gt;understand the discrepancy between Descriptive analysis and Predictive analysis&lt;/strong&gt; (many people are still ambiguous between these 2 concepts). An example of descriptive analysis is the relationship between factors; while prescriptive analysis deals with figuring out the future outcomes of that relationship. &lt;em&gt;Descriptive analysis delivers historical insights and prescriptive analysis foresees future values&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identify the specific type of data&lt;/strong&gt; to assist the problem approach: The target variable and other variables are continuous, categorical or binary.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Understand key problem approaches&lt;/strong&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Binary&lt;/em&gt; (2 possible answers) or &lt;em&gt;Multi-clas&lt;/em&gt;s (more than 2 possible answers) classification;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Regression relationship&lt;/em&gt; (relationship between 2 or more factors) or &lt;em&gt;Regression prediction&lt;/em&gt; (predict future value using regression model);&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Clustering&lt;/em&gt; (cluster unlabeled observation into groups of similar characteristics) or &lt;em&gt;Segmentation&lt;/em&gt; (divide observations into specific groups);&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Trend detection&lt;/em&gt; (historical movement) or &lt;em&gt;Time-series forecasting&lt;/em&gt; (project future value of that movement).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-acquire-the-appropriate-data&#34;&gt;3. Acquire the appropriate data&lt;/h3&gt;
&lt;p&gt;After identifying the business questions and the approach above, set up data requirement and extract the appropriate data from the data warehouse are the next thing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data selection&lt;/strong&gt; sounds to be straightforward but indeed complicated. To solve the business questions, which kind of data is in need of. For example, is it necessary to have customer&#39;s birthday information for the task of predicting churn probability?&lt;/p&gt;
&lt;p&gt;Ingest sufficient data will save you tons of effort afterward. Bear in mind the unspoken truth: &lt;em&gt;Garbage in is Garbage out&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Two major problems that usually occur during Data Collection&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The unavailability of data&lt;/li&gt;
&lt;li&gt;The bias of training data&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;31-first-lets-look-at-the-unavailability-of-data&#34;&gt;3.1 First, let&#39;s look at the unavailability of data&lt;/h4&gt;
&lt;p&gt;This problem is very common globally in which data is unable to capture at the moment of collection due to the limitation of current digital connection. For instance, it is merely impossible to acquire the &lt;em&gt;time spent in cooking at home&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;As a common sense, when the data is nonexistent, you will instantly think of way to get the data. However, you have to consider the consequences of unavailability data including cost, time, resources and if the data is indeed not too important to your model, all the effort you put into it will be down the drain.&lt;/p&gt;
&lt;p&gt;Therefore, the solution for this case is to &lt;strong&gt;defer inaccessible data&lt;/strong&gt; and in case the model requires this data for a better result, you will have more resources and confident to invest in obtaining it in the future.&lt;/p&gt;
&lt;p&gt;What you can do more if you need more data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bring along a data request summary&lt;/strong&gt; when you visit the database owner if you need to talk to other parties for this. The summary form should include background of you project, data requirement, your request. This will help smoothing the discussion and the business partner will give the adequate solution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Change the process/method of collecting data&lt;/strong&gt; to acquire the right information needed. Work with database owner or IT team or propose to your up-line a system revision plan for approval.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prepare budget and contact an outside data owner&lt;/strong&gt; if the additional data is vital to improve the model and inaccessible for you.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;32-second-the-bias-of-data&#34;&gt;3.2 Second, the bias of data&lt;/h4&gt;
&lt;p&gt;This problem is serious especially &lt;strong&gt;when the training set gets bias from the beginning, the model will learnt accordingly to that bias and results into an inaccuracy prediction&lt;/strong&gt; when comparing to the real world.&lt;/p&gt;
&lt;p&gt;One of the most famous flaws of bias in data is the Amazon recruiting AI tool that showed bias against women. The tool reviewed candidate&#39;s resumes in order to pick the top talents within them. The tool showed an obvious bias against women because its training data is &lt;strong&gt;not gender-neutral&lt;/strong&gt; from the beginning.&lt;/p&gt;
&lt;p&gt;Therefore, at first hand, be careful with data and its natural distribution are critical responsibility of every Data Scientist.&lt;/p&gt;
&lt;p&gt;What you can do more to eliminate the bias:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ensure the statistical distribution of data and its representatives over population&lt;/strong&gt;. For example, if the population is made up of 56% of male and 43% of female and 1% of others, the data distribution must be in similar ratio.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Verify the split of train, validate and test sets&lt;/strong&gt; in prediction models to establish a similar allocation of variable and categories.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Choose the learning model fitting the problem and reduce the skewness&lt;/strong&gt;. Some models can reduce the bias in data including clustering or dimension reduction.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monitor the performance in real data&lt;/strong&gt;. Frequently run statistical test on real data to pick out uncommon case. If the test result shows a statically significant in churn rate among male than female, dig it out. Is it the sudden shift or result of bias?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;After getting all the data you need, the next step is thing that a Data scientist usually does:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;DS_Flow.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The order can be flexible and this is the standard progress that I usually do in my project and my job. Sometimes, after tuning and the accuracy does not meet my expectation, I need to go back to the feature engineering step to find other way to deal with features.&lt;/p&gt;
&lt;p&gt;These are the key bottlenecks beside the technical skills that I want to head up for Data Scientist who want to become more than just a Data insight extractor.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quick deep dive at Data Scientist Skill set</title>
      <link>/2020/01/17/quick-deep-dive-at-data-scientist-skill-set/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/2020/01/17/quick-deep-dive-at-data-scientist-skill-set/</guid>
      <description>&lt;p&gt;One year ago, when I truly and seriously considered improving my skill in Data Science, two questions were always lingering in my mind:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frankly saying, what are the skills that a Data Scientist needs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What skill will support me in the future and How do I improve myself in the right direction at the most efficacy?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moreover, some friends, acquaintances of mine reached out to me for a thought of what they should learn to develop their career as a Data Scientist.&lt;/p&gt;
&lt;p&gt;Actually, I can share some of my experiences with them, but as you&#39;ve already known, this field evolves unpreceedingly fast, technology and new required skills change on the yearly basis at the slowest.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;What you learn today will be the old of tomorrow!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Luckily, when I was known of the Kaggle dataset on Survey of Data Science and Machine learning, this data will definitely give me some insights for my questions.&lt;/p&gt;
&lt;p&gt;The Data source is &lt;a href=&#34;https://www.kaggle.com/c/kaggle-survey-2019&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this post, I will summary my key findings from the survey. The complete analysis can be found through this &lt;a href=&#34;https://www.kaggle.com/geninhu/direction-toward-a-great-data-scientist/data&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This analysis uses R language with tidyverse package for the best insight visualization.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Before looking at the result, below are the data cleaning and preparation for the analysis.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Feature selection:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This dataset includes the written answer of the respondents if they picked &lt;code&gt;&amp;quot;Other&amp;quot;&lt;/code&gt; choice for some questions. However, the written answers are stored in another &lt;em&gt;csv&lt;/em&gt; file so that all the variables containing &lt;code&gt;&amp;quot;Other&amp;quot;&lt;/code&gt; will not a valuable variable for the analysis, therefore they will be excluded from the dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For this analysis, the &lt;code&gt;&amp;quot;Duration&amp;quot;&lt;/code&gt; variable is not the factor that I want to explore so it will be excluded as well.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Other data cleaning:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Shorten some typical column names for easier understanding.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set level for values in the variables that require level later on, such as &lt;em&gt;Company size, Compensation/Salary, year of experience with machine learning&lt;/em&gt;&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Create functions for plotting (Example is as below)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_df_ds &amp;lt;- function (df_draw_ds, columnName1, columnName2) {
    names(df_draw_ds)[columnName1] &amp;lt;- paste(&amp;quot;value&amp;quot;)
    names(df_draw_ds)[columnName2] &amp;lt;- paste(&amp;quot;value2&amp;quot;)
    df_draw_ds &amp;lt;- df_draw_ds %&amp;gt;% 
    select (value, value2) %&amp;gt;%
    group_by(value, value2) %&amp;gt;%
    filter(value != &amp;quot;Not employed&amp;quot;,value != &amp;quot;Other&amp;quot;) %&amp;gt;% 
    summarise(count=n()) %&amp;gt;% 
    mutate(perc= prop.table(count))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Now, let&#39;s explore the result.&lt;/p&gt;
&lt;h3 id=&#34;role-and-responsibilities&#34;&gt;&lt;strong&gt;Role and Responsibilities:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A Data scientist is responsible for many tasks but the Top 3 includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Analyzing data&lt;/strong&gt; to influence product or to support business decisions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explore new areas of Machine learning&lt;/strong&gt; through builidng prototypes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Experiment to &lt;strong&gt;improve company existing Machine learning models&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/responsibility.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;programming-skills&#34;&gt;&lt;strong&gt;Programming skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Python, SQL and R&lt;/strong&gt; are all the programming languages that are essential for Data Scientist as they stand in the Top 3 respectively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/PL.png&#34; width=&#34;300&#34; height=&#34;450&#34; &gt;&lt;/p&gt;
&lt;h3 id=&#34;machine-learning-skills&#34;&gt;&lt;strong&gt;Machine Learning skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;It is not a surprise if all Data Scientists use Machine Learning in their daily job but I was amazed that almost all of them use both Natural Language Processing and Computer Vision. These 2 fields has no longer been specified areas to some groups of users but expanded to much wider application and require Data Scientist to own these skills.&lt;/p&gt;
&lt;h3 id=&#34;machine-learning-algorithm-skills&#34;&gt;&lt;strong&gt;Machine Learning algorithm skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt; and &lt;strong&gt;Tree-based algorithms&lt;/strong&gt; are the models used by Data Scientist. With the long history in statistics and analytics, these models are still being favor in analyzing data. Another advantage of these traditional models is that they are easy to explain to business partner. Other models such as Neuron network, or Deep learning is hard to explain the result.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/ML.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another interesting thing is that 51% of Data scientists is applying &lt;strong&gt;Boosting method&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;python-skills&#34;&gt;&lt;strong&gt;Python skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Scikit-learn, Keras, Xgboost, TensorFlow and RandomForest libraries&lt;/strong&gt; are the top used frameworks given their benefit and convenience.&lt;/p&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/package.png&#34; width=&#34;300&#34; height=&#34;450&#34;&gt;
&lt;h3 id=&#34;data-scientist-credential&#34;&gt;&lt;strong&gt;Data Scientist Credential:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Foundation skills&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Writing code&lt;/strong&gt; is an important skill of a good Data scientist. Being an excellent coder is not required but preferable to have, especially big companies have the tendency to hire Data Scientist Engineer with coding skill.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is essential to &lt;strong&gt;build up your business skills&lt;/strong&gt; in addition to technical skills. Many data scientists, especially the junior may excel at machine learning and algorithm but usually cry out for business point of view. Their recommendations are getting off the track from what the company is doing. That is the worst thing you want to face in the world of Data science when no one values your idea..&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Specialized skills&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Build expertise in &lt;strong&gt;Python, SQL and/or R&lt;/strong&gt; on various online/offline platforms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fluency in using Local development environments and Intergrated development Environment, including but not limited to Jupyter, RStudio, Pycharm. Try to &lt;strong&gt;learn on job&lt;/strong&gt; or &lt;strong&gt;learn on practice&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Strong in foundation and rich in practical experience&lt;/strong&gt;. Develop side projects within your interested fields with proper models showcase. The model can be either traditional Linear regression, Decision Tree/Random Forest, or cutting-edge XGBoost, Recurrent Neural Network.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enrich your knowledge and application in convenient frameworks&lt;/strong&gt; such as Scikit-learn, Keras, Tensorflow, Xgboost. Working on your own project/cooperating project is a good choice to get practical experience if you have not obtained the chance in a company.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Computer Visio&lt;/strong&gt;n and &lt;strong&gt;NLP&lt;/strong&gt; are the booming areas in Data science so it is beneficial to prepare yourself with these skills.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although &lt;strong&gt;AutoML&lt;/strong&gt; is a newly emerging field, it will probably become one of the important tools and skills for Data Scientist, including Automated hyperparameter tuning, Data augmentation, Feature engineering/selection and Auto ML pipelines.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Skills in working with &lt;strong&gt;Big data&lt;/strong&gt; and Big data products e.g Google Bigquerry, Databricks, Redshift are the must to own.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is the same with usage of &lt;strong&gt;cloud computing skills&lt;/strong&gt;. In big company, it is a common to work on cloud so if you do not know how to conduct machine learning on cloud, it will become your minus comparing to other candidates.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And one last important thing: &lt;strong&gt;Always believe in yourself, your choice and never give up&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;end-notes&#34;&gt;End notes&lt;/h4&gt;
&lt;p&gt;As the big survey focusing on the Data science / Machine Learning areas, it appeared as a great source for me to gain valuable information.&lt;/p&gt;
&lt;p&gt;Beside understanding which typical skills requiring to advance in the Data Science field, I want to tbuild a model to predict the salary range and I will update on that in upcoming days.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
