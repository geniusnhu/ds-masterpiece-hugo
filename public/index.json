[{"authors":["admin"],"categories":null,"content":"About me I'm Nhu Hoang, Data Scientist and Marketing Portfolio Planning Specialist in Amway Japan. I earned Master Degree in Interdisciplinary Studies from Tohoku University, Japan with research on using Regression model to predict the Wind power price and improve the learning curve.\nI own decent foundation and acquired rich experience in Data Science, Analytics within various industries. My current focus is time series forecasting and tree-based models in application of predicting customer behavior.\n","date":1592092800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1592092800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"About me I'm Nhu Hoang, Data Scientist and Marketing Portfolio Planning Specialist in Amway Japan. I earned Master Degree in Interdisciplinary Studies from Tohoku University, Japan with research on using Regression model to predict the Wind power price and improve the learning curve.\nI own decent foundation and acquired rich experience in Data Science, Analytics within various industries. My current focus is time series forecasting and tree-based models in application of predicting customer behavior.","tags":null,"title":"Nhu Anh Quynh Hoang","type":"authors"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Data Science","Deep learning"],"content":"Part 1: Initialization, Activation function and Batch Normalization/Gradient Clipping\nPart 2: Optimizer\nTraining a deep neural network is an extremely time-consuming task especially with complex problems. Using a faster optimizer for the network is an efficient way to speed up the training speed, rather than simply using the regular Gradient Descent optimizer. Below, I will discuss and show training results/speed of 5 popular Optimizer approaches: Gradient Descent with momentum and Nesterov Accelerated Gradient, AdaGrad, RMSProp, Adam, and Nadam optimizer.\nOne of the dangers of using inappropriate optimizers is that the model takes a long time to converge to a global minimum or it will be stuck at a local minimum, resulting in a worse model. Therefore, knowing which Optimizer suits mostly on the problem will save you tons of training hours. The main purpose of tuning Optimizer is to speed up the training speed but it also helps to improve the model's performance.\n1. Gradient Descent Computing the gradient of the associated cost function with regard to each theta and getting the gradient vector pointing uphill, then going in the opposite direction with the vector direction (downhill) using the below equation:\n $\\theta_{next step} = \\theta - \\eta* \\nabla_{\\theta}J(\\theta)$\n$\\theta$: weight of the model\n$\\eta$: learning rate\n Therefore, the speed of Gradient Descent optimizer depends solely on the learning rate parameter ($\\eta$). With a small learning rate, GD will take small and unchanged steps downward on a gentle surface, and a bit faster steps on a steep surface. Consequently, in a large neural network, it repeats millions of slow steps until it reaches the global minimum (or gets lost in the local minimum). Therefore, the runtime becomes extremely slow.\nResult of training with Fashion MNIST dataset using SGD:\n\rFirgure 1: Loss and accuracy of model using SGD with learning rate 0.001\r\rThe loss declined gradually and will be closer and closer to global minimum after several more epochs.\nThere are other versions of Gradient Descent such as Batch Gradient Descent (running on a full dataset), Mini-batch Gradient Descent (running on random subsets of a dataset), Stochastic Gradient Descent - SGD (picking a random instance at each step), and all have pros and cons. Batch Gradient Descent can reach the global minimum at a terribly slow pace. Mini-batch Gradient Descent gets to the global minimum faster than BGD but it is easier to get stuck in the local minimum, and SGD is usually harder to get to the global minimum compared to the other two.\n2. Momentum Optimization Let's imagine, when a ball rolls from the summit, downward the sloping side to the foot of the hill, it will start slowly then increase the speed as the momentum picks up and eventually reaches a fast pace toward the minimum. This is how Momentum Optimization works. This is enabled by adding a momentum vector m and update the theta parameter with this new weight from momentum vector $m$\n $m$ ‚Üê $\\beta m - \\eta * \\nabla_{\\theta}J(\\theta)$\n$\\theta_{next step}$ ‚Üê $\\theta + m$\n Gradient descent does not take into account the previous gradients. By adding the momentum vector, it updates the weight $m$ after each iteration. The momentum $\\beta$ is the parameter controls how fast the terminal velocity is, which is typically set at 0.9 but it should be tuned from 0.5 to 0.99. As a result, Momentum Optimizer converges better and faster than SGD.\n# Implement Momentum optimizer in Tensorflow\roptimizer=keras.optimizers.SGD(lr=0.001, momentum=0.99)\r \rFigure 2: Loss and accuracy of models using SGD compared to momentum optimizer\r\rMomentum converges faster and eventually reaches a better result than SGD.\n3. Nesterov Accelerated Gradient Another variation of Momentum Optimizer is Nesterov Accelerated Gradient - NAG.\n $m$ ‚Üê $\\beta m - \\eta * \\nabla_{\\theta}J(\\theta + \\beta m)$\n$\\theta_{next step}$ ‚Üê $\\theta + m$\n The gradient of the cost function is measured at location $\\theta + \\beta m$ (instead of $\\theta$ in the original momentum optimization). The reason behind this is that momentum optimization has already pointed toward the right direction, so we should use a slightly ahead location (an approximately next position of the $\\theta$) to moderately accelerating the speed of convergence.\n# Implement Nesterov Accelerated Gradient optimizer in Tensorflow\roptimizer=keras.optimizers.SGD(lr=0.001, momentum=0.9,nesterov=True\r \rFigure 3: Loss and accuracy of models using momentum compared to Nesterov Accelerated Gradient optimizer\r\rNAG showed only a slightly better result than original Momentum.\n4. AdaGrad One of the Adaptive learning rate methods, in which the algorithm goes faster down the steep slopes than the gentle slopes. AdaGrad performs well in a simple quadratic problem but not in training a neural network because it tends to slow down a bit too fast and stops before reaching the global minimum. Due to this drawback, I do not usually use AdaGrad for Neural Network but instead apply RMSProp, an alternative of AdaGrad.\n5. RMSProp - Root Mean Square Prop This is one of the most frequently used optimizers, which continues the idea of Adagrad in trying to minimize the vertical movement and updating the model in a horizontal direction toward the global minimum.\nAdagrad sums the gradients from the first iteration and that is why it usually never converges to the global minimum, while RMSProp accumulates the gradients from the previous iterations:\n $s$ ‚Üê $\\beta s - (1-\\beta) \\nabla_{\\theta}J(\\theta)^2$\n$\\theta_{nextstep}$ ‚Üê $\\theta + \\frac{\\eta \\nabla_{\\theta}J(\\theta)}{\\sqrt{s + \\epsilon}}$\n $\\beta$: decay rate, typically set at 0.9\n$s$: exponential average square of past gradients\n# Implement RMSProp optimizer in Tensorflow\roptimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9)\r \rFigure 4: Loss and accuracy of models using RMSProp compared to Adagrad optimizer\r\rRMSProp converges better than Adagrad which is lost at a plateau.\n6. Adam Adam optimizer is the combination of Momentum and RMSProp optimizers. In other words, it takes into account both the exponential decay average of past gradients and the exponential decay average of past squared gradients.\nWith these characteristics, Adam is suitable for handling sparse gradients on complex problems with complex data and a large number of features.\n $m$ ‚Üê $\\beta_1 m - (1-\\beta_1) \\nabla_{\\theta}J(\\theta)$\n$s$ ‚Üê $\\beta_2 s - (1-\\beta_2) \\nabla_{\\theta}J(\\theta)$\n$\\hat{m}$ ‚Üê $\\frac{m}{1-\\beta_1^T}$\n$\\hat{s}$ ‚Üê $\\frac{s}{1-\\beta_2^T}$\n$\\theta_{nextstep}$ ‚Üê $\\theta + \\frac{\\eta \\hat{m}}{\\sqrt{\\hat{s} + \\epsilon}}$\n $\\eta$: learning rate\n$s$: exponential average square of past gradients\n$m$: momentum vector\n$\\beta_1$: momentum decay, typlically set at 0.9\n$\\beta_2$: scaling decay, typlically set at 0.999\n$\\epsilon$: smoothing term\n# Implement Adam optimizer in Tensorflow\roptimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\r \rFigure 5: Loss and accuracy of models using Adagrad, RMSProp, and Adam\r\r7. Nadam Another variation of Adam is Nadam (using Adam optimizer with Nesterov technique), resulting in a little faster training time than Adam.\n# Implement Nadam optimizer in Tensorflow\roptimizer=keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)\r \rFigure 6: Loss and accuracy of models using RMSProp, Adam and Nadam\r\rAdagrad, RMSProp, Ada, Nadam, and Adamax are Adaptive learning rate algorithms, which require less tuning on hyperparameters. In case the performance of the model does not meet your expectation, you can try to change back to Momentum optimizer or Nesterov Accelerated Gradient.\nFinal words ü§ò In conclusion, most of the time, Adaptive learning rate algorithms outperform Gradient descent and its variants in terms of speed, especially in a deep neural network. However, Adaptive learning rate algorithms do not ensure an absolute convergence to the global minimum.\nIf your model is not too complex with a small number of features, and training time is not your priority, using Momentum, Nesterov Accelerated Gradient or SGD is the optimal starting point, then tune the learning rate, activation functions, change Initialization technique to improve the model rather than using Adaptive learning rate Optimizers because the later ones hinder the risk of not converging to the global minimum.\n\rFigure 7: Summary model performance on training loss of different optimization techniques\r\r Regular SGD or regular Gradient Descent takes much more time to converge to the global minimum. Adagrad often stops too early before reaching the global minimum so in time it becomes the worse optimizer. With the Fashion MNIST dataset, Adam/Nadam eventually performs better than RMSProp and Momentum/Nesterov Accelerated Gradient. This depends on the model, usually, Nadam outperforms Adam but sometimes RMSProp gives the best performance. With my experience, I found out that Momentum, RMSProp, and Adam (or Nadam) should be the first try of the model.  Summary    Optimizer Training speed Converge quality Note     Gradient Descent / SGD Medium for simple model\nSlow for complex model Good Risk of converging to local minimum.\nCan be controled by assigning the correct learning rate   Momentum Fast for simple model\nMedium for complex model Good Suitable for less complex NN with small number of features\nNeed to consider tuning the momentum hyperparameter   Nesterov Accelerated\nGradient Fast for simple model\nMedium for complex model Good Suitable for less complex NN with small number of features\nNeed to consider tuning the momentum hyperparameter   AdaGrad Fast Usually miss global minimum\ndue to early stopping Suitable for simple quadratic problem, not NN   RMSProp Fast Acceptable Suitable for complex NN\nNeed to tune Decay rate for better performance   Adam Fast Acceptable Suitable for sparse gradients on complex model\nwith a large number of features   Nadam Fast Good Suitable for sparse gradients on complex model\nwith a large number of features     This article was originally published in Towards Data Science\nSource code: here\n","date":1592092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592092800,"objectID":"8180f2d99fce244f85e825d822fe8f26","permalink":"/project/2020-06-14-optimization-and-neural-network-performance/","publishdate":"2020-06-14T00:00:00Z","relpermalink":"/project/2020-06-14-optimization-and-neural-network-performance/","section":"project","summary":"Speed up deep neural network training by tuning Optimizer in Tensorflow","tags":["Deep learning","Data Science","Optimization","Hyperparameter tuning","Tensorflow"],"title":"Improve deep neural network training speed and performance with Optimization","type":"project"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Data Science","Deep learning"],"content":"Training a large and deep neural network is a time and computation consuming task and was the main reason for the unpopularity of DNN 20 years ago. As several techniques have been found out to push up the training speed, Deep learning has come back to the light. So which technique to use, how and when to use which? Let's discuss it here!\nPerformance summary is shown at the end of the post for Classification \u0026amp; Regression examples\n 1. Applying Initialization Initialization is one of the first technique used to fasten the training time of Neuron Network (as well as improve performance). Let's briefly explain its importance. In Artificial Neural Network (ANN), there are numerous connections between different neurons. One neuron in the current layer connects to several neurons in the next layer and is attached to various ones in the previous layer. If 2 neurons interact frequently than another pair, their connection (i.e the weights) will be stronger than the other one.\nHowever, one problem with the ANN is that if the weights aren't specified from the beginning of training, the connection weights can be either too small or too large which makes them too tiny or too massive to use further in the network. In other words, the network will fall into Vanishing Gradients or Exploding Gradients problems.\nSo if the weights are set at suitable random values from the beginning of the training, these problem can be avoided. This technique was proposed by Glorot and Bengio, which then significantly lifted these unstable problems. This initialization strategy is called Xavier initialization or Glorot initialization.\nIn this strategy, the connection weights between neurons are initialized randomly using the Normal distribution with $mean=0$ and variance $\\sigma^2 = \\frac{2}{fan_{in}+fan_{out}}$ , in which $fan_{in}$ is the number of input neurons and $fan_{out}$ is the number of output neurons.\nThere are 2 other popular initialization techniques beside Glorot (used in Keras as default): He and LeCun.\nLet's examine different initialization techniques\u0026rsquo; effect on model performance and training time with fashion MNIST dataset.\nplt.figure(figsize=(10, 10))\rfor row in range(5):\rfor col in range(5):\rindex = 5 * row + col\rplt.subplot(5, 5, index + 1)\rplt.imshow(X_train_full[index], cmap=\u0026quot;binary\u0026quot;, interpolation=\u0026quot;nearest\u0026quot;)\rplt.axis('off')\rplt.title(y_train_full[index], fontsize=12)\rplt.show()\r \rHere is the example of Fashion MNIST, in which the predictors are a set of values in the shape of [28,28] representing the image; and the target value is 10 types of cloth and shoes (denoted from 0 to 9)\r\rFirst, let's start with the default setting of Keras on a network consisting of 5 hidden layers and 300, 100, 50, 50, 50 neurons each.\ntf.random.set_seed(50)\rnp.random.seed(50)\rmodel_default = keras.models.Sequential()\rmodel_default.add(keras.layers.Flatten(input_shape=[28, 28]))\rfor n_layers in (300, 100, 50, 50, 50):\rmodel_default.add(keras.layers.Dense(n_layers, activation ='relu'))\rmodel_default.add(keras.layers.Dense(10, activation='softmax'))\rmodel_default.compile(loss=\u0026quot;sparse_categorical_crossentropy\u0026quot;,\roptimizer=keras.optimizers.SGD(lr=1e-3),\rmetrics=[\u0026quot;accuracy\u0026quot;])\rstart_time = time.time()\rhistory = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)\rprint(\u0026quot;--- %s seconds ---\u0026quot; % (time.time() - start_time))\r Result\n# Show the highest accuracy epoch\rEpoch 20/20\r1688/1688 [==============================] - 5s 3ms/step - loss: 0.4185 - accuracy: 0.8526 - val_loss: 0.4256 - val_accuracy: 0.8518\r--- 99.03307843208313 seconds ---\r The train set reached 85.26% accuracy and Val set reached 85.18% within 99.3 seconds. If activation ='relu' is not set (i.e. no Activation function in the hidden layers), the accuracy is 85.32% and 84.95% respectively with 104.5 seconds needed to train on.\nComparing this with weight initialization to all Zeros and all Ones:\n# Zeros initialization\rEpoch 20/20\r1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925\r--- 69.43926930427551 seconds ---\r# Ones initialization\rEpoch 20/20\r1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925\r--- 67.2280786037445 seconds ---\r The performance in both cases is much worse and actually the model stopped improving from 5th epoch.\nAnother Initialization that can be considered to use is He Initialization, enabling in Keras by adding kernel_initializer=\u0026quot;he_normal\u0026quot; argument to the hidden layers.\nResult\n# Show the highest accuracy epoch\rEpoch 20/20\r1688/1688 [==============================] - 5s 3ms/step - loss: 0.3780 - accuracy: 0.8672 - val_loss: 0.3924 - val_accuracy: 0.8637\r--- 99.76096153259277 seconds ---\r The accuracy actually improved but the running time was half a second slower than Glorot Initialization\nThere are also discussions about the performance of normal distribution and uniform distribution in initialization technique, but there is indeed no one shows better performance than the other one. The result of init = keras.initializers.VarianceScaling(scale=2.,mode='fan_avg',distribution='uniform') does not improve for this data set (Train set accuracy: 87.05%, Val set: 86.27% and took 100.82 seconds to run)\n2. Get along with the right Activation function Choosing an unfit activation function is one of the reasons leading to poor model performance. sigmoid might be a good choice but I prefer to use SELU, ReLU, or its variants instead.\nLet's talk about ReLU first. Simply saying, if the value is larger than 0, the function returns the value itself; else it returns 0. This activation is fast to compute but in return there will be a case that it stops outputting anything other than 0 (i.e neurons were died). This issue usually happens in case of a large learning rate.\n\rReLU, Leaky ReLU and SELU\r\rSome of the solutions for this problem is to use alternative versions of ReLU: LeakyReLU, Randomized LeakyReLU or Scaled ReLU (SELU).\nWith LeakyReLU:\nif x\u0026gt;0:\rreturn x\relse:\rreturn ax\r in which a is $\\alpha$, the slope of the $x$ given $x\u0026lt;0$. $\\alpha$ is usually set at 0.01, serving as a small leak (that's why this technique is called LeakyReLU). Using $\\alpha$ helps to stop the dying problem (i.e. slope=0).\nIn case of Randomized LeakyReLU, $\\alpha$ is selected randomly given a range. This method can reduce the Overfitting issue but requires more running time due to extra computation.\nOne of the outperformed activation function for DNN is Scaled ReLU (SELU).\nif x\u0026gt;0:\rreturn Lambda*x\relse:\rreturn Lambda*(alpha*exp(x)-alpha)\r In this function, each layer outputs\u0026rsquo; mean is 0 and standard deviation is 1. Note when using this activation function:\n It must be used with kernel_initializer=\u0026quot;lecun_normal\u0026quot; The input features must be standardized The NN's architecture must be sequential  Let's try different Activation functions on the fashion MNIST dataset.\nResult of LeakyReLU\n# Show the highest accuracy epoch\rEpoch 20/20\r1688/1688 [==============================] - 5s 3ms/step - loss: 0.3791 - accuracy: 0.8670 - val_loss: 0.3910 - val_accuracy: 0.8615\r--- 101.87710905075073 seconds ---\r Result of Randomized LeakyReLU\n# Show the highest accuracy epoch\rEpoch 20/20\r1688/1688 [==============================] - 6s 3ms/step - loss: 0.3779 - accuracy: 0.8667 - val_loss: 0.3918 - val_accuracy: 0.8630\r--- 113.58738899230957 seconds ---\r Result of SELU\n# Show the highest accuracy epoch\rEpoch 19/20\r1688/1688 [==============================] - 5s 3ms/step - loss: 0.3526 - accuracy: 0.8763 - val_loss: 0.3755 - val_accuracy: 0.8647\r--- 106.25733232498169 seconds ---\r SELU seems to achieve slightly better performance over ReLU and its variants but the speed is slower (as expected).\nIf the NN performs relatively well at a low learning rate, ReLU is an optimal choice given the fastest training time. In case of the deep NN, SELU is an excellent try. Detailed explanation about these activations can be found in here: ReLU, LeakyReLU, Randomized LeakyReLU and SELU\n3. Batch Normalization To ensure Vanishing/Exploding Gradients problems do not happen again during training (as Initialization and Activation function can help reduce these issues at the beginning of the training), Batch Normalization is implemented.\nBatch Normalization zeros centers and normalizes each input, then scales and shifts the result using 1 parameter vector for scaling and 1 for shifting. This technique evaluates the $mean$ and $standard deviation$ of the input over the current mini-batch and repeats this calculation across all mini-batches of the training set. $\\mu$ and $\\sigma$ are estimated during training but only used after training.\nThe vector of input means $\\mu$ and vector of input standard devition $\\sigma$ will become non-trainable parameters (i.e. untouchable by backpropagation) and be used to compute the moving averages at the end of the training. Subsequently, these final parameters will be used to normalize new data to make prediction.\nIf using Batch Normalization, the input data will not need to be standardized prior training.\ntf.random.set_seed(50)\rnp.random.seed(50)\rmodel_default = keras.models.Sequential()\rmodel_default.add(keras.layers.Flatten(input_shape=[28, 28]))\rfor n_layers in (300, 100, 50, 50, 50):\rmodel_default.add(keras.layers.BatchNormalization())\rmodel_default.add(keras.layers.Dense(n_layers, activation ='relu', kernel_initializer=\u0026quot;he_normal\u0026quot;))\rmodel_default.add(keras.layers.Dense(10, activation='softmax'))\rmodel_default.compile(loss=\u0026quot;sparse_categorical_crossentropy\u0026quot;,\roptimizer=keras.optimizers.SGD(lr=1e-3),\rmetrics=[\u0026quot;accuracy\u0026quot;])\rstart_time = time.time()\rhistory = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)\rprint(\u0026quot;--- %s seconds ---\u0026quot; % (time.time() - start_time))\r Result\n# Show the highest accuracy epoch\rEpoch 20/20\r1688/1688 [==============================] - 8s 5ms/step - loss: 0.3799 - accuracy: 0.8645 - val_loss: 0.3571 - val_accuracy: 0.8685\r--- 167.6186249256134 seconds ---\r Obviously, training is slower in Batch Normalization given more computations during training but in contrast, in Batch Normalization, the model convergences faster so fewer epoches are needed to reach the same performance.\n Batch Normalization is strictly implemented in Recurrent NN   4. Gradient Clipping As Batch Normalization is recommended not to use with Recurrent NN, Gradient Clipping is the alternative choice for RNN.\nDetails about Gradient Clipping\n Summary of the result of Classification task with Fashion MNIST dataset    Initialization Activation fuction Train set accuracy Val set accuracy Running time (seconds)     Glorot - Zeros/Ones ReLU 10.08% 9.25% 69.43/67.22   Glorot None 85.32% 84.95% 104.5   Glorot - Normal Dist ReLU 85.26% 85.18% 99.03   He - Normal Dist ReLU 86.72% 86.37% 99.76   He - Uniform Dist ReLU 87.05% 86.27% 100.82   He - Normal Dist Leaky ReLU 86.7% 86.15% 101.87   He - Normal Dist Randomized LeakyReLU 86.67% 86.3% 113.58   LeCun SELU 87.63% 86.47% 106.25   Batch normalization He - Normal Dist ReLU 86.45% 86.85% 167.618    Summary of the result of Regression task with California housing dataset    Initialization Activation fuction Train set MSE Val set MSE Running time (seconds)     Glorot None 0.3985 0.3899 9.34   Glorot - Normal Dist ReLU 0.3779 0.3819 9.36   He - Normal Dist ReLU 0.3517 0.35 9.19   He - Normal Dist Leaky ReLU 0.3517 0.35 9.48   He - Normal Dist Randomized LeakyReLU 0.3517 0.35 10.71   LeCun SELU 0.3423 0.326 9.38   Batch normalization He - Normal Dist ReLU 0.4365 0.5728 13.64    MSE of Train and Validation set\n\r\r\r\rFashion MNIST consists of image on 10 types of fashion\r\r These performances are subject to change depending on the dataset and NN's architecture   Final thoughts on this part üîÜ  Glorot Initialization is the good starting point for most of the cases. He Initialization technique sometimes performs better than Glorot (slower in the above Classification example while faster in Regression example). ReLU or Leaky ReLU are great choices if running time is the priority. ReLU should be avoided if high Learning rate is used. SELU is the good choice for complex dataset and deep neural network but might be traded off by running time. However, if the NN's architecture does not allow self-normalization, use ELU instead of SELU. SELU and Batch Normalization cannot be applied in RNN. Gradient Clipping is the alternative strategy for Batch Normalization in RNN.   5. Transfer Learning Another important technique too improve the performance of DNN is Transfer Learning, using pretrained layers to train similar new task. There is much to say about this technique and it will be covered in another post.\nSource code can be accessed here\n Reference:\n Glorot, X., \u0026amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. PMLR He, K., Zhang, X., Ren,S., \u0026amp; Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) Geron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media, Inc., Xu, B., Wang, N., Chen, T., \u0026amp; Li, M. (2015). Empirical Evaluation of Rectified Activations in Convolutional Network. Retrieved from https://arxiv.org/abs/1505.00853 on May 5, 2020. Klambauer, G., Unterthiner, T., Mayr, A., \u0026amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. Advances in Neural Information Processing Systems 30 (NIPS 2017)  ","date":1590796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590796800,"objectID":"ae57a76991e3154512a36d44148fb763","permalink":"/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/","publishdate":"2020-05-30T00:00:00Z","relpermalink":"/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/","section":"project","summary":"Discussion of 5 popular techniques to speed up training in deep neural net (Initialization, Activation function and Batch Normalization/Gradient Clipping) using TensoFlow","tags":["Deep learning","Data Science","Initialization","Activation function","Batch Normalization","Tensorflow","Hyperparameter tuning"],"title":"Speed up training and improve performance in deep neural net","type":"project"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Data Science","Support Vector Machine","Classification"],"content":"In a classification task, there are several ways to do the trick. It can be solved by separating classes by linear (straight) line, or using a tree to split up attributes according to certain thresholds until reaching to the expected level, or calculating the probability of the event to belong to which class.\nSupport Vector Machine is a non-probabilistic binary linear classifier and a versatile Machine Learning algorithm that can perform both classification and regression tasks. Another advantages of SVM is its ability to solve on both linear and non-linear datasets.\nGiven these numerous benefits, there are many concepts and solutions in SVM that I found just a few articles/videos really gives an easily understandable explanation, especially targeting ones who are new to SVM. I hope this post reach you in the most comprehensive way.\n Original concept All of this started with the idea of using a line (with 2D dataset) or a hyperplane (more than 3D) to separate the instances into 2 classes, and try to maximize the distance between the line and the closest instances. This distance is denoted as Margin. The below figure illustrates this.\n\r\r\rWhy it needs to maximize this margin? The reason is that a Decision boundary lies right between 2 classes is much better than one that falls nearer at one class than another.\nHowever, imagine that there is an outlier of the Orange class and it lies closers to Blue class than its own. If we strictly impose the above concept to this dataset, it will result into the below picture. Now, the margin satisfies the requirement but turns out to be much smaller than the above one. This is called Hard margin.\n\r\r\rIf there is a new Blue class data that falls near this orange instance, that new data will be misclassified as Orange, which in other word, means the model performs worse on new data than on train one (which we never wants to have with our model).\n\r\r\rSoft margin There is one way to solve this, by allowing some misclassification on outliers of train set to maximize the margin on the rest of training data. This concept was named Soft margin or in other word, Support Vector Machine.\n\r\r\rOnce there is new data, it will be correctly classified as Blue.\n\r\r\rThen, one question arises. How can we decide the Soft margin? (How do we know which instances to be misclassified in training?).\nActually, there is no perfect answer for this. You train the data on several values of margin decides to use the optimal one for your problem. The hyperparameter controls this in VC models in scikit-learn is denoted as $C$. If the model is overfitting, reduce $C$.\nIt is also because that SVM uses only 1 linear line or hyperplane to do the classification job, it is a binary classification solver. In case of multiclass problem, +One-versus-All (or One-versus-Rest) strategy* will be implied.\n Therefore, one of the most important rule of SVM algorithm is that it tries to find a good balance between maximizing the margin street, and limiting the Margin violation (misclassification)\n SVM on non-linear dataset However, for non-linear separable data, how can we use this trick? Looking at the below illustration, we will need 3 lines to separate the data into 2 classes, and with more complex data, we will need even more. This is computationally inefficient.\n\r\r\rThen, here comes the Kernel trick.\nInstead of teaching the model on 2D data, the kernel trick will add other features such as polynomial features and then SVM will utilize a hyperplane to split up data into 2 classes. The above data after adding 2-degree polynomial feature will look like this:\n\rAfter quadratic feature added, instances are now distintively separated into 2 classes\r\r Let's use data to further understand this.\nimport random\rnp.random.seed(42)\rm = 500\rX1 = 2 * np.random.rand(m, 1)\rX2 = (4 + 3 * X1**2 + np.random.randn(m, 1)).ravel()\rX12 = np.column_stack((X1,X2))\ry1 = np.zeros((500))\rX3 = np.random.rand(m, 1)\rX4 = (1 + X1**1 + 2*np.random.randn(m, 1)).ravel()\rX34 = np.column_stack((X3,X4))\ry2 = np.ones((500))\rX = np.concatenate((X12, X34), axis=0)\ry = np.concatenate((y1, y2), axis=0)\rdef plot_dataset(X, y, axes):\rplt.plot(X[:, 0][y==0], X[:, 1][y==0], \u0026quot;bs\u0026quot;)\rplt.plot(X[:, 0][y==1], X[:, 1][y==1], \u0026quot;g^\u0026quot;)\rplt.axis(axes)\rplt.grid(True, which='both')\rplt.xlabel(\u0026quot;Feature 1\u0026quot;, fontsize=20)\rplt.ylabel(\u0026quot;Feature 2\u0026quot;, fontsize=20, rotation=0)\rplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\rplt.show();\r \rVizualize data with 2 classes\r\rPolynomial kernel I used SVC class in scikit-learn with polynomial kernel at 3 degree with $coef$ hyperparameter equals to 1 (it controls how much the model is influenced by high-degree vs low-degree polynomials). $LinearSVC(loss=\u0026quot;hinge\u0026rdquo;)$ with an prior $PolynomialFeatures(degree=3)$ transformer will do the same trick.\nIf you have very large dataset, go ahead with $LinearSVC$ because it is faster than $SVC$ in handling big data.\n One thing to remember, always scaling data before training SVM\n poly_kernel_svm_clf = Pipeline([\r(\u0026quot;scaler\u0026quot;, StandardScaler()),\r(\u0026quot;svm_clf\u0026quot;, SVC(kernel=\u0026quot;poly\u0026quot;, degree=3, coef0=1, C=0.001))\r])\rpoly_kernel_svm_clf.fit(X_train, y_train)\rpoly_kernel_svm_clf10 = Pipeline([\r(\u0026quot;scaler\u0026quot;, StandardScaler()),\r(\u0026quot;svm_clf\u0026quot;, SVC(kernel=\u0026quot;poly\u0026quot;, degree=3, coef0=1, C=10))\r])\rpoly_kernel_svm_clf10.fit(X_train, y_train)\r# Plot the model overall prediction\rdef plot_predictions(model, axes):\r\u0026quot;\u0026quot;\u0026quot;\rVizualize the classification result of the model to see how it\rcorresponds to training data\r\u0026quot;\u0026quot;\u0026quot;\rx0s = np.linspace(axes[0], axes[1], 1000)\rx1s = np.linspace(axes[2], axes[3], 1000)\rx0, x1 = np.meshgrid(x0s, x1s)\rX = np.c_[x0.ravel(), x1.ravel()]\ry_pred = model.predict(X).reshape(x0.shape)\ry_decision = model.decision_function(X).reshape(x0.shape)\rplt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\rplt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\rfig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\rplt.sca(axes[0])\rplot_predictions(poly_kernel_svm_clf, [-0.25,2.25,-5,20])\rplot_dataset(X_train, y_train)\rplt.title(r\u0026quot;$degree=3, C=0.001$\u0026quot;, fontsize=18)\rplt.sca(axes[1])\rplot_predictions(poly_kernel_svm_clf10, [-0.25,2.25,-5,20])\rplot_dataset(X_train, y_train)\rplt.title(r\u0026quot;$degree=3, C=10$\u0026quot;, fontsize=18)\rplt.show()\r \rVizualize data with 2 classes\r\rThe model with value of C equals to 10 seems to get to the point quite well, let's measure its performance on test set.\nfrom sklearn.metrics import f1_score\rmodel_list = [rbf_kernel_svm_clf,rbf_kernel_svm_clf10]\rfor model in model_list:\ry_pred = model.predict(X_test)\rprint(f1_score(y_test, y_pred, average='weighted'))\r0.6459770114942529\r0.8542027171311809\r Gaussian RBF Kernel Now, I want to try a different kernel with this data, I will use Gaussian RBF Kernel.\nAs my data is not too large, Gaussian RBF Kernel does not take much time. However, with a large dataset, Gaussian RBF Kernel will consume quite amount of your time.\nfrom sklearn.svm import SVC\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.pipeline import Pipeline\rfrom sklearn.model_selection import train_test_split\rX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42)\r# Create pipeline for training\rrbf_kernel_svm_clf = Pipeline([\r(\u0026quot;scaler\u0026quot;, StandardScaler()),\r(\u0026quot;svm_clf\u0026quot;, SVC(kernel=\u0026quot;rbf\u0026quot;, gamma=0.1, C=0.001))\r])\rrbf_kernel_svm_clf.fit(X_train, y_train)\rrbf_kernel_svm_clf10 = Pipeline([\r(\u0026quot;scaler\u0026quot;, StandardScaler()),\r(\u0026quot;svm_clf\u0026quot;, SVC(kernel=\u0026quot;rbf\u0026quot;, gamma=5, C=10))\r])\rrbf_kernel_svm_clf10.fit(X_train, y_train)\r# Plot the model overall prediction\rfig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\rplt.sca(axes[0])\rplot_predictions(rbf_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\rplot_dataset(X_train, y_train)\rplt.title(r\u0026quot;$gamma=5, C=0.001$\u0026quot;, fontsize=18)\rplt.sca(axes[1])\rplot_predictions(rbf_kernel_svm_clf10, [-1.5, 2.5, -1, 1.5])\rplot_dataset(X_train, y_train)\rplt.title(r\u0026quot;$gamma=5, C=10$\u0026quot;, fontsize=18)\rplt.show();\r \rVizualize data with 2 classes\r\r2 values of C seems to produce similar model. Let's predict test set and evaluate with metrics.\nfrom sklearn.metrics import f1_score\rmodel_list = [rbf_kernel_svm_clf,rbf_kernel_svm_clf10]\rfor model in model_list:\ry_pred = model.predict(X_test)\rprint(f1_score(y_test, y_pred, average='weighted'))\r0.8417207792207791\r0.8544599213495534\r As expected, 2 models perform quite equivalent with C = 10 has slightly higher value and also slightly higher than the polynomial kernel model above. We can improve this with tuning hyperparameter, cross validation, add other type of feature transformation.\n","date":1588204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588204800,"objectID":"19fd9203b20ed672d3f867dc66281965","permalink":"/project/2020-04-30-support-vector-machine-explanation-and-application/","publishdate":"2020-04-30T00:00:00Z","relpermalink":"/project/2020-04-30-support-vector-machine-explanation-and-application/","section":"project","summary":"How to make Support Vector Machine algorithm do exactly the way you want","tags":["Support Vector Machine","Classification","Application"],"title":"Support Vector Machine explanation and application","type":"project"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Visualization","Analytics"],"content":"Useful charts created with Python code\n1. Continuous variable with Categorical variable Bar chart\nShow trend/values among categorical variables.\nThis serves best in case of showing the differene between various categories.\nax = data[['x','y']].plot(kind='bar', figsize =(8,5))\rpositions = (0,1, 2, 3)\rax.set_xticklabels([\u0026quot;2015\u0026quot;, \u0026quot;2016\u0026quot;, \u0026quot;2017\u0026quot;, \u0026quot;2018\u0026quot;], rotation=30)\rax.set_title('Sales and number of order')\rfor i in ax.patches:\r# get_x pulls left or right; get_height pushes up or down\rax.text(i.get_x()+.01, i.get_height()+50, \\\rstr(round((i.get_height()), 2)), fontsize=12);\r \rBar chart: 2 categorical variables with continuous vales\r\rSubplots for multiple categorical variables\nBreaking several categories into different subplots will help generating insights, which is related to trend of each category.\nplt.figure(figsize=(20,10))\rplt.subplot(221)\rdata[data['type']==0].groupby('Y')['Quantity'].sum().plot(color='green', linewidth=7.0)\rplt.title('Item Quantity - Product class 0')\rplt.xlabel(xlabel='')\rplt.xticks([]) # delete the x axis tick value\rplt.subplot(222)\rdata[data['type']==2].groupby('Y')['Quantity'].sum().plot(color='red',linewidth=7.0)\rplt.title('Item Quantity - Product class 2')\r# Other subplot can continue with plt.subplot(223) ...\r \rLine subplot: 2 categorical variables with continuous vales\r\rThis can also be changed to Mutiple lines plot as below\nplt.plot(data['line1'], label='Line 1')\rplt.plot(data['line1'], color='red', label='Line 2')\rplt.legend()\rplt.title('2 Line plot')\rplt.show()\r \rMultiple Line plot\r\rBox plot (distribution box plot)\nTalking about distribution, boxplot will initiate many insights, especially when it is used to detect outlier.\nfig_dims = (10, 8)\rfig, ax = plt.subplots(figsize=fig_dims)\rsns.boxplot(x='X', y='Y', data=data)\r \rBox plot - Distribution vizualization\r\rPolar chart\nTHe below Polar chart used to detech seasonality among 12 months. It is clearly seen that the data at November and December observed spike or in orderword, an annual seasonality.\nimport plotly.express as px\rdata['Month'] = data['Date'].dt.month_name()\rfig = px.line_polar(data, theta=\u0026quot;Month\u0026quot;,r=\u0026quot;Weekly_Sales\u0026quot;,\rcolor='Year',\rline_close=True,template=\u0026quot;plotly_dark\u0026quot;)\rfig.show();\r \rPolar chart\r\r2. Continuous with continuous variables Scatter plot\nOne of the most popular type of plot to observe the relationship between 2 variables and sometimes help identify the correlation between features. corr function is used to get this correlation.\nfig_dims = (8,5)\rfig, ax = plt.subplots(figsize=fig_dims)\rabc = data.groupby(['A','B','C']).agg({'D':'sum'}).reset_index()\rsns.scatterplot(x='C', y='A', hue='B', data=abc, palette=\u0026quot;Set2\u0026quot;).set(title = 'Order throughout a month');\r \r\r\r3. Percentage plot Pie chart\nThere is a controversy that pie chart can hardly do a good job in representing the percentage. However, if the number of catogories are low, aka below 6, Pie chart proves no problem.\nlabels = 'G1','G2', 'G3', 'G4'\rfig1, ax1 = plt.subplots(figsize=(5,5))\rax1.pie(data.groupby('ProductClass').agg({'ItemID':'count'}), labels=labels, autopct='%1.1f%%',\rshadow=True, startangle=90)\rax1.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.\rplt.title('Proportion of each Group')\rplt.show();\r \rPie plot\r\rDonut chart (Multiple categorical variables with percentage)\nDonut chart is the combination of 2 pie chart, the smaller lies within the bigger. This shows the percentage within of the big group as well as the proportion within each subgroup, which provides a transparent distribution of 2 categorical variables within each other.\nsubgroup_names = 'PC0','PC1','PC2','PC0','PC1','PC2','PC3','PC0','PC1','PC2','PC3'\rlabels = 'Group 1','Group 2', 'Group 3'\r# Create colors\ra, b, c=[plt.cm.Blues, plt.cm.Reds, plt.cm.Greens]\rfig, ax = plt.subplots(figsize=(5,5))\rax.axis('equal')\rmypie, _ = ax.pie(list(data.groupby(['group']).agg({'Order':'nunique'}).Quantity), radius=1.3, labels=labels, colors=[a(0.6), b(0.6), c(0.6)] , labeldistance=1.05)\rplt.setp( mypie, width=0.3, edgecolor='white')\rmypie2, _ = ax.pie(list(data.groupby(['group','subgroup']).agg({'Order':'nunique'}).Quantity), radius=1.3-0.3, labels=subgroup_names, labeldistance=0.8, colors=[a(0.5), a(0.4), a(0.3), b(0.5), b(0.4), b(0.3), b(0.2),c(0.5), c(0.4), c(0.3),c(0.2)])\rplt.setp( mypie2, width=0.4, edgecolor='white')\rplt.title('Proportion of by groups and subgroups');\r \rDonut chart\r\r4. Change in Order plot Bump chart\n\u0026ldquo;How the rank changes over time\u0026rdquo; is the question that is answered by the below graph, called Bump chart\n\rChange in order\r\rdef bumpchart(df, show_rank_axis= True, rank_axis_distance= 1.1, ax= None, scatter= False, holes= False,\rline_args= {}, scatter_args= {}, hole_args= {}, number_of_lines=10):\rif ax is None:\rleft_yaxis= plt.gca()\relse:\rleft_yaxis = ax\r# Creating the right axis.\rright_yaxis = left_yaxis.twinx()\raxes = [left_yaxis, right_yaxis]\r# Creating the far right axis if show_rank_axis is True\rif show_rank_axis:\rfar_right_yaxis = left_yaxis.twinx()\raxes.append(far_right_yaxis)\rfor col in df.columns:\ry = df[col]\rx = df.index.values\r# Plotting blank points on the right axis/axes # so that they line up with the left axis.\rfor axis in axes[1:]:\raxis.plot(x, y, alpha= 10)\rleft_yaxis.plot(x, y, **line_args, solid_capstyle='round')\r#left_yaxis.annotate(x,xy=(3,1))\r# Adding scatter plots\rif scatter:\rleft_yaxis.scatter(x, y, **scatter_args)\rfor x,y in zip(x,y):\rplt.annotate(col, (x,y), textcoords=\u0026quot;offset points\u0026quot;, xytext=(0,10), ha='center') #Adding see-through holes\rif holes:\rbg_color = left_yaxis.get_facecolor()\rleft_yaxis.scatter(x, y, color= bg_color, **hole_args)\r# Number of lines\ry_ticks = [*range(1, number_of_lines+1)]\r# Configuring the axes so that they line up well.\rfor axis in axes:\raxis.invert_yaxis()\raxis.set_yticks(y_ticks)\raxis.set_ylim((number_of_lines + 0.5, 0.5))\r# Sorting the labels to match the ranks.\rleft_labels = [*range(1, len(df.iloc[0].index))]\rright_labels = left_labels\r#left_labels = df.iloc[0].sort_values().index\r#right_labels = df.iloc[-1].sort_values().index\rleft_yaxis.set_yticklabels(left_labels)\rright_yaxis.set_yticklabels(right_labels)\r# Setting the position of the far right axis so that it doesn't overlap with the right axis\rif show_rank_axis:\rfar_right_yaxis.spines[\u0026quot;right\u0026quot;].set_position((\u0026quot;axes\u0026quot;, rank_axis_distance))\rreturn axes\r 5. Other customization Add x axis tick label\ndata[['x','y']].plot(kind='bar',figsize =(8,5))\rpositions = (0,1, 2, 3)\rlabels = (\u0026quot;2015\u0026quot;, \u0026quot;2016\u0026quot;, \u0026quot;2017\u0026quot;, \u0026quot;2018\u0026quot;)\rplt.xticks(positions, labels, rotation=0) #Assign x axis tick labels\rplt.ylabel('Sales', fontsize =12)\rplt.xlabel('')\rplt.title('Sales by year');;\r \rCustome x axis tick labels\r\rSet legend label\nplt.legend(['Qty by day in week','# of daily orders'])\r To be updated\n","date":1585785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585785600,"objectID":"2aadcc8d072c11925ba5ffd896a022df","permalink":"/2020/04/02/useful-visualization-with-source-code/","publishdate":"2020-04-02T00:00:00Z","relpermalink":"/2020/04/02/useful-visualization-with-source-code/","section":"post","summary":"All visualization with code: bar chart, line chart, pie chart, violin chart, scatter chart, donut chart and customization","tags":["Visualization","Trend","Analytics"],"title":"Useful visualization with source code","type":"post"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Data Science","Time Series","Visualization"],"content":"When visualizing time series data, there are several things to be set in mind:\n Although we use the same plotting technique as for non-time-series one, but it will not work with the same implication. Reshaped data (aka lag, difference extraction, downsampling, upsampling, etc) is essential. It is informative to confirm the trend, seasonality, cyclic pattern as well as correlation among the series itself (Self-correlation/Autocorrelation) and the series with other series. Watch out for the Spurious correlation: high correlation is always a trap rather than a prize for data scientist. Many remarks this as correlation-causation trap . If you observe a trending and/or seasonal time-series, be careful with the correlation. Check if the data is a cummulative sum or not. If it is, spurious correlation is more apt to appear.  The below example with plots will give more details on this.\n 1. Time series patterns Time series can be describe as the combination of 3 terms: Trend, Seasonality and Cyclic.\nTrend is the changeing direction of the series. Seasonality occurs when there is a seasonal factor is seen in the series. Cyclic is similar with Seasonality in term of the repeating cycle of a similar pattern but differs in term of the length nd frequency of the pattern.\nThe below graph was plot simply with plot function of matplotlib, one of the most common way to observe the series\u0026rsquo; trend, seasonality or cyclic.\n\r\r\rLooking at the example figure, there is no trend but there is a clear annual seasonlity occured in December. No cyclic as there is no pattern with frequency longer than 1 year.\n2. Confirming seasonality There are several ways to confirm the seasonlity. Below, I list down vizualization approaches (which is prefered by non-technical people).\nSeasonal plot: This gives a better prove to spot seasonality, spike and drop. As seen in the below chart, there is a large jump in December, followed by a drop in January.\n\r\r\rCode can be found below (I am using the new Cyberpunk of Matplotlib, can be found here with heptic neon color)\ncolors = ['#08F7FE', # teal/cyan\r'#FE53BB', # pink\r'#F5D300'] # matrix green\rplt.figure(figsize=(10,6))\rw =data.groupby(['Year','Month'])['Weekly_Sales'].sum().reset_index()\rsns.lineplot(\u0026quot;Month\u0026quot;, \u0026quot;Weekly_Sales\u0026quot;, data=w, hue='Year', palette=colors,marker='o', legend=False)\rmplcyberpunk.make_lines_glow()\rplt.title('Seasonal plot: Total sales of Walmart 45 stores in 3 years',fontsize=20 )\rplt.legend(title='Year', loc='upper left', labels=['2010', '2011','2012'],fontsize='x-large', title_fontsize='20')\rplt.xticks(fontsize=14)\rplt.yticks(fontsize=14);\r Seasonal subseries plot Next is an another way of showing the distribution of time-series data in each month. Insteading of using histogram (which I considered difficult to understand the insight in time series), I generated box plot.\nOf note, the main purpose of this plot is to show the values changing from one month to another as well as how the value distributed within each month.\n\r\r\rBox plot is strongly recommended in case of confirming the mean, median of the seasonal period comparing to other periods.\n3. Correlation Alike other type of data, Scatter plot stands as the first choice for identifying the correlation between different time series. This is especially the case if one series can be used to explain another series. Below is the correlation of sales and its lag 1.\n\r\r\rdata_lag = data.copy()\rdata_lag['lag_1'] = data['Weekly_Sales'].shift(1) # Create lag 1 feature\rdata_lag.dropna(inplace=True) plt.style.use(\u0026quot;cyberpunk\u0026quot;)\rplt.figure(figsize=(10,6))\rsns.scatterplot(np.log(data_lag.Weekly_Sales), np.log(data_lag.lag_1), data =data_lag)\rmplcyberpunk.make_lines_glow()\rplt.title('Weekly sales vs its 1st lag',fontsize=20 );\r It is apparant that the correlation between the original data and its 1st lag is not too strong and there seems some outlier in the top left of the graph.\nIt is also interesting to identify if this correlation actually exists and can we use lag 1 to predict the original series. The correlation between the original difference and the 1st lag difference will give proof for hypothesis.\n\rThe correlation between the original difference and the 1st lag difference disappeared, indicating that lag1 does not appear to predict sales.\r\rdata_lag['lag_1_diff'] = data_lag['lag_1'].diff() # Create lag 1 difference feature\rdata_lag['diff'] = data_lag['Weekly_Sales'].diff() # Create difference feature\rdata_lag.dropna(inplace=True) plt.style.use(\u0026quot;cyberpunk\u0026quot;)\rplt.figure(figsize=(10,6))\rsns.scatterplot(data_lag['diff'], data_lag.lag_1_diff, data =data_lag)\rmplcyberpunk.make_lines_glow()\rplt.title('The correlation between original series difference with its 1st lag difference',fontsize=15);\r Moving average and Original series plot \r\r\rdef plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\rrolling_mean = series.rolling(window=window).mean()\rplt.figure(figsize=(15,5))\rplt.title(\u0026quot;Moving average\\n window size = {}\u0026quot;.format(window))\rplt.plot(rolling_mean, \u0026quot;g\u0026quot;, label=\u0026quot;Rolling mean trend\u0026quot;)\r# Plot confidence intervals for smoothed values\rif plot_intervals:\rmae = mean_absolute_error(series[window:], rolling_mean[window:])\rdeviation = np.std(series[window:] - rolling_mean[window:])\rlower_bond = rolling_mean - (mae + scale * deviation)\rupper_bond = rolling_mean + (mae + scale * deviation)\rplt.plot(upper_bond, \u0026quot;r--\u0026quot;, label=\u0026quot;Upper Bond / Lower Bond\u0026quot;)\rplt.plot(lower_bond, \u0026quot;r--\u0026quot;)\r# Having the intervals, find abnormal values\rif plot_anomalies:\ranomalies = pd.DataFrame(index=series.index, columns=series.columns)\ranomalies[series\u0026lt;lower_bond] = series[series\u0026lt;lower_bond]\ranomalies[series\u0026gt;upper_bond] = series[series\u0026gt;upper_bond]\rplt.plot(anomalies, \u0026quot;ro\u0026quot;, markersize=10)\rplt.plot(series[window:], label=\u0026quot;Actual values\u0026quot;)\rplt.legend(loc=\u0026quot;upper left\u0026quot;)\rplt.grid(True)\rplotMovingAverage(series, window, plot_intervals=True, scale=1.96,\rplot_anomalies=True)\r ACF / PACF plots (Autocorrelation / Partial Autocorrelation plots) First, talking about Autocorrelaltion, by definition,\n Autocorrelation implies how data points at different points in time are linearly related to one another.\n The blue area represents the distance that is not significant than 0 or the critical region, in orther word, the correlation points that fall beyond this area are significantly different than 0, and these the points needed our attention. This region is same for both ACF and PACF, which denoted as $ \\pm 1.96\\sqrt{n}$\nThe details of ACF and PACF plot implication and how to use them for further forecast can be found here\n\rACF shows a significant negativve correlation at lag 3 and no positive correlation, indicating that the series has no correlation with its previous values. PACF reveals that lag 3, lag 6, lag 9, lag 18 and probably lag 19 are important to the original series\r\r# ACF and PACF for time series data\rseries=train.dropna()\rfig, ax = plt.subplots(2,1, figsize=(10,8))\rfig = sm.graphics.tsa.plot_acf(series, lags=None, ax=ax[0])\rfig = sm.graphics.tsa.plot_pacf(series, lags=None, ax=ax[1])\rplt.show()\r Actual vs Predicted values plot \rActual vs Predicted values plot\r\rdef plotModelResults(model, X_train, X_test, y_train, y_test, plot_intervals=False, plot_anomalies=False):\rprediction = model.predict(X_test)\rplt.figure(figsize=(12, 8))\rplt.plot(prediction, \u0026quot;g\u0026quot;, label=\u0026quot;prediction\u0026quot;, linewidth=2.0, color=\u0026quot;blue\u0026quot;)\rplt.plot(y_test.values, label=\u0026quot;actual\u0026quot;, linewidth=2.0, color=\u0026quot;olive\u0026quot;)\rif plot_intervals:\rcv = cross_val_score(model, X_train, y_train, cv=tscv, scoring=\u0026quot;neg_mean_absolute_error\u0026quot;)\rmae = cv.mean() * (-1)\rdeviation = cv.std()\rscale = 1\rlower = prediction - (mae + scale * deviation)\rupper = prediction + (mae + scale * deviation)\rplt.plot(lower, \u0026quot;r--\u0026quot;, label=\u0026quot;upper bond / lower bond\u0026quot;, alpha=0.5)\rplt.plot(upper, \u0026quot;r--\u0026quot;, alpha=0.5)\rif plot_anomalies:\ranomalies = np.array([np.NaN]*len(y_test))\ranomalies[y_test\u0026lt;lower] = y_test[y_test\u0026lt;lower]\ranomalies[y_test\u0026gt;upper] = y_test[y_test\u0026gt;upper]\rplt.plot(anomalies, \u0026quot;o\u0026quot;, markersize=10, label = \u0026quot;Anomalies\u0026quot;)\rerror = mean_absolute_percentage_error(y_test,prediction)\rplt.title(\u0026quot;Mean absolute percentage error {0:.2f}%\u0026quot;.format(error))\rplt.legend(loc=\u0026quot;best\u0026quot;)\rplt.tight_layout()\rplt.grid(True);\rplotModelResults(linear, X_train, X_test, y_train, y_test,\rplot_intervals=True, plot_anomalies=True)  To be updated\n","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585526400,"objectID":"f1ebf921c3fac90fdb70cb7c224819bc","permalink":"/project/2020-03-30-complete-guide-for-time-series-visualization/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/project/2020-03-30-complete-guide-for-time-series-visualization/","section":"project","summary":"Exploring Time series with visualization and identify neccessary trend, seasonality, cyclic in order to prepare for time series forecast","tags":["Time series","Data Science","Forecasting","Visualization","Seasonality","Trend","Spurious correlation"],"title":"Complete guide for Time series Visualization","type":"project"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Machine Learning","NLP"],"content":"My 2020 started awesomely with a Machine Learning Forum @Google Japan and followed with 2 Meetups about Natural Language Processing application. NLP has been the focus of the Machine Learning society for the last decade and it is reaching to its ultimate point with several outbreak of innovations and application.\nAt this moment, 2020, NLP is heading towards speed and big data, which means that the increasing of the speed and size of data is the key objective of future NLP innovations. At the moment, I am impressed with BERT - Bidirectional Encoder Representations from Transformers, a powerful state-of-the-art NPL model introduced by Google in 2018; and *Google Meena, a lift toward dealing with big-NLP database using the Transformer base introducted on Jan 28, 2020.\n1.\tWhy are BERT and Transformer being called the revolution of the NLP world? To understand Transformer model inclduing BERT, we need to take a look of the progress from Seq2Seq (sequence to sequence) and its evolution to attention and to BERT.\n1.1 Seq2Seq model: In NLP, the end that a machine is expected to understand is the meaning of the sentence, not only word by word. Seq2Seq is a technique to train the machine in which it takes a sequence of an item and generates another sequence as output.\nWithin the model, there contains Encoder and Decoder. The Encoder receives the original text and convert the text into a Context vector that the Machine can read. Then, the Decoder does the job of generating a new sequence of items based on the Context vector.\nIn Seq2Seq, a sentence does not need to go through both Encoder and Decoder, it can stop at Encoder. Some example of Encoder only is the suggested word \u0026ldquo;message\u0026rdquo; after you type \u0026ldquo;Thank you for your\u0026rdquo;.\nContext or Context vector is a vector of floats representing the input sequence. \u0026ldquo;Word Embedding\u0026rdquo; is the algorithm used to transform text into vector, and the size of vector is usually 265, 512 or 1024 dimensions.\n1.2 Attention One of the disadvantages of Context in Seq2Seq is the dealing with long sentences and handling the sequence of output. Context is generated by the Word embedding algorithm and the longer the sentence or the paragraph, the bigger the size of vector and the more memory consuming.\nMoreover, the context in Seq2Seq was not built to figure out the similarity between words because it does not focus on the relevancy of the words in the sentence. This leads to the issue that a sentence in English cannot be translated correctly to Japanese which has the reserve order in sentence structure.\nThe concept of Attention was introduced in Bahdanau et al., 2014 and Luong et al., 2015 in which it takes into account the relevant parts in the sentence.\nInstead of passing the last hidden state to the Decoder, the Attention model passes all the hidden states to the Decoder with the summary process as below:\n Give each hidden state a score Use Softmax function to multiply each hidden state. This brings about high Hidden state scores and low hidden state scores or in other word, it generates the probability of each hidden state associating with the input word. The Decoder will sum up all the weighted softmax Hidden state vectors into a context vector and concatenate it with its original hidden state vector.  The advantage of this model is the capability to choose the decoding word based on the probability of that word in associating with the original input without losing the sequential characteristics of the sentence. This has high effectiveness in dealing with translating of common words such as \u0026ldquo;the\u0026rdquo;, \u0026ldquo;his\u0026rdquo;, \u0026ldquo;of\u0026rdquo;, etc. and the sequence of different languages.\n1.3 Transformer: Transformer is built on the foundation of Attention model. Therefore, Transformer can deal with the relevancy of the sentence rather than converting from word to word in Seq2Seq.\nThe biggest difference of Transformer vs Seq2Seq is that instead of generating 1 vector from Encoder, Transformer model uses 3 vectors in order to decide which other parts of the sentence are important (or unimportant) to that word.\nThe below table show more details of the calculation.\n   Word Q vector K vector V vector Score Softmax Sum     First word         I Q1 K1 V1 Q1xK1 S1=Q1xK1/8xV1    love  K2 V2 Q1xK2 S2=Q1xK2/8xV2 Z1 = S1+S2+S3   data  K3 V3 Q1xK3 S3=Q1xK3/8xV3             Second word         I  K1 V1 Q2xK1 S1=Q2xK1/8xV1    love Q2 K2 V2 Q2xK2 S2=Q2xK2/8xV2 Z2 = S1+S2+S3   data  K3 V3 Q2xK3 S3=Q2xK3/8xV3             Third word         I  K1 V1 Q3xK1 S1=Q3xK1/8xV1    love  K2 V2 Q3xK2 S2=Q3xK2/8xV2 Z3 = S1+S2+S3   data Q3 K3 V3 Q3xK3 S3=Q3xK3/8xV3      One example of this model application is the suggestion of relevant words when typing sentence. Gmail can suggest \u0026ldquo;message\u0026rdquo;, \u0026ldquo;reply\u0026rdquo;, \u0026ldquo;call\u0026rdquo; at the same time based on the typed sentence \u0026ldquo;Thank you for your\u0026quot;.\n This is a brief introduction on the transition from Seq2Seq to Transformer and how the Transformer model outstands Seq2Seq at the moment.\n2. Business Applications of NLP   Chatbot: This is obviously the forefront application of NLP, which can be seen across all industries and companies. Given its popularity, there are several tools to support building a chatbot such as Google DialogFlow, Microsoft LUIS. These tools can be customized based on the user's needs; however, they can only deal with simple requests.\n  Machine translation: such as Google translate or pocket translator device.\n  Search engine: 5 years ago, when you searched something on search engine, whether you type key words \u0026ldquo;to Tokyo\u0026rdquo; or the whole sentences \u0026ldquo;How to go to Tokyo\u0026rdquo;, the machine would generate quite similar results. However, with the evolution of BERT and Transformer, searching the whole sentence will throw you to a better search result.\n  Monitoring of brand and product - Sentiment analysis: This is the field that I used to analyze during my first job. I used sentiment analysis on big scale online platforms including online forums, social network, brand website and e-commerce sites to understand the reaction of consumers toward a campaign or a brand in order to react promptly toward negative trend related to the brand.\n  Place to display an advertisement: display ads based on context or categorization and make sure that the article is appropriate at the placing place. Honestly saying, I have not seen much of this application around.\n  Remarketing: an online advertisement based on the browsing history of a user to target them with similar advertising product to drive them back the previous interest. This personalized application is a very effective tool in today online market in which thousands of sellers trying to attract each of their customers. Youtube, Facebook or Google are the biggest applicators.\n  Medical assistant: although called \u0026ldquo;assistant\u0026rdquo;, the major task of this service is to transcript the discussion between doctors and patients.\n  Text generation: this is one of the applications of Decoding in NLP, in which the machine will generate a complete article from what it was learnt or summarize a paragraph. As you may know, there are many contradictions about this application, especially the emergence of fake news in recent years. With the completion of this technology, whether the fake news issue continues its expansion or is stopped is still a big question.\n  Information extraction: extract dynamic required information that is sleeping in the database system.\n  Resume reviews: use NLP to scan the applicants\u0026rsquo; resume to figure out potential candidate for the interview. This application sticks with Amazon big scandal. Amazon used to use this to scan the resume which led to the inequality between male and female with the result preferred male than female. This is due to 0the bias toward male in the training set of the model.\n  Voice command: an emerging technology in recent years with the appearance of smart device such as Siri in Iphone, Alexa of Amazone, Google home or Cortana of Microsoft.\n  Beside the major technological trends mentioned above, the application trend is heading toward diversity in languages and translation efficacy. Moreover, not only applying NLP alone, there are more applications combining NLP and Voice recognition or Computer Vision.\nNLP is a powerful Machine Learning area and its application is supporting human's life even more than we can expect. Therefore, NLP is one of the most used Machine Learning fields by Data Scienctist.\n","date":1581465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581465600,"objectID":"53c766c2f2f04d7cd79b0e5c75b32c69","permalink":"/2020/02/12/the-beauty-of-transformer-in-bringing-more-applications-to-life/","publishdate":"2020-02-12T00:00:00Z","relpermalink":"/2020/02/12/the-beauty-of-transformer-in-bringing-more-applications-to-life/","section":"post","summary":"What is Transformer in today NLP outlook and how does Natural Language Processing contribute to our life","tags":["NLP","Data Science","Machine learning","Concept","Application"],"title":"The beauty of Transformer in bringing more applications to life","type":"post"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Data Science","Time Series","Forecasting","ARIMA"],"content":"1. What is Time Series Time series is a sequence of value corresponding with time. Retail sales data, Daily temperature, production, demand, natural reserves are time series data because the later values depend on their historical values.\n2. What makes up Time Series There are 4 components in Time Series: Level, Trend, Seasonality and Noise.\n Level: the average value of the time series Trend: The movement of the series values from 1 period to another period Seasonality: The short-term cyclical behavior of the series that can be observed several times Noise: the random variation that results from the measurement of error  It is not always that we will be able to distinguish the first 3 elements from Noise because they are usually invisble which need some techniques to be noticeable\nTo observe and identify the existence of these components, we can consider.\n Plot the Time series (this is the best way to detect the characteristics of the series) Zoom in a specify shorter period of time Change scale of the series to observe the trend more clearly Suppress seasonality: aggregate the time series to a bigger time scale (from hourly scale to daily scale, from monthly scale to yearly scale, etc.)  I used 3-year weekly sales of a Retail store as an illustration.\nplt.plot(\u0026quot;Date\u0026quot;, \u0026quot;Weekly_Sales\u0026quot;, data=Wal_sales)\rplt.hlines(y=Wal_sales.Weekly_Sales.mean(), xmin=0, xmax=len(Wal_sales), linestyles='dashed')\r With this data, there are obviously 2 peaks which denotes quite a clear seasonality at the end of year (probably Christmas and New year period). There might be other sesonality but it is hard to observe it from the plot. Auto correlation can be used to confirm the seasonality.\n3. Autocorrelation Autocorrelation describes the connection between the value of time series and its neighbors. Thus, to compute Autocorrelation, we calculate the correlation of the series with its lagged versions. Lag-n version is produced from the original dataset by moving the series values forward n period. For example, lag-1 is moved forward 1 period, Lag-10 series is moved forward 10 periods.\nBy observing the correlation of the series and its lags, we can confirm the seasonality of the series.\nauto_cor = sales.groupby(\u0026quot;Date\u0026quot;)[\u0026quot;Weekly_Sales\u0026quot;].sum()\rauto_cor = pd.DataFrame(auto_cor)\rauto_cor.columns = [\u0026quot;y\u0026quot;]\r# Adding the lag of the target variable from 1 steps back up to 52 (due to a seasonality at the end of the year)\rfor i in range(1, 53):\rauto_cor[\u0026quot;lag_{}\u0026quot;.format(i)] = auto_cor.y.shift(i)\r# Compute autocorrelation of the series and its lags\rlag_corr = auto_cor.corr()\rlag_corr = lag_corr.iloc[1:,0]\rlag_corr.columns = [\u0026quot;corr\u0026quot;]\rorder = lag_corr.abs().sort_values(ascending = False)\rlag_corr = lag_corr[order.index]\r# Plot the Autocorrelation\rplt.figure(figsize=(12, 6))\rlag_corr.plot(kind='bar')\rplt.grid(True, axis='y')\rplt.title(\u0026quot;Autocorrelation\u0026quot;)\rplt.hlines(y=0, xmin=0, xmax=len(lag_corr), linestyles='dashed')\r  Judging from the Autocorrelation plot above, there is a strong positive autocorrelation in lag-52 as well as lag-51 as we expected when observing the time series plot. This implies a cyclical annual pattern at the end of the year. The second strong correlation is lag-1, which connotes as the second week of february, or Valentine period. The autocorrelation reveals both Positive and Negative autocorrelation, which implies that the series does not move in the same direction but ups and downs.  Autocorrelation can plotted easily through using autocorrelation_plot function from pandas.plotting in Python or acf function from tseries package in R.\n4. Forecasting Time series There are several methods to forecast Time series.\n Model-based method through multiple linear regression to explore the correlation of the series with other features. Alike other cross-sessional data, model-based method compute the dependence of the time series to other features, but does not take into account the dependence between time series values within different periods. Data-driven method in which learns the pattern from the data itself and estimate the next value of the time series in correspondence with its previous values. The data-driven method is important in time series given in the time series context, the values in adjoining period tend to be correlated with each other. Such correlation is denoted as Autocorrelation. Combining method by forecasting the future values of the series as well as the future value of residual that generated from the first forecasting model, and then combine the result of 2 forecast together. The residual forecast acts as the correct for the first forecast. Ensembles method by averaging multiple methods to get the result  Forecasting using Data-driven method: ARIMA model is the most frequent choice to compute data-driven forecasting. You can find detail for ARIMA model in this post. Here I will apply the ARIMA to the data.\nIt is useful to use *:auto_arima**function from pmdarima in Python or auto.arima function from forecast packgage in R.\nThere is one thing to note is that from the Autocorrelation above, there is a clear seasonality at lag 52 so we will need to include this into the ARIMA model.\nstepwise_model = pm.auto_arima(Wal_sales.iloc[:,1].values, start_p=1, start_q=1,\rmax_p=20, max_q=20, m=52,\rstart_P=0, seasonal=True,\rd=1, D=1, trace=True,\rerror_action='ignore', suppress_warnings=True, stepwise=True)\rprint(stepwise_model.aic())\r Result\nPerforming stepwise search to minimize aic\rFit ARIMA: (1, 1, 1)x(0, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds\rFit ARIMA: (0, 1, 0)x(0, 1, 0, 52) (constant=True); AIC=2898.191, BIC=2903.190, Time=0.423 seconds\rFit ARIMA: (1, 1, 0)x(1, 1, 0, 52) (constant=True); AIC=2883.839, BIC=2893.839, Time=5.555 seconds\rFit ARIMA: (0, 1, 1)x(0, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds\rFit ARIMA: (0, 1, 0)x(0, 1, 0, 52) (constant=False); AIC=2907.540, BIC=2910.039, Time=0.371 seconds\rFit ARIMA: (1, 1, 0)x(0, 1, 0, 52) (constant=True); AIC=2893.265, BIC=2900.764, Time=0.807 seconds\rFit ARIMA: (1, 1, 0)x(2, 1, 0, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds\rFit ARIMA: (1, 1, 0)x(1, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds\rFit ARIMA: (1, 1, 0)x(0, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds\rFit ARIMA: (1, 1, 0)x(2, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds\rFit ARIMA: (0, 1, 0)x(1, 1, 0, 52) (constant=True); AIC=2890.759, BIC=2898.258, Time=7.666 seconds\rFit ARIMA: (2, 1, 0)x(1, 1, 0, 52) (constant=True); AIC=2884.464, BIC=2896.963, Time=7.595 seconds\rFit ARIMA: (1, 1, 1)x(1, 1, 0, 52) (constant=True); AIC=2884.895, BIC=2897.394, Time=20.608 seconds\rFit ARIMA: (0, 1, 1)x(1, 1, 0, 52) (constant=True); AIC=2883.040, BIC=2893.039, Time=6.410 seconds\rFit ARIMA: (0, 1, 1)x(0, 1, 0, 52) (constant=True); AIC=2893.770, BIC=2901.269, Time=5.440 seconds\rFit ARIMA: (0, 1, 1)x(2, 1, 0, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds\rFit ARIMA: (0, 1, 1)x(1, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds\rFit ARIMA: (0, 1, 1)x(2, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds\rFit ARIMA: (0, 1, 2)x(1, 1, 0, 52) (constant=True); AIC=2887.816, BIC=2900.315, Time=7.108 seconds\rFit ARIMA: (1, 1, 2)x(1, 1, 0, 52) (constant=True); AIC=2889.929, BIC=2904.928, Time=17.358 seconds\rTotal fit time: 79.418 seconds\r2883.039997060003\r This fuction chose the lowest AIC-score model and embed it for further model usage.\nSplit train-test set, train the model and make prediction.\n#Split train and test\rtrain = Wal_sales.iloc[:106,1].values\rtest = Wal_sales.iloc[106:,1].values\r# Train the model\rstepwise_model.fit(train)\r# Predict test set\rpred = stepwise_model.predict(n_periods=37)\r# Reframe the data\rtest_pred = Wal_sales.iloc[106:,:2]\rtest_pred[\u0026quot;Predict_sales\u0026quot;] = np.array(pred,dtype=\u0026quot;float\u0026quot;)\r# Visualize the prediction\rplt.figure(figsize=(12,8))\rplt.plot( 'Date', 'Weekly_Sales', data=Wal_sales, markersize=12, color='olive', linewidth=3)\rplt.plot( 'Date', 'Predict_sales', data=test_pred, marker='', color='blue', linewidth=3)\rplt.title(\u0026quot;Predicted sales vs Actual sales\u0026quot;)\rplt.legend()\rprint(\u0026quot;MAPE score: \u0026quot;, mean_absolute_percentage_error(test, pred))\r  In overal, the model seem to work moderately on the data but there is still room to improve further. The MAPE (mean absolute percentage error) score is 5.7%, which is not too high, not too low. The ARIMA model seems to perform well in early predicted value and gets worse in later predicted values.  One question emerged, does this model truly capture the values of the time series data? It is helpful to take a look at the Residual of the model (or the diference between predicted values and actual values). Examining the residuals of the forecasting model is suggested to evaluate whether the specified model has adequately captured the information of the data. This can be done through exploring the correlation of one period's residual with other periods\u0026rsquo; ones.\nThe Residuals of a good time series forecasting model have the following properties:\n Residuals are uncorrelated Residuals have zero or nearly-zero mean (which means the model is unbiased in any directions) Residuals should have normal distribution Residuals should have constant variance  If the result is lack of any of the above attributes, the forecasting model can be further improved.\nLet's compute the Residuals Autocorrelation and judge the result.\n# Compute Residual\rtrain_pred = stepwise_model.predict(n_periods=106)\rr_train = train - train_pred\rr_test = test - pred\rresidual = pd.DataFrame(np.concatenate((r_train,r_test)), columns={\u0026quot;y\u0026quot;})\r# Generate lag of Residuals from 1 step to 52 steps\r# Adding the lag of the target variable from 1 steps back up to 52 for i in range(1, 53):\rresidual[\u0026quot;lag_{}\u0026quot;.format(i)] = residual.y.shift(i)\r# Compute correlation of the Residual series and its lags\rlag_corr = residual.corr()\rlag_corr = lag_corr.iloc[1:,0]\rlag_corr.columns = [\u0026quot;corr\u0026quot;]\rorder = lag_corr.abs().sort_values(ascending = False)\rlag_corr = lag_corr[order.index]\r# Plot the Residual Autocorrelation\rplt.figure(figsize=(12, 6))\rlag_corr.plot(kind='bar')\rplt.grid(True, axis='y')\rplt.title(\u0026quot;Autocorrelation\u0026quot;)\rplt.hlines(y=0, xmin=0, xmax=len(lag_corr), linestyles='dashed')\r Other criteria:\n# Residual mean and Distribution\rprint(\u0026quot;Residual mean: \u0026quot;,residual.iloc[:,0].mean())\rplt.hist(residual.iloc[:,0], bins=20)\rplt.title(\u0026quot;Residual Distribution\u0026quot;)\r Residual mean: -6308833.905274585\r # Residual variance\rplt.plot(residual.iloc[:,0])\rplt.title(\u0026quot;Residual\u0026quot;)\rplt.hlines(y=0, xmin=0, xmax=len(residual), linestyles='dashed')\r Let's judge the Autocorrelation of Residual based on the following criteria:\n Residuals are uncorrelated: the Residual series is still observed some correlations with its lags. Residuals have zero or nearly-zero mean (which means the model is unbiased in any directions): the mean is -6308833.905274585. So this criteria is not met. Residuals should have normal distribution: Not quite a normal distribution Residuals should have constant variance: No as consistent with mean does not equal to 0.  Hence, the forecasting model has a lot of rooms to improve further by finding the way to capture the correlation in the Residuals, adding the values that is currently staying in residuals to the prediction.\n5. Data partitioning One of the biggest characteristics of Time series distinguishing it with normal cross-sessional data is the dependence of the future values with their historical values. Therefore, the Data partitioning for Time series cannot be done randomly but instead, trim the series into 2 periods, the earlier to train set and the later to validation set.\nThe below code will help split the tran-test sets with respect to time series structure.\n# Split train and test sets in correspondence with Time series data\rdef ts_train_test_split(X, y, test_size):\rtest_index = int(len(X)*(1-test_size))\rX_train = X.iloc[:test_index]\ry_train = y.iloc[:test_index]\rX_test = X.iloc[test_index:]\ry_test = y.iloc[test_index:]\rreturn X_train, X_test, y_train, y_test\r Sales does not only correlated with its own past but also might be affected by other factors such as special occasions (i.e Holiday in this dataset), weekday and weekend, etc\u0026hellip; The method-driven models will be presented in the next article with feature extraction, feature selection.\n","date":1580947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581638400,"objectID":"585c439f050037eaf642e83dc5766f31","permalink":"/project/2020-03-05-time-series-forecasting/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/project/2020-03-05-time-series-forecasting/","section":"project","summary":"Denote characteristics of the Time series and forecast its future values using Retail sales data as example code","tags":["Time series","Forecasting","Retail sales","ARIMA"],"title":"Comprehensive understanding on Time Series forecasting","type":"project"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Data Science","Forecasting","ARIMA"],"content":"1.\tConcept Introduction Auto Regressive Integrated Moving Average: \u0026lsquo;explains\u0026rsquo; a given time series based on its own past values. ARIMA is expressed as $ARIMA(p,d,q)$\nThe evolution of ARIMA started with the model ARMA or Auto Regressive Moving Average. However, this model does not include the Integrated term, or differencing order (I'll talk about this later on) so this model can only be used with Stationary data. For non-stationary data, we will use ARIMA.\nThere are 3 parts in the ARIMA model: Auto Regressive (AR) $p$, Integrated (I) $d$, Moving Average (MA) $q$\n  Integrated (Or stationary): A time series which needs to be differenced to become stationary is the integrated version of stationary series. One of the characteristics of Stationary is that the effect of an observation dissipated as time goes on. Therefore, the best long-term predictions for data that has stationary is the historical mean of the series.\n  Auto Regressive: is simply defined a the linear or non-linear model between current value of the series with its previous values (so called lags), and there are unlimited number of lags in the model. The basic assumption of this model is that the current series value depends on its previous values. This is the long memory model because the effect slowly dissipates across time. p is preferred as the maximum lag of the data series. The AR can be denoted as $Y_{t}=\\omega_{0}+\\alpha_{1}Y_{t-1}+\\alpha_{2}Y_{t-2}+\u0026hellip;+\\xi$\n  Moving Average: deal with \u0026lsquo;shock\u0026rsquo; or error in the model, or how abnormal your current value is compared to the previous values (has some residual effect). The MA is denoted as $Y_{t}=m_1\\xi_{t-1}+\\xi_t$\n  $p$, $d$, and $q$ are non-negative integers;\n $p$: The number of Autoregressive terms. Autoregressive term is the lag of the staionarized series in the forecasting equation. $d$: the degree of differencing (the number of times the data have had past values subtracted). $q$: the order of the moving-average terms (The size of the moving average window) or in order word, the lag of the forecast errors  A value of 0 can be used for a parameter, which indicates to not use that element of the model. When two out of the three parameters are zeros, the model may be referred to non-zero parameter. For example, $ARIMA (1,0,0)$ is $AR(1)$ (i.e. the ARIMA model is configured to perform the function if a AR model), $ARIMA(0,1,0)$ is $I(1)$, and $ARIMA(0,0,1)$ is $MA(1)$\n2. Is the data predictable? One of the key important thing to define before fitting ro forecast any sets of data is confirm whether the data is predictable or it is just a \u0026ldquo;Random Walk\u0026rdquo;.\nRandom walk means the movement of the data is random and cannot be detected. The Random Walk is denoted as $ARIMA(0,1,0)$. If the data is not stationary, Random walk is the simplest model to fit\nThe forecasting equation for Random Walk is:\n$\\hat{Y_{t}}-Y_{t-1}=\\mu$\nIn other word, Random walk is the $AR(1)$ model with coefficient $\\beta_1=0$.\nTherefore, to test this hypothesis, we use hypothesis testing with null hypothesis $H_0 = 1$ vs. $H_1 \\neq 1 $. The $AR(1)$ model is fitted to the data and we examine the coefficient. If the coefficient is statistically significantly different than 1, we can conclude that the data is predictable and vice versa.\nLet's work with some data.\nplt.figure(figsize=(15,8));\rdata.plot()\r \rFigure 1: Weekly Sales of suppermarket A from 2010 to 2019\r\rDistribution\n\rFigure 2: Distribution of time series weekly sales\r\rThese plots show a high probability that the data is not Stationary.\nOn the other hand, this data shows a seasonlity trend so instead of ARIMA, wI will use SARIMA, another seasonal-detected ARIMA model.\nSARIMA is denoted as $SARIMA(p,d,q)(P,D,Q)m$\n3. Confirm the data's Stationarity It is essential to confirm the data to be stationary or not because this impacts directly to your model selection for the highest accuracy.\nThere are several methods to examine the data. One of the most statistical accurate way is the Augmented Dicky-Fuller method in which it tests the data with 2 hypothesis. The Null hypothesis is not staionary and the alternative hypothese is stationary.\n# Run test\rseries = data.values\rresult = adfuller(data)\rprint('ADF Statistic: %f' % result[0])\rprint('p-value: %f' % result[1])\rprint('Critical Values:')\rfor key, value in result[4].items():\rprint('\\t%s: %.3f' % (key, value))\r ADF Statistic: -1.557214\rp-value: 0.505043\rCritical Values:\r1%: -3.492\r5%: -2.889\r10%: -2.581\r p-value is higher than 0.05 so we fail to reject the Null hypothesis which means the data is stationary.\n4. Differencing the data Differencing is the methid to stationarize the time series data.\nThere is quite a clear 3-month seasonality with this data so I'll conduct 3 month seasonaliry differencing.\n# Difference the orginal sales data\rplt.figure(figsize=(15,8));\rtrain_diff_seasonal = train - train.shift(3)\rplt.plot(train_diff_seasonal)\r# Conduct the test\rseries = train_diff_seasonal.dropna().values\rresult = adfuller(series)\rprint('ADF Statistic: %f' % result[0])\rprint('p-value: %f' % result[1])\rprint('Critical Values:')\rfor key, value in result[4].items():\rprint('\\t%s: %.3f' % (key, value))\r ADF Statistic: -3.481334\rp-value: 0.008480\rCritical Values:\r1%: -3.529\r5%: -2.904\r10%: -2.590\r \rFigure 3: Seasonal differencing with order of 3\r\rThe data became stationary with p-value of the test is less than 0.05. Let's examine ACF and PACF of the data\n# Split train, validation and test sets\rtrain = data[:84]\rvalidation = data[84:108]\rtest = data[108:]\r# ACF and PACF for orginal data\rseries=train.dropna()\rfig, ax = plt.subplots(2,1, figsize=(10,8))\rfig = sm.graphics.tsa.plot_acf(series, lags=None, ax=ax[0])\rfig = sm.graphics.tsa.plot_pacf(series, lags=None, ax=ax[1])\rplt.show()\r \rFigure 4: ACF and PACF of orginal tiem series weekly sales\r\rSome observation\nWith the p-value from the test is now significantly lower than 0.05, and the number of significantly peaks in ACF has dropped, the data has become stationary. Let's set the parameters for SARIMA\n $p$ is most probably 3 as this is the last significant lag on the PACF. $d$ should equal 0 as we do not have differencing (only seasonal differencing and this will be reflected later on) $q$ should be around 3 $P$ should be 2 as 3th, and 9th lags are somewhat significant on the PACF $D$ should be 1 as we performed seasonal differencing $Q$ is probably 2 as the 3th lag and 9th lag are significant in ACF plot while other 6th and 9th lags are not.   It is not suggestable to use only ACF and PACF plots to decide the value within ARIMA model. The reason is that ACF and PACF are useful in case either $p$ or $q$ is positive. In a situation that both $p$ and $q$ are positive, these 2 plots will give no value.\n The $ARIMA(p,d,0)$ is decided given the following conditions observed from ACF and PACF plots:\n ACF is exponentially decaying There is a significant spike at lag $p$ in the PACF, but none beyond lag $p$  For $ARIMA(0,d,q)$:\n PACF is exponentially decaying There is a significant spike at lag $q$ in the PACF, but none beyond lag $q$   TIP The ACF of stationary data should drop to zero quickly. For nonstationary data the value at lag 1 is positive and large.\n Another way to have an idea for which $p$ and $q$ values in $ARIMA$ model are opt to be used is through grid search with assigned parameter to identify the optimal comnbination based on score (aka AIC and BIC)\nps = range(3,5)\rd= 0\rqs = range(2,5)\rPs= range(1,4)\rD=1\rQs=range(0,3)\rs=6 # annual seasonality\rparameters = product(ps,qs,Ps, Qs)\rparameters_list = list(parameters)\rresult_table = optimizeSARIMA(parameters_list, d, D, s)\r# set the parameters that give the lowest AIC\rp, q, P, Q = result_table.parameters[0]\rbest_model=sm.tsa.statespace.SARIMAX(data, order=(p, d, q),\rseasonal_order=(P, D, Q, zs)).fit(disp=-1)\rprint(best_model.summary())\r# Examine the residuals\r# ACF and PACF for orginal data\rplt.plot(best_model.resid)\rfig, ax = plt.subplots(2,1, figsize=(10,8))\rfig = sm.graphics.tsa.plot_acf(best_model.resid, lags=None, ax=ax[0])\rfig = sm.graphics.tsa.plot_pacf(best_model.resid, lags=None, ax=ax[1])\rplt.show()\r \rFigure 5: ACF and PACF plots of Residuals\r\rLag-1 of the residual in PACF still shows the sign of autocorrelation which implies that it needs more adjustment with the model.\nBelow is the General process for forecasting using an ARIMA model (Source: Hyndman, R.J., \u0026amp; Athanasopoulos, G. )\n\rFigure 6: General process for forecasting using an ARIMA model\r\r5. Model evaluation There are 2 common measures to evaluate the predicted values with the validation set.\n1.\tMean Absolute Error (MAE): \u0026hellip;How far your predicted term to the real value on absolute term. One of the drawbacks of the MAE is because it shows the absolute value so there is no strong evidence and comparison on which the predicted value is actually lower or higher.\n$MAE=\\frac{1}{n}\\sum_{i = 1}^{n} |Y_{t}-\\hat{Y_{t}}|$\ncan be run with R\nmean(abs(Yp - Yv))\r or in Python\nfrom sklearn import metrics\rmetrics.mean_absolute_error(y_test, y_pred)\r 2. Mean absolute percentage error (MAPE):\nThe MAE score shows the absolute value and it is hardly to define whether that number is good or bad, close or far from expectation. This is when MAPE comes in.\nMAPE measures how far your predicted term to the real value on absolute percentage term.\n$MAPE=100\\frac{1}{n}\\sum_{i = 1}^{n} \\frac{|Y_t-\\hat{Y_t}|} {\\hat{Y_{t}}}$\nCan compute as\n100 x mean(abs(Yp - Yv) / Yv )\r  Reference\nHyndman, R.J., \u0026amp; Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on March 31, 2020\n","date":1579996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585612800,"objectID":"4f3e9cb0895ac80df3f7503a9c06b215","permalink":"/project/2020-01-26-arima-autoregressive-intergreated-moving-average/","publishdate":"2020-01-26T00:00:00Z","relpermalink":"/project/2020-01-26-arima-autoregressive-intergreated-moving-average/","section":"project","summary":"Comprehensive summary of ARIMA model and how to apply it to forecasting","tags":["Data Science","Forecasting","ARIMA"],"title":"ARIMA Autoregressive Integrated Moving Average model family","type":"project"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Data Science"],"content":"Have you ever wondered why you, a talented Data Scientist, is not an adviser or a consultant for business, but just a normal employee spending time solving other's requests?\nThis is such a common complaint that I heard so many times from my friends, my fellows and especially from the junior. Years ago during the beginning of my career, I was also a victim of this trap. Now, I realized that this is a real bottleneck of Data Scientist and I want to share my experience to others so that it will help other Data Scientist achieving a brighter career.\nWhenever I meet other Data Science fellows, most of the time was spent talking about RNN, NLP, Deep learning, or Machine learning algorithms. But when I called on a question of why they used RNN instead of Deep learning or how their model supports the business, they could either provide an unconvincing reason or stay on a lengthy explanation of concept, algorithm without a comprehensive business thinking.\nIt is a routine for a Data Scientist to saturate with technical models while understates the role of business mindset. However, I totally do not deny the integral role of technical work of a data scientist, I only want to emphasize the importance of understanding the business concept at first before any other activities.\nTherefore, I list out below a standard flow for starting a data science projects with key points, which I have been applying throughout 4 years working for 2 multinational companies as a data analyst and data scientist. This post was written based on my experience. Hence, take it as your reference and adjust to your own needs.\n 1. First and foremost importance - Clarifying Business question During my years in analytics and data science, in addition to technological concept explanation, business question clarification is one of the most difficult tasks when a Data scientist communicates with business partner.\nI am sure that you catch this phrase everywhere, in numerous articles, warning you to clarify business question in every situation.\n But how?\n Working and researching are not identical. In business, top people never stop expecting a Data Scientist to become a wise man who knows the answer for all of their questions. Therefore, digging the problem is our job and how to do that is our responsibility.\nIt must be very familiar with you when a sales manager asks you \u0026ldquo;I want to know why sales declined?\u0026quot; or a marketing director demands \u0026ldquo;How to increase the efficiency of the promotion activity for brand A on our website?\u0026quot;\nWhen you hear these questions, can you imagine the right approach or the answer to the case? Or you will be very vague and keep asking yourself \u0026ldquo;Is that they want me to do this.\u0026rdquo; or \u0026ldquo;I think they want to know that.\u0026quot;, and if you deliver the result based on this understanding, how much confidence you have on your result?\nIn reality, if you keep it this way, the one and only response you will get from them is:\n\u0026ldquo;This is not what I need\u0026rdquo;\nOMG! How terrible it is when you spent so much effort into this, but no one values it.\n This is because you did not truly understand problem so that you did not touch the right pain point!\n For example, the director wants to know way to improve his marketing activity efficacy, so what does Efficacy mean here? What kind of activity he points to? What are the real pain points? If these concerns are plainly clarified, the request will be interpreted as \u0026ldquo;How to optimize the budget spending on online promotion in order to increase the purchase rate and new customers vs last year\u0026rdquo;. This will end up with increasing efficacy.\nOne of the common advises is to ask \u0026ldquo;Why\u0026rdquo; in order to dig into the real problems. However, this solution is not applicable all times because the business partners might not know why for all of your questions.\nWhat can you do more:\n  Ask about the background of the question, why and how they come up with the request after you receive it.\n  Be sure that the request is your responsibility to answer. If your company has several Data teams such as Data Scientist, Data Analyst and BI, make sure to understand the Role \u0026amp; Responsibility of each team and know when to jump in and when to leap out. However, don't ever say \u0026ldquo;This is not my job. Ask BI\u0026rdquo;. Instead, show them that you know everything about company and data \u0026ldquo;With your request, the BI team has already got the data that can help with your question, I suggest you to meet BI and ask for sales and churn rate data of the last 3 years\u0026rdquo;.\n  Engage with other teams in the company to frequently get updates about other things happened within your company. Moreover, it is extremely important to always raise up questions such as \u0026ldquo;What are the latest company strategy, calendar, current key projects and recent performance?\u0026quot; or \u0026ldquo;Do I understand the vision and objectives of the projects that are critical to my company?\u0026quot;\n  Think of initiatives and what you can do more within your expertise to bring the projects to the next level.\n  Be a thinker, not a doer!\n2. Identify the approach for the problem This part is to place the methodology for the analysis\nThis step requires a broad knowledge on either statistical models or machine learning approached. In some companies, especially non-tech savvy ones, a data scientist is in charge of both analytics and data science work stream.\nWith a mixed role of Analytics and Data Science, the approaches for the problem will also diversified with various concepts and models. For example: Linear Regression cannot be used to segment customers or Descriptive analysis cannot predict customer churn.\nAt first, choosing methodology seems to be effortless but indeed always drives you crazy. If the Sales Director asked Data Science team to forecast the sales for next year based on the amount of budget spending while putting online appearance as company's focus, then which approach/model should be used? If the business wanted the forecast based on the market movement with the outlook of maintaining the current company's leadership position, which approach is correct?\nWhat you can do more:\n  It is fundamental to understand the discrepancy between Descriptive analysis and Predictive analysis (many people are still ambiguous between these 2 concepts). An example of descriptive analysis is the relationship between factors; while prescriptive analysis deals with figuring out the future outcomes of that relationship. Descriptive analysis delivers historical insights and prescriptive analysis foresees future values.\n  Identify the specific type of data to assist the problem approach: The target variable and other variables are continuous, categorical or binary.\n  Understand key problem approaches:\n   Binary (2 possible answers) or Multi-class (more than 2 possible answers) classification; Regression relationship (relationship between 2 or more factors) or Regression prediction (predict future value using regression model); Clustering (cluster unlabeled observation into groups of similar characteristics) or Segmentation (divide observations into specific groups); Trend detection (historical movement) or Time-series forecasting (project future value of that movement).  3. Acquire the appropriate data After identifying the business questions and the approach above, set up data requirement and extract the appropriate data from the data warehouse are the next thing.\nData selection sounds to be straightforward but indeed complicated. To solve the business questions, which kind of data is in need of. For example, is it necessary to have customer's birthday information for the task of predicting churn probability?\nIngest sufficient data will save you tons of effort afterward. Bear in mind the unspoken truth: Garbage in is Garbage out.\nTwo major problems that usually occur during Data Collection\n The unavailability of data The bias of training data  3.1 First, let's look at the unavailability of data This problem is very common globally in which data is unable to capture at the moment of collection due to the limitation of current digital connection. For instance, it is merely impossible to acquire the time spent in cooking at home.\nAs a common sense, when the data is nonexistent, you will instantly think of way to get the data. However, you have to consider the consequences of unavailability data including cost, time, resources and if the data is indeed not too important to your model, all the effort you put into it will be down the drain.\nTherefore, the solution for this case is to defer inaccessible data and in case the model requires this data for a better result, you will have more resources and confident to invest in obtaining it in the future.\nWhat you can do more if you need more data:\n  Bring along a data request summary when you visit the database owner if you need to talk to other parties for this. The summary form should include background of you project, data requirement, your request. This will help smoothing the discussion and the business partner will give the adequate solution.\n  Change the process/method of collecting data to acquire the right information needed. Work with database owner or IT team or propose to your up-line a system revision plan for approval.\n  Prepare budget and contact an outside data owner if the additional data is vital to improve the model and inaccessible for you.\n  3.2 Second, the bias of data This problem is serious especially when the training set gets bias from the beginning, the model will learnt accordingly to that bias and results into an inaccuracy prediction when comparing to the real world.\nOne of the most famous flaws of bias in data is the Amazon recruiting AI tool that showed bias against women. The tool reviewed candidate's resumes in order to pick the top talents within them. The tool showed an obvious bias against women because its training data is not gender-neutral from the beginning.\nTherefore, at first hand, be careful with data and its natural distribution are critical responsibility of every Data Scientist.\nWhat you can do more to eliminate the bias:\n  Ensure the statistical distribution of data and its representatives over population. For example, if the population is made up of 56% of male and 43% of female and 1% of others, the data distribution must be in similar ratio.\n  Verify the split of train, validate and test sets in prediction models to establish a similar allocation of variable and categories.\n  Choose the learning model fitting the problem and reduce the skewness. Some models can reduce the bias in data including clustering or dimension reduction.\n  Monitor the performance in real data. Frequently run statistical test on real data to pick out uncommon case. If the test result shows a statically significant in churn rate among male than female, dig it out. Is it the sudden shift or result of bias?\n   After getting all the data you need, the next step is thing that a Data scientist usually does:\nThe order can be flexible and this is the standard progress that I usually do in my project and my job. Sometimes, after tuning and the accuracy does not meet my expectation, I need to go back to the feature engineering step to find other way to deal with features.\nThese are the key bottlenecks beside the technical skills that I want to head up for Data Scientist who want to become more than just a Data insight extractor.\n","date":1579996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579996800,"objectID":"e3e7f7444f283d27743934177f63d36c","permalink":"/2020/01/26/practical-flow-of-a-data-science-project/","publishdate":"2020-01-26T00:00:00Z","relpermalink":"/2020/01/26/practical-flow-of-a-data-science-project/","section":"post","summary":"Overcome Data Scientist bottle neck in conducting a Data Science Project in real business context to become a valuable employee'","tags":["Process","Business","Data Science"],"title":"Practical flow of a Data Science Project","type":"post"},{"authors":["Nhu Anh Quynh Hoang"],"categories":["Data Science","Analytics"],"content":"One year ago, when I truly and seriously considered improving my skill in Data Science, two questions were always lingering in my mind:\nFrankly saying, what are the skills that a Data Scientist needs?\nWhat skill will support me in the future and How do I improve myself in the right direction at the most efficacy?\nMoreover, some friends, acquaintances of mine reached out to me for a thought of what they should learn to develop their career as a Data Scientist.\nActually, I can share some of my experiences with them, but as you've already known, this field evolves unpreceedingly fast, technology and new required skills change on the yearly basis at the slowest.\n What you learn today will be the old of tomorrow!\n Luckily, when I was known of the Kaggle dataset on Survey of Data Science and Machine learning, this data will definitely give me some insights for my questions.\nThe Data source is here\nIn this post, I will summary my key findings from the survey. The complete analysis can be found through this link\nThis analysis uses R language with tidyverse package for the best insight visualization.\n Before looking at the result, below are the data cleaning and preparation for the analysis.\n Feature selection:    This dataset includes the written answer of the respondents if they picked \u0026quot;Other\u0026quot; choice for some questions. However, the written answers are stored in another csv file so that all the variables containing \u0026quot;Other\u0026quot; will not a valuable variable for the analysis, therefore they will be excluded from the dataset.\n  For this analysis, the \u0026quot;Duration\u0026quot; variable is not the factor that I want to explore so it will be excluded as well.\n  Other data cleaning:    Shorten some typical column names for easier understanding.\n  Set level for values in the variables that require level later on, such as Company size, Compensation/Salary, year of experience with machine learning\u0026hellip;\n  Create functions for plotting (Example is as below)  plot_df_ds \u0026lt;- function (df_draw_ds, columnName1, columnName2) {\rnames(df_draw_ds)[columnName1] \u0026lt;- paste(\u0026quot;value\u0026quot;)\rnames(df_draw_ds)[columnName2] \u0026lt;- paste(\u0026quot;value2\u0026quot;)\rdf_draw_ds \u0026lt;- df_draw_ds %\u0026gt;% select (value, value2) %\u0026gt;%\rgroup_by(value, value2) %\u0026gt;%\rfilter(value != \u0026quot;Not employed\u0026quot;,value != \u0026quot;Other\u0026quot;) %\u0026gt;% summarise(count=n()) %\u0026gt;% mutate(perc= prop.table(count))\r}\r  Now, let's explore the result.\nRole and Responsibilities: A Data scientist is responsible for many tasks but the Top 3 includes:\n  Analyzing data to influence product or to support business decisions\n  Explore new areas of Machine learning through builidng prototypes\n  Experiment to improve company existing Machine learning models\n  Programming skills: Python, SQL and R are all the programming languages that are essential for Data Scientist as they stand in the Top 3 respectively.\nMachine Learning skills: It is not a surprise if all Data Scientists use Machine Learning in their daily job but I was amazed that almost all of them use both Natural Language Processing and Computer Vision. These 2 fields has no longer been specified areas to some groups of users but expanded to much wider application and require Data Scientist to own these skills.\nMachine Learning algorithm skills: Regression and Tree-based algorithms are the models used by Data Scientist. With the long history in statistics and analytics, these models are still being favor in analyzing data. Another advantage of these traditional models is that they are easy to explain to business partner. Other models such as Neuron network, or Deep learning is hard to explain the result.\nAnother interesting thing is that 51% of Data scientists is applying Boosting method.\nPython skills: Scikit-learn, Keras, Xgboost, TensorFlow and RandomForest libraries are the top used frameworks given their benefit and convenience.\nData Scientist Credential: Foundation skills\n  Writing code is an important skill of a good Data scientist. Being an excellent coder is not required but preferable to have, especially big companies have the tendency to hire Data Scientist Engineer with coding skill.\n  It is essential to build up your business skills in addition to technical skills. Many data scientists, especially the junior may excel at machine learning and algorithm but usually cry out for business point of view. Their recommendations are getting off the track from what the company is doing. That is the worst thing you want to face in the world of Data science when no one values your idea..\n  Specialized skills\n  Build expertise in Python, SQL and/or R on various online/offline platforms.\n  Fluency in using Local development environments and Intergrated development Environment, including but not limited to Jupyter, RStudio, Pycharm. Try to learn on job or learn on practice.\n  Strong in foundation and rich in practical experience. Develop side projects within your interested fields with proper models showcase. The model can be either traditional Linear regression, Decision Tree/Random Forest, or cutting-edge XGBoost, Recurrent Neural Network.\n  Enrich your knowledge and application in convenient frameworks such as Scikit-learn, Keras, Tensorflow, Xgboost. Working on your own project/cooperating project is a good choice to get practical experience if you have not obtained the chance in a company.\n  Computer Vision and NLP are the booming areas in Data science so it is beneficial to prepare yourself with these skills.\n  Although AutoML is a newly emerging field, it will probably become one of the important tools and skills for Data Scientist, including Automated hyperparameter tuning, Data augmentation, Feature engineering/selection and Auto ML pipelines.\n  Skills in working with Big data and Big data products e.g Google Bigquerry, Databricks, Redshift are the must to own.\n  This is the same with usage of cloud computing skills. In big company, it is a common to work on cloud so if you do not know how to conduct machine learning on cloud, it will become your minus comparing to other candidates.\n  And one last important thing: Always believe in yourself, your choice and never give up\nEnd notes As the big survey focusing on the Data science / Machine Learning areas, it appeared as a great source for me to gain valuable information.\nBeside understanding which typical skills requiring to advance in the Data Science field, I want to tbuild a model to predict the salary range and I will update on that in upcoming days.\n","date":1579219200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580169600,"objectID":"addccc1ce34adde634e74110a12ec6ef","permalink":"/2020/01/17/quick-deep-dive-at-data-scientist-skill-set/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/2020/01/17/quick-deep-dive-at-data-scientist-skill-set/","section":"post","summary":"Skill set that Data Scientist needs to master through studying Kaggle survey 2019 using R","tags":["Analytics","Skill","Business","Skill"],"title":"Quick deep dive at Data Scientist Skill set","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let's make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"}]