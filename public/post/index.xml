<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science Portfolio | Nhu Hoang</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Science Portfolio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Nhu Hoang</copyright><lastBuildDate>Mon, 30 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Data Science Portfolio</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Complete guide for Time series Exploration and Visualization</title>
      <link>/2020/03/30/complete-guide-for-time-series-exploration-and-visualization/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/2020/03/30/complete-guide-for-time-series-exploration-and-visualization/</guid>
      <description>&lt;h2 id=&#34;1-time-series-patterns&#34;&gt;1. Time series patterns&lt;/h2&gt;
&lt;p&gt;Time series can be describe as the combination of 3 terms: &lt;strong&gt;Trend&lt;/strong&gt;, &lt;strong&gt;Seasonality&lt;/strong&gt; and &lt;strong&gt;Cyclic&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trend&lt;/strong&gt; is the changeing direction of the series. &lt;strong&gt;Seasonality&lt;/strong&gt; occurs when there is a seasonal factor is seen in the series. &lt;strong&gt;Cyclic&lt;/strong&gt; is similar with Seasonality in term of the repeating cycle of a similar pattern but differs in term of the length nd frequency of the pattern.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;total_sales.png&#34; alt=&#34;&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Looking at the example figure, there is no &lt;strong&gt;trend&lt;/strong&gt; but there is a clear annual seasonlity occured in December. No cyclic as there is no pattern with frequency longer than 1 year.&lt;/p&gt;
&lt;h2 id=&#34;2-confirming-seasonality&#34;&gt;2. Confirming seasonality&lt;/h2&gt;
&lt;p&gt;There are several ways to confirm the seasonlity. Below, I list down vizualization approaches (which is prefered by non-technical people).&lt;/p&gt;
&lt;h3 id=&#34;seasonal-plot&#34;&gt;Seasonal plot:&lt;/h3&gt;
&lt;p&gt;There is a large jump in December, followed by a drop in January.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;seasonal_plot.png&#34; 
style=&#34;float: left; margin-right: 15px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Code can be found below (I am using the new Cyberpunk of Matplotlib, can be found &lt;a href=&#34;https://github.com/dhaitz/mplcyberpunk&#34;&gt;here&lt;/a&gt; with heptic neon color)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;colors = [&#39;#08F7FE&#39;,  # teal/cyan
          &#39;#FE53BB&#39;,  # pink
          &#39;#F5D300&#39;] # matrix green
plt.figure(figsize=(10,6))
w =data.groupby([&#39;Year&#39;,&#39;Month&#39;])[&#39;Weekly_Sales&#39;].sum().reset_index()
sns.lineplot(&amp;quot;Month&amp;quot;, &amp;quot;Weekly_Sales&amp;quot;, data=w, hue=&#39;Year&#39;, palette=colors,marker=&#39;o&#39;, legend=False)
mplcyberpunk.make_lines_glow()
plt.title(&#39;Seasonal plot: Total sales of Walmart 45 stores in 3 years&#39;,fontsize=20 )
plt.legend(title=&#39;Year&#39;, loc=&#39;upper left&#39;, labels=[&#39;2010&#39;, &#39;2011&#39;,&#39;2012&#39;],fontsize=&#39;x-large&#39;, title_fontsize=&#39;20&#39;)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;seasonal-subseries-plot&#34;&gt;Seasonal subseries plot&lt;/h3&gt;
&lt;p&gt;Boxplot is a great tool to observe the time series pattern.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sub_seasonal.png&#34; 
style=&#34;float: left; margin-right: 15px;&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;moving-average-and-original-series-plot&#34;&gt;Moving average and Original series plot&lt;/h3&gt;
&lt;figure&gt;
  &lt;img src=&#34;moving_average.png&#34; alt=&#34;&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):

    rolling_mean = series.rolling(window=window).mean()
    
    plt.figure(figsize=(15,5))
    plt.title(&amp;quot;Moving average\n window size = {}&amp;quot;.format(window))
    plt.plot(rolling_mean, &amp;quot;g&amp;quot;, label=&amp;quot;Rolling mean trend&amp;quot;)

    # Plot confidence intervals for smoothed values
    if plot_intervals:
        mae = mean_absolute_error(series[window:], rolling_mean[window:])
        deviation = np.std(series[window:] - rolling_mean[window:])
        lower_bond = rolling_mean - (mae + scale * deviation)
        upper_bond = rolling_mean + (mae + scale * deviation)
        plt.plot(upper_bond, &amp;quot;r--&amp;quot;, label=&amp;quot;Upper Bond / Lower Bond&amp;quot;)
        plt.plot(lower_bond, &amp;quot;r--&amp;quot;)
        
        # Having the intervals, find abnormal values
        if plot_anomalies:
            anomalies = pd.DataFrame(index=series.index, columns=series.columns)
            anomalies[series&amp;lt;lower_bond] = series[series&amp;lt;lower_bond]
            anomalies[series&amp;gt;upper_bond] = series[series&amp;gt;upper_bond]
            plt.plot(anomalies, &amp;quot;ro&amp;quot;, markersize=10)
        
    plt.plot(series[window:], label=&amp;quot;Actual values&amp;quot;)
    plt.legend(loc=&amp;quot;upper left&amp;quot;)
    plt.grid(True)
    
plotMovingAverage(series, window, plot_intervals=True, scale=1.96,
                  plot_anomalies=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;acf--pacf-plots&#34;&gt;ACF / PACF plots&lt;/h3&gt;
&lt;p&gt;The details of ACF and PACF plot implication can be found &lt;a href=&#34;https://geniusnhu.netlify.com/publication/arima-autoregressive-intergreated-moving-average/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;ACF_PACF.png&#34; alt=&#34;ACF / PACF plots&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;ACF / PACF plots&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ACF and PACF for time series data
series=train.dropna()
fig, ax = plt.subplots(2,1, figsize=(10,8))
fig = sm.graphics.tsa.plot_acf(series, lags=None, ax=ax[0])
fig = sm.graphics.tsa.plot_pacf(series, lags=None, ax=ax[1])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;actual-vs-predicted-values-plot&#34;&gt;Actual vs Predicted values plot&lt;/h3&gt;
&lt;figure&gt;
  &lt;img src=&#34;actual_predicted.png&#34; alt=&#34;Actual vs Predicted values plot&#34; style=&#34;width:100%&#34;&gt;
  &lt;figcaption&gt;Actual vs Predicted values plot&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plotModelResults(model, X_train, X_test, y_train, y_test, plot_intervals=False, plot_anomalies=False):

    prediction = model.predict(X_test)
    
    plt.figure(figsize=(12, 8))
    plt.plot(prediction, &amp;quot;g&amp;quot;, label=&amp;quot;prediction&amp;quot;, linewidth=2.0, color=&amp;quot;blue&amp;quot;)
    plt.plot(y_test.values, label=&amp;quot;actual&amp;quot;, linewidth=2.0, color=&amp;quot;olive&amp;quot;)
    
    if plot_intervals:
        cv = cross_val_score(model, X_train, y_train, 
                                    cv=tscv, 
                                    scoring=&amp;quot;neg_mean_absolute_error&amp;quot;)
        mae = cv.mean() * (-1)
        deviation = cv.std()
        
        scale = 1
        lower = prediction - (mae + scale * deviation)
        upper = prediction + (mae + scale * deviation)
        
        plt.plot(lower, &amp;quot;r--&amp;quot;, label=&amp;quot;upper bond / lower bond&amp;quot;, alpha=0.5)
        plt.plot(upper, &amp;quot;r--&amp;quot;, alpha=0.5)
        
        if plot_anomalies:
            anomalies = np.array([np.NaN]*len(y_test))
            anomalies[y_test&amp;lt;lower] = y_test[y_test&amp;lt;lower]
            anomalies[y_test&amp;gt;upper] = y_test[y_test&amp;gt;upper]
            plt.plot(anomalies, &amp;quot;o&amp;quot;, markersize=10, label = &amp;quot;Anomalies&amp;quot;)
    
    error = mean_absolute_percentage_error(y_test,prediction)
    plt.title(&amp;quot;Mean absolute percentage error {0:.2f}%&amp;quot;.format(error))
    plt.legend(loc=&amp;quot;best&amp;quot;)
    plt.tight_layout()
    plt.grid(True);

plotModelResults(linear, X_train, X_test, y_train, y_test,
                 plot_intervals=True, plot_anomalies=True)    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;To be updated&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comprehensive understanding on Time Series forecasting</title>
      <link>/2020/02/06/comprehensive-understanding-on-time-series-forecasting/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/2020/02/06/comprehensive-understanding-on-time-series-forecasting/</guid>
      <description>&lt;h2 id=&#34;1-what-is-time-series&#34;&gt;1. What is Time Series&lt;/h2&gt;
&lt;p&gt;Time series is a sequence of value corresponding with time. Retail sales data, Daily temperature, production, demand, natural reserves are time series data because the later values depend on their historical values.&lt;/p&gt;
&lt;h2 id=&#34;2-what-makes-up-time-series&#34;&gt;2. What makes up Time Series&lt;/h2&gt;
&lt;p&gt;There are 4 components in Time Series: Level, Trend, Seasonality and Noise.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Level&lt;/strong&gt;: the average value of the time series&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trend&lt;/strong&gt;: The movement of the series values from 1 period to another period&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seasonality&lt;/strong&gt;: The short-term cyclical behavior of the series that can be observed several times&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Noise&lt;/strong&gt;: the random variation that results from the measurement of error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is not always that we will be able to distinguish the first 3 elements from Noise because they are usually invisble which need some techniques to be noticeable&lt;/p&gt;
&lt;p&gt;To observe and identify the existence of these components, we can consider.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plot the Time series (this is the best way to detect the characteristics of the series)&lt;/li&gt;
&lt;li&gt;Zoom in a specify shorter period of time&lt;/li&gt;
&lt;li&gt;Change scale of the series to observe the trend more clearly&lt;/li&gt;
&lt;li&gt;Suppress seasonality: aggregate the time series to a bigger time scale (from hourly scale to daily scale, from monthly scale to yearly scale, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I used 3-year weekly sales of a Retail store as an illustration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(&amp;quot;Date&amp;quot;, &amp;quot;Weekly_Sales&amp;quot;, data=Wal_sales)
plt.hlines(y=Wal_sales.Weekly_Sales.mean(), xmin=0, xmax=len(Wal_sales), linestyles=&#39;dashed&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;TS plot.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this data, there are obviously 2 peaks which denotes quite a clear &lt;strong&gt;seasonality at the end of year&lt;/strong&gt; (probably Christmas and New year period). There might be other sesonality but it is hard to observe it from the plot. &lt;strong&gt;Auto correlation&lt;/strong&gt; can be used to confirm the seasonality.&lt;/p&gt;
&lt;h2 id=&#34;3-autocorrelation&#34;&gt;3. Autocorrelation&lt;/h2&gt;
&lt;p&gt;Autocorrelation describes the &lt;strong&gt;connection between the value of time series  and its neighbors&lt;/strong&gt;. Thus, to compute Autocorrelation, we calculate the correlation of the series with its &lt;strong&gt;lagged versions&lt;/strong&gt;. Lag-n version is produced from the original dataset by moving the series values forward n period. For example, lag-1 is moved forward 1 period, Lag-10 series is moved forward 10 periods.&lt;/p&gt;
&lt;p&gt;By observing the correlation of the series and its lags, we can confirm the seasonality of the series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_cor = sales.groupby(&amp;quot;Date&amp;quot;)[&amp;quot;Weekly_Sales&amp;quot;].sum()
auto_cor = pd.DataFrame(auto_cor)
auto_cor.columns = [&amp;quot;y&amp;quot;]

# Adding the lag of the target variable from 1 steps back up to 52 (due to a seasonality at the end of the year)
for i in range(1, 53):
    auto_cor[&amp;quot;lag_{}&amp;quot;.format(i)] = auto_cor.y.shift(i)

# Compute autocorrelation of the series and its lags
lag_corr = auto_cor.corr()
lag_corr = lag_corr.iloc[1:,0]
lag_corr.columns = [&amp;quot;corr&amp;quot;]
order = lag_corr.abs().sort_values(ascending = False)
lag_corr = lag_corr[order.index]

# Plot the Autocorrelation
plt.figure(figsize=(12, 6))
lag_corr.plot(kind=&#39;bar&#39;)
plt.grid(True, axis=&#39;y&#39;)
plt.title(&amp;quot;Autocorrelation&amp;quot;)
plt.hlines(y=0, xmin=0, xmax=len(lag_corr), linestyles=&#39;dashed&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Autocorrelation.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Judging from the Autocorrelation plot above, there is a strong positive autocorrelation in lag-52 as well as lag-51 as we expected when observing the time series plot. This implies a &lt;strong&gt;cyclical annual pattern at the end of the year&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The second strong correlation is lag-1, which connotes as the second week of february, or Valentine period.&lt;/li&gt;
&lt;li&gt;The autocorrelation reveals both &lt;strong&gt;Positive&lt;/strong&gt; and &lt;strong&gt;Negative&lt;/strong&gt; autocorrelation, which implies that the series does not move in the same direction but ups and downs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Autocorrelation can plotted easily through using &lt;em&gt;autocorrelation_plot&lt;/em&gt; function from &lt;em&gt;pandas.plotting&lt;/em&gt; in Python or &lt;em&gt;acf&lt;/em&gt; function from &lt;em&gt;tseries&lt;/em&gt; package in R.&lt;/p&gt;
&lt;h2 id=&#34;4-forecasting-time-series&#34;&gt;4. Forecasting Time series&lt;/h2&gt;
&lt;p&gt;There are several methods to forecast Time series.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Model-based method through multiple linear regression&lt;/strong&gt; to explore the correlation of the series with other features. Alike other cross-sessional data, model-based method compute the dependence of the time series to other features, but does not take into account the dependence between time series values within different periods.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data-driven method in which learns the pattern from the data itself&lt;/strong&gt; and estimate the next value of the time series in correspondence with its previous values. The data-driven method is important in time series given in the time series context, the values in adjoining period tend to be correlated with each other. Such correlation is denoted as &lt;strong&gt;Autocorrelation&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combining method by forecasting the future values of the series as well as the future value of residual that generated from the first forecasting model&lt;/strong&gt;, and then combine the result of 2 forecast together. The residual forecast acts as the correct for the first forecast.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ensembles method&lt;/strong&gt; by averaging multiple methods to get the result&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;forecasting-using-data-driven-method&#34;&gt;Forecasting using Data-driven method:&lt;/h3&gt;
&lt;p&gt;ARIMA model is the most frequent choice to compute data-driven forecasting. You can find detail for ARIMA model in this &lt;a href=&#34;https://geniusnhu.netlify.com/publication/arima-autoregressive-intergreated-moving-average/&#34;&gt;post&lt;/a&gt;. 
Here I will apply the ARIMA to the data.&lt;/p&gt;
&lt;p&gt;It is useful to use *:auto_arima**function from &lt;strong&gt;pmdarima&lt;/strong&gt; in Python or &lt;strong&gt;auto.arima&lt;/strong&gt; function from &lt;strong&gt;forecast&lt;/strong&gt; packgage in R.&lt;/p&gt;
&lt;p&gt;There is one thing to note is that from the Autocorrelation above, there is a clear seasonality at lag 52 so we will need to include this into the ARIMA model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stepwise_model = pm.auto_arima(Wal_sales.iloc[:,1].values, start_p=1, start_q=1,
                               max_p=20, max_q=20, m=52,
                               start_P=0, seasonal=True,
                               d=1, D=1, trace=True,
                               error_action=&#39;ignore&#39;,  
                               suppress_warnings=True, 
                               stepwise=True)
print(stepwise_model.aic())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Performing stepwise search to minimize aic
Fit ARIMA: (1, 1, 1)x(0, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 0)x(0, 1, 0, 52) (constant=True); AIC=2898.191, BIC=2903.190, Time=0.423 seconds
Fit ARIMA: (1, 1, 0)x(1, 1, 0, 52) (constant=True); AIC=2883.839, BIC=2893.839, Time=5.555 seconds
Fit ARIMA: (0, 1, 1)x(0, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 0)x(0, 1, 0, 52) (constant=False); AIC=2907.540, BIC=2910.039, Time=0.371 seconds
Fit ARIMA: (1, 1, 0)x(0, 1, 0, 52) (constant=True); AIC=2893.265, BIC=2900.764, Time=0.807 seconds
Fit ARIMA: (1, 1, 0)x(2, 1, 0, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (1, 1, 0)x(1, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (1, 1, 0)x(0, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (1, 1, 0)x(2, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 0)x(1, 1, 0, 52) (constant=True); AIC=2890.759, BIC=2898.258, Time=7.666 seconds
Fit ARIMA: (2, 1, 0)x(1, 1, 0, 52) (constant=True); AIC=2884.464, BIC=2896.963, Time=7.595 seconds
Fit ARIMA: (1, 1, 1)x(1, 1, 0, 52) (constant=True); AIC=2884.895, BIC=2897.394, Time=20.608 seconds
Fit ARIMA: (0, 1, 1)x(1, 1, 0, 52) (constant=True); AIC=2883.040, BIC=2893.039, Time=6.410 seconds
Fit ARIMA: (0, 1, 1)x(0, 1, 0, 52) (constant=True); AIC=2893.770, BIC=2901.269, Time=5.440 seconds
Fit ARIMA: (0, 1, 1)x(2, 1, 0, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 1)x(1, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 1)x(2, 1, 1, 52) (constant=True); AIC=nan, BIC=nan, Time=nan seconds
Fit ARIMA: (0, 1, 2)x(1, 1, 0, 52) (constant=True); AIC=2887.816, BIC=2900.315, Time=7.108 seconds
Fit ARIMA: (1, 1, 2)x(1, 1, 0, 52) (constant=True); AIC=2889.929, BIC=2904.928, Time=17.358 seconds
Total fit time: 79.418 seconds
2883.039997060003
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This fuction chose the lowest AIC-score model and embed it for further model usage.&lt;/p&gt;
&lt;p&gt;Split train-test set, train the model and make prediction.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Split train and test
train = Wal_sales.iloc[:106,1].values
test = Wal_sales.iloc[106:,1].values
# Train the model
stepwise_model.fit(train)

# Predict test set
pred = stepwise_model.predict(n_periods=37)

# Reframe the data
test_pred = Wal_sales.iloc[106:,:2]
test_pred[&amp;quot;Predict_sales&amp;quot;] = np.array(pred,dtype=&amp;quot;float&amp;quot;)

# Visualize the prediction
plt.figure(figsize=(12,8))
plt.plot( &#39;Date&#39;, &#39;Weekly_Sales&#39;, data=Wal_sales, markersize=12, color=&#39;olive&#39;, linewidth=3)
plt.plot( &#39;Date&#39;, &#39;Predict_sales&#39;, data=test_pred, marker=&#39;&#39;, color=&#39;blue&#39;, linewidth=3)
plt.title(&amp;quot;Predicted sales vs Actual sales&amp;quot;)
plt.legend()

print(&amp;quot;MAPE score: &amp;quot;, mean_absolute_percentage_error(test, pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;ARIMA_forecast.jpg&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In overal, the model seem to work moderately on the data but there is still room to improve further.&lt;/li&gt;
&lt;li&gt;The MAPE (mean absolute percentage error) score is 5.7%, which is not too high, not too low.&lt;/li&gt;
&lt;li&gt;The ARIMA model seems to perform well in early predicted value and gets worse in later predicted values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;one-question-emerged-does-this-model-truly-capture-the-values-of-the-time-series-data&#34;&gt;One question emerged, does this model truly capture the values of the time series data?&lt;/h4&gt;
&lt;p&gt;It is helpful to take a look at the &lt;strong&gt;Residual&lt;/strong&gt; of the model (or the &lt;strong&gt;diference between predicted values and actual values&lt;/strong&gt;). Examining the residuals of the forecasting model is suggested to evaluate whether the specified model has adequately captured the information of the data. This can be done through exploring the correlation of one period&#39;s residual with other periods&amp;rsquo; ones.&lt;/p&gt;
&lt;p&gt;The Residuals of a &lt;strong&gt;good time series forecasting model&lt;/strong&gt; have the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Residuals are &lt;strong&gt;uncorrelated&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Residuals have &lt;strong&gt;zero or nearly-zero mean&lt;/strong&gt; (which means the model is unbiased in any directions)&lt;/li&gt;
&lt;li&gt;Residuals should have &lt;strong&gt;normal distribution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Residuals should have &lt;strong&gt;constant variance&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the result is lack of any of the above attributes, the forecasting model can be further improved.&lt;/p&gt;
&lt;p&gt;Let&#39;s compute the Residuals Autocorrelation and judge the result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute Residual
train_pred = stepwise_model.predict(n_periods=106)
r_train = train - train_pred
r_test = test - pred
residual = pd.DataFrame(np.concatenate((r_train,r_test)), columns={&amp;quot;y&amp;quot;})


# Generate lag of Residuals from 1 step to 52 steps
# Adding the lag of the target variable from 1 steps back up to 52 
for i in range(1, 53):
    residual[&amp;quot;lag_{}&amp;quot;.format(i)] = residual.y.shift(i)

# Compute correlation of the Residual series and its lags
lag_corr = residual.corr()
lag_corr = lag_corr.iloc[1:,0]
lag_corr.columns = [&amp;quot;corr&amp;quot;]
order = lag_corr.abs().sort_values(ascending = False)
lag_corr = lag_corr[order.index]

# Plot the Residual Autocorrelation
plt.figure(figsize=(12, 6))
lag_corr.plot(kind=&#39;bar&#39;)
plt.grid(True, axis=&#39;y&#39;)
plt.title(&amp;quot;Autocorrelation&amp;quot;)
plt.hlines(y=0, xmin=0, xmax=len(lag_corr), linestyles=&#39;dashed&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;residual_autocorrelation.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Other criteria:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Residual mean and Distribution
print(&amp;quot;Residual mean: &amp;quot;,residual.iloc[:,0].mean())
plt.hist(residual.iloc[:,0], bins=20)
plt.title(&amp;quot;Residual Distribution&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Residual mean:  -6308833.905274585
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;residual_distribution.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Residual variance
plt.plot(residual.iloc[:,0])
plt.title(&amp;quot;Residual&amp;quot;)
plt.hlines(y=0, xmin=0, xmax=len(residual), linestyles=&#39;dashed&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;residual_variance.png&#34;
style=&#34;float: left; margin-right: 10px;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&#39;s judge the Autocorrelation of Residual based on the following criteria:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Residuals are uncorrelated: the Residual series is still observed some correlations with its lags.&lt;/li&gt;
&lt;li&gt;Residuals have zero or nearly-zero mean (which means the model is unbiased in any directions): the mean is -6308833.905274585. So this criteria is not met.&lt;/li&gt;
&lt;li&gt;Residuals should have normal distribution: Not quite a normal distribution&lt;/li&gt;
&lt;li&gt;Residuals should have constant variance: No as consistent with mean does not equal to 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, the forecasting model has a lot of rooms to improve further by finding the way to capture the correlation in the Residuals, adding the values that is currently staying in residuals to the prediction.&lt;/p&gt;
&lt;h2 id=&#34;5-data-partitioning&#34;&gt;5. Data partitioning&lt;/h2&gt;
&lt;p&gt;One of the biggest characteristics of Time series distinguishing it with normal cross-sessional data is the &lt;strong&gt;dependence of the future values with their historical values&lt;/strong&gt;. Therefore, the Data partitioning for Time series cannot be done randomly but instead, trim the series into 2 periods, the earlier to train set and the later to validation set.&lt;/p&gt;
&lt;p&gt;The below code will help split the tran-test sets with respect to time series structure.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Split train and test sets in correspondence with Time series data
def ts_train_test_split(X, y, test_size):
    test_index = int(len(X)*(1-test_size))
    
    X_train = X.iloc[:test_index]
    y_train = y.iloc[:test_index]
    X_test = X.iloc[test_index:]
    y_test = y.iloc[test_index:]
    
    return X_train, X_test, y_train, y_test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sales does not only correlated with its own past but also might be affected by other factors such as special occasions (i.e Holiday in this dataset), weekday and weekend, etc&amp;hellip; The method-driven models will be presented in the next article with feature extraction, feature selection.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quick deep dive at Data Scientist Skill set</title>
      <link>/2020/01/17/quick-deep-dive-at-data-scientist-skill-set/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/2020/01/17/quick-deep-dive-at-data-scientist-skill-set/</guid>
      <description>&lt;p&gt;One year ago, when I truly and seriously considered improving my skill in Data Science, two questions were always lingering in my mind:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frankly saying, what are the skills that a Data Scientist needs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What skill will support me in the future and How do I improve myself in the right direction at the most efficacy?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moreover, some friends, acquaintances of mine reached out to me for a thought of what they should learn to develop their career as a Data Scientist.&lt;/p&gt;
&lt;p&gt;Actually, I can share some of my experiences with them, but as you&#39;ve already known, this field evolves unpreceedingly fast, technology and new required skills change on the yearly basis at the slowest.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;What you learn today will be the old of tomorrow!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Luckily, when I was known of the Kaggle dataset on Survey of Data Science and Machine learning, this data will definitely give me some insights for my questions.&lt;/p&gt;
&lt;p&gt;The Data source is &lt;a href=&#34;https://www.kaggle.com/c/kaggle-survey-2019&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this post, I will summary my key findings from the survey. The complete analysis can be found through this &lt;a href=&#34;https://www.kaggle.com/geninhu/direction-toward-a-great-data-scientist/data&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This analysis uses R language with tidyverse package for the best insight visualization.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Before looking at the result, below are the data cleaning and preparation for the analysis.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Feature selection:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This dataset includes the written answer of the respondents if they picked &lt;code&gt;&amp;quot;Other&amp;quot;&lt;/code&gt; choice for some questions. However, the written answers are stored in another &lt;em&gt;csv&lt;/em&gt; file so that all the variables containing &lt;code&gt;&amp;quot;Other&amp;quot;&lt;/code&gt; will not a valuable variable for the analysis, therefore they will be excluded from the dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For this analysis, the &lt;code&gt;&amp;quot;Duration&amp;quot;&lt;/code&gt; variable is not the factor that I want to explore so it will be excluded as well.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Other data cleaning:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Shorten some typical column names for easier understanding.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set level for values in the variables that require level later on, such as &lt;em&gt;Company size, Compensation/Salary, year of experience with machine learning&lt;/em&gt;&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Create functions for plotting (Example is as below)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_df_ds &amp;lt;- function (df_draw_ds, columnName1, columnName2) {
    names(df_draw_ds)[columnName1] &amp;lt;- paste(&amp;quot;value&amp;quot;)
    names(df_draw_ds)[columnName2] &amp;lt;- paste(&amp;quot;value2&amp;quot;)
    df_draw_ds &amp;lt;- df_draw_ds %&amp;gt;% 
    select (value, value2) %&amp;gt;%
    group_by(value, value2) %&amp;gt;%
    filter(value != &amp;quot;Not employed&amp;quot;,value != &amp;quot;Other&amp;quot;) %&amp;gt;% 
    summarise(count=n()) %&amp;gt;% 
    mutate(perc= prop.table(count))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Now, let&#39;s explore the result.&lt;/p&gt;
&lt;h3 id=&#34;role-and-responsibilities&#34;&gt;&lt;strong&gt;Role and Responsibilities:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A Data scientist is responsible for many tasks but the Top 3 includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Analyzing data&lt;/strong&gt; to influence product or to support business decisions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explore new areas of Machine learning&lt;/strong&gt; through builidng prototypes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Experiment to &lt;strong&gt;improve company existing Machine learning models&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/responsibility.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;programming-skills&#34;&gt;&lt;strong&gt;Programming skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Python, SQL and R&lt;/strong&gt; are all the programming languages that are essential for Data Scientist as they stand in the Top 3 respectively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/PL.png&#34; width=&#34;300&#34; height=&#34;450&#34; &gt;&lt;/p&gt;
&lt;h3 id=&#34;machine-learning-skills&#34;&gt;&lt;strong&gt;Machine Learning skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;It is not a surprise if all Data Scientists use Machine Learning in their daily job but I was amazed that almost all of them use both Natural Language Processing and Computer Vision. These 2 fields has no longer been specified areas to some groups of users but expanded to much wider application and require Data Scientist to own these skills.&lt;/p&gt;
&lt;h3 id=&#34;machine-learning-algorithm-skills&#34;&gt;&lt;strong&gt;Machine Learning algorithm skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt; and &lt;strong&gt;Tree-based algorithms&lt;/strong&gt; are the models used by Data Scientist. With the long history in statistics and analytics, these models are still being favor in analyzing data. Another advantage of these traditional models is that they are easy to explain to business partner. Other models such as Neuron network, or Deep learning is hard to explain the result.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/ML.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another interesting thing is that 51% of Data scientists is applying &lt;strong&gt;Boosting method&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;python-skills&#34;&gt;&lt;strong&gt;Python skills:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Scikit-learn, Keras, Xgboost, TensorFlow and RandomForest libraries&lt;/strong&gt; are the top used frameworks given their benefit and convenience.&lt;/p&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/geniusnhu/ds-masterpiece-hugo/master/content/post/2020-01-17-quick-deep-dive-at-data-scientist-skill-set/package.png&#34; width=&#34;300&#34; height=&#34;450&#34;&gt;
&lt;h3 id=&#34;data-scientist-credential&#34;&gt;&lt;strong&gt;Data Scientist Credential:&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Foundation skills&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Writing code&lt;/strong&gt; is an important skill of a good Data scientist. Being an excellent coder is not required but preferable to have, especially big companies have the tendency to hire Data Scientist Engineer with coding skill.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is essential to &lt;strong&gt;build up your business skills&lt;/strong&gt; in addition to technical skills. Many data scientists, especially the junior may excel at machine learning and algorithm but usually cry out for business point of view. Their recommendations are getting off the track from what the company is doing. That is the worst thing you want to face in the world of Data science when no one values your idea..&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Specialized skills&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Build expertise in &lt;strong&gt;Python, SQL and/or R&lt;/strong&gt; on various online/offline platforms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fluency in using Local development environments and Intergrated development Environment, including but not limited to Jupyter, RStudio, Pycharm. Try to &lt;strong&gt;learn on job&lt;/strong&gt; or &lt;strong&gt;learn on practice&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Strong in foundation and rich in practical experience&lt;/strong&gt;. Develop side projects within your interested fields with proper models showcase. The model can be either traditional Linear regression, Decision Tree/Random Forest, or cutting-edge XGBoost, Recurrent Neural Network.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enrich your knowledge and application in convenient frameworks&lt;/strong&gt; such as Scikit-learn, Keras, Tensorflow, Xgboost. Working on your own project/cooperating project is a good choice to get practical experience if you have not obtained the chance in a company.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Computer Visio&lt;/strong&gt;n and &lt;strong&gt;NLP&lt;/strong&gt; are the booming areas in Data science so it is beneficial to prepare yourself with these skills.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although &lt;strong&gt;AutoML&lt;/strong&gt; is a newly emerging field, it will probably become one of the important tools and skills for Data Scientist, including Automated hyperparameter tuning, Data augmentation, Feature engineering/selection and Auto ML pipelines.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Skills in working with &lt;strong&gt;Big data&lt;/strong&gt; and Big data products e.g Google Bigquerry, Databricks, Redshift are the must to own.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is the same with usage of &lt;strong&gt;cloud computing skills&lt;/strong&gt;. In big company, it is a common to work on cloud so if you do not know how to conduct machine learning on cloud, it will become your minus comparing to other candidates.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And one last important thing: &lt;strong&gt;Always believe in yourself, your choice and never give up&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;end-notes&#34;&gt;End notes&lt;/h4&gt;
&lt;p&gt;As the big survey focusing on the Data science / Machine Learning areas, it appeared as a great source for me to gain valuable information.&lt;/p&gt;
&lt;p&gt;Beside understanding which typical skills requiring to advance in the Data Science field, I want to tbuild a model to predict the salary range and I will update on that in upcoming days.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
