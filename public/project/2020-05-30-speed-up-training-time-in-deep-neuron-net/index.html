<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Nhu Hoang">

  
  
  
    
  
  <meta name="description" content="Discussion of 5 popular techniques to speed up training in deep neural net (Initialization, Activation function and Batch Normalization/Gradient Clipping) using TensoFlow">

  
  <link rel="alternate" hreflang="en-us" href="/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/">

  


  
  
  
  <meta name="theme-color" content="#01395e">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/agate.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/agate.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:700%7CWork+Sans&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158640946-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-158640946-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hubeb4088d9e1ef3a12a3be812bce5943c_42313_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hubeb4088d9e1ef3a12a3be812bce5943c_42313_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@nhu_hoang">
  <meta property="twitter:creator" content="@nhu_hoang">
  
  <meta property="og:site_name" content="Nhu Hoang">
  <meta property="og:url" content="/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/">
  <meta property="og:title" content="Speed up training and improve performance in deep neural net | Nhu Hoang">
  <meta property="og:description" content="Discussion of 5 popular techniques to speed up training in deep neural net (Initialization, Activation function and Batch Normalization/Gradient Clipping) using TensoFlow"><meta property="og:image" content="/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/featured.jpg">
  <meta property="twitter:image" content="/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-05-30T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-05-30T00:00:00&#43;00:00">
  

  


    











<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/"
  },
  "headline": "Speed up training and improve performance in deep neural net",
  
  "image": [
    "/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/featured.jpg"
  ],
  
  "datePublished": "2020-05-30T00:00:00Z",
  "dateModified": "2020-05-30T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Nhu Hoang"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Nhu Hoang",
    "logo": {
      "@type": "ImageObject",
      "url": "img//"
    }
  },
  "description": "Discussion of 5 popular techniques to speed up training in deep neural net (Initialization, Activation function and Batch Normalization/Gradient Clipping) using TensoFlow"
}
</script>

  

  


  


  





  <title>Speed up training and improve performance in deep neural net | Nhu Hoang</title>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  






  


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/"><img src="/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_0x70_resize_lanczos_2.png" alt="Nhu Hoang"></a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/"><img src="/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_0x70_resize_lanczos_2.png" alt="Nhu Hoang"></a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#portfolios"><span>Portfolio</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts/News</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#skills"><span>Experience/Skill</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      

      

    </ul>

  </div>
</nav>


  <article class="article article-project">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Speed up training and improve performance in deep neural net</h1>

  
  <p class="page-subtitle">Speed up training and improve performance in deep neural net. Part 1: Initialization, Activation function and Batch Normalization/Gradient Clipping</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Nhu Hoang</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 30, 2020
  </span>
  

  

  

  
  
  
  <span class="middot-divider"></span>
  <a href="/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/data-science/">Data Science</a>, <a href="/categories/deep-learning/">Deep learning</a></span>
  

</div>

  














</div>


<div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width: 1200px; max-height: 629px;">
  <div style="position: relative">
    <img src="/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/featured_huc7c3cf35002a2878e0d6cf921f0c8be5_139478_1200x0_resize_q90_lanczos.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Training a large and deep neural network is a time and computation consuming task and was the main reason for the unpopularity of DNN 20 years ago. As several techniques have been found out to push up the training speed, Deep learning has come back to the light. So which technique to use, how and when to use which? Let's discuss it here!</p>
<p><em>Performance summary is shown at the end of the post for Classification &amp; Regression examples</em></p>
<hr>
<h2 id="1-applying-initialization">1. Applying Initialization</h2>
<p>Initialization is one of the first technique used to fasten the training time of Neuron Network (as well as improve performance). Let's briefly explain its importance. In Artificial Neural Network (ANN), there are numerous connections between different neurons. One neuron in the current layer connects to several neurons in the next layer and is attached to various ones in the previous layer. If 2 neurons interact frequently than another pair, their connection (i.e the weights) will be stronger than the other one.</p>
<p>However, one problem with the ANN is that if the weights aren't specified from the beginning of training, the connection weights can be either too small or too large which makes them too tiny or too massive to use further in the network. In other words, the network will fall into <strong>Vanishing Gradients</strong> or <strong>Exploding Gradients</strong> problems.</p>
<p>So if the weights are set at suitable random values from the beginning of the training, these problem can be avoided. This technique was proposed by <a href="http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot and Bengio</a>, which then significantly lifted these unstable problems. This initialization strategy is called <em>Xavier initialization</em> or <em>Glorot initialization</em>.</p>
<p>In this strategy, the connection weights between neurons are initialized randomly using the Normal distribution with $mean=0$ and variance $\sigma^2 = \frac{2}{fan_{in}+fan_{out}}$ , in which $fan_{in}$ is the number of input neurons and $fan_{out}$ is the number of output neurons.</p>
<p>There are 2 other popular initialization techniques beside <strong>Glorot</strong> (used in Keras as default): <strong>He</strong> and <strong>LeCun</strong>.</p>
<p>Let's examine different initialization techniques&rsquo; effect on model performance and training time with <code>fashion MNIST</code> dataset.</p>
<pre><code class="language-python">plt.figure(figsize=(10, 10))
for row in range(5):
  for col in range(5):
    index = 5 * row + col
    plt.subplot(5, 5, index + 1)
    plt.imshow(X_train_full[index], cmap=&quot;binary&quot;, interpolation=&quot;nearest&quot;)
    plt.axis('off')
    plt.title(y_train_full[index], fontsize=12)
plt.show()
</code></pre>
<figure>
  <img src="fashion_set.png" alt="" style="width:80%">
  <figcaption>Here is the example of Fashion MNIST, in which the predictors are a set of values in the shape of [28,28] representing the image; and the target value is 10 types of cloth and shoes (denoted from 0 to 9)</figcaption>
</figure>
<p>First, let's start with the default setting of Keras on a network consisting of 5 hidden layers and 300, 100, 50, 50, 50 neurons each.</p>
<pre><code class="language-python">tf.random.set_seed(50)
np.random.seed(50)

model_default = keras.models.Sequential()
model_default.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_layers in (300, 100, 50, 50, 50):
  model_default.add(keras.layers.Dense(n_layers, activation ='relu'))
model_default.add(keras.layers.Dense(10, activation='softmax'))
    
model_default.compile(loss=&quot;sparse_categorical_crossentropy&quot;,
                      optimizer=keras.optimizers.SGD(lr=1e-3),
                      metrics=[&quot;accuracy&quot;])

start_time = time.time()
history = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)
print(&quot;--- %s seconds ---&quot; % (time.time() - start_time))
</code></pre>
<p>Result</p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.4185 - accuracy: 0.8526 - val_loss: 0.4256 - val_accuracy: 0.8518
--- 99.03307843208313 seconds ---
</code></pre>
<p>The train set reached 85.26% accuracy and Val set reached 85.18% within 99.3 seconds. If <code>activation ='relu'</code> is not set (i.e. no Activation function in the hidden layers), the accuracy is 85.32% and 84.95% respectively with 104.5 seconds needed to train on.</p>
<p>Comparing this with weight initialization to all Zeros and all Ones:</p>
<pre><code class="language-python"># Zeros initialization
Epoch 20/20
1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925
--- 69.43926930427551 seconds ---

# Ones initialization
Epoch 20/20
1688/1688 [==============================] - 3s 2ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0925
--- 67.2280786037445 seconds ---
</code></pre>
<p>The performance in both cases is much worse and actually the model stopped improving from 5th epoch.</p>
<p>Another Initialization that can be considered to use is <code>He Initialization</code>,  enabling in Keras by adding <code>kernel_initializer=&quot;he_normal&quot;</code> argument to the hidden layers.</p>
<p>Result</p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3780 - accuracy: 0.8672 - val_loss: 0.3924 - val_accuracy: 0.8637
--- 99.76096153259277 seconds ---
</code></pre>
<p>The accuracy actually improved but the running time was half a second slower than <strong>Glorot Initialization</strong></p>
<p>There are also discussions about the performance of <strong>normal distribution</strong> and <strong>uniform distribution</strong> in initialization technique, but there is indeed no one shows better performance than the other one. The result of <code>init = keras.initializers.VarianceScaling(scale=2.,mode='fan_avg',distribution='uniform')</code> does not improve for this data set (Train set accuracy: 87.05%, Val set: 86.27% and took 100.82 seconds to run)</p>
<h2 id="2-get-along-with-the-right-activation-function">2. Get along with the right Activation function</h2>
<p>Choosing an unfit activation function is one of the reasons leading to poor model performance. <code>sigmoid</code> might be a good choice but I prefer to use <strong>SELU, ReLU, or its variants</strong> instead.</p>
<p>Let's talk about <strong>ReLU</strong> first. Simply saying, if the value is larger than 0, the function returns the value itself; else it returns 0. This activation is fast to compute but in return there will be a case that it stops outputting anything other than 0 (i.e neurons were died). This issue usually happens in case of a large learning rate.</p>
<figure>
  <img src="relu_and_lrelu.png" alt="" style="width:100%">
  <figcaption>ReLU, Leaky ReLU and SELU</figcaption>
</figure>
<p>Some of the solutions for this problem is to use alternative versions of ReLU: <strong>LeakyReLU, Randomized LeakyReLU or Scaled ReLU (SELU)</strong>.</p>
<p>With <strong>LeakyReLU</strong>:</p>
<pre><code class="language-python">if x&gt;0:
  return x
else:
  return ax
</code></pre>
<p>in which a is $\alpha$, the slope of the $x$ given $x&lt;0$. $\alpha$ is usually set at 0.01, serving as a small leak (that's why this technique is called LeakyReLU). Using $\alpha$ helps to stop the dying problem (i.e. slope=0).</p>
<p>In case of <strong>Randomized LeakyReLU</strong>, $\alpha$ is selected randomly given a range. This method can reduce the Overfitting issue but requires more running time due to extra computation.</p>
<p>One of the outperformed activation function for DNN is <strong>Scaled ReLU (SELU)</strong>.</p>
<pre><code class="language-python">if x&gt;0:
  return Lambda*x
else:
  return Lambda*(alpha*exp(x)-alpha)
</code></pre>
<p>In this function, each layer outputs&rsquo; mean is 0 and standard deviation is 1. Note when using this activation function:</p>
<ul>
<li><input checked="" disabled="" type="checkbox">It must be used with <code>kernel_initializer=&quot;lecun_normal&quot;</code></li>
<li><input checked="" disabled="" type="checkbox">The input features must be standardized</li>
<li><input checked="" disabled="" type="checkbox">The NN's architecture must be sequential</li>
</ul>
<p>Let's try different Activation functions on the <code>fashion MNIST</code> dataset.</p>
<p>Result of <strong>LeakyReLU</strong></p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3791 - accuracy: 0.8670 - val_loss: 0.3910 - val_accuracy: 0.8615
--- 101.87710905075073 seconds ---
</code></pre>
<p>Result of <strong>Randomized LeakyReLU</strong></p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 6s 3ms/step - loss: 0.3779 - accuracy: 0.8667 - val_loss: 0.3918 - val_accuracy: 0.8630
--- 113.58738899230957 seconds ---
</code></pre>
<p>Result of <strong>SELU</strong></p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 19/20
1688/1688 [==============================] - 5s 3ms/step - loss: 0.3526 - accuracy: 0.8763 - val_loss: 0.3755 - val_accuracy: 0.8647
--- 106.25733232498169 seconds ---
</code></pre>
<p><strong>SELU</strong> seems to achieve slightly better performance over ReLU and its variants but the speed is slower (as expected).</p>
<span class="markup-quote"><strong>If the NN performs relatively well at a low learning rate, ReLU is an optimal choice given the fastest training time. In case of the deep NN, SELU is an excellent try.</strong></span>
<p>Detailed explanation about these activations can be found in here: <a href="http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">ReLU</a>, <a href="https://arxiv.org/abs/1505.00853">LeakyReLU, Randomized LeakyReLU</a> and <a href="https://arxiv.org/abs/1706.02515">SELU</a></p>
<h2 id="3-batch-normalization">3. Batch Normalization</h2>
<p>To ensure Vanishing/Exploding Gradients problems do not happen again during training (as Initialization and Activation function can help reduce these issues at the beginning of the training), <strong>Batch Normalization</strong> is implemented.</p>
<p><strong>Batch Normalization</strong> zeros centers and normalizes each input, then scales and shifts the result using 1 parameter vector for scaling and 1 for shifting. This technique evaluates the $mean$ and $standard deviation$ of the input over the current mini-batch and repeats this calculation across all mini-batches of the training set. $\mu$ and $\sigma$ are estimated during training but only used after training.</p>
<p>The vector of input means $\mu$ and vector of input standard devition $\sigma$ will become non-trainable parameters (i.e. untouchable by backpropagation) and be used to compute the moving averages at the end of the training. Subsequently, these final parameters will be used to normalize new data to make prediction.</p>
<p>If using <strong>Batch Normalization</strong>, the input data will not need to be standardized prior training.</p>
<pre><code class="language-python">tf.random.set_seed(50)
np.random.seed(50)

model_default = keras.models.Sequential()
model_default.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_layers in (300, 100, 50, 50, 50):
  model_default.add(keras.layers.BatchNormalization())
  model_default.add(keras.layers.Dense(n_layers, activation ='relu', kernel_initializer=&quot;he_normal&quot;))
model_default.add(keras.layers.Dense(10, activation='softmax'))
    
model_default.compile(loss=&quot;sparse_categorical_crossentropy&quot;,
                      optimizer=keras.optimizers.SGD(lr=1e-3),
                      metrics=[&quot;accuracy&quot;])

start_time = time.time()
history = model_default.fit(X_train_full, y_train_full, epochs=20, validation_split=0.1)
print(&quot;--- %s seconds ---&quot; % (time.time() - start_time))
</code></pre>
<p>Result</p>
<pre><code class="language-python"># Show the highest accuracy epoch
Epoch 20/20
1688/1688 [==============================] - 8s 5ms/step - loss: 0.3799 - accuracy: 0.8645 - val_loss: 0.3571 - val_accuracy: 0.8685
--- 167.6186249256134 seconds ---
</code></pre>
<p>Obviously, training is slower in <strong>Batch Normalization</strong> given more computations during training but in contrast, in <strong>Batch Normalization</strong>, the model convergences faster so fewer epoches are needed to reach the same performance.</p>
<div class="alert alert-warning">
  <div>
    Batch Normalization is strictly implemented in Recurrent NN
  </div>
</div>
<h2 id="4-gradient-clipping">4. Gradient Clipping</h2>
<p>As <strong>Batch Normalization</strong> is recommended not to use with Recurrent NN, <strong>Gradient Clipping</strong> is the alternative choice for RNN.</p>
<p>Details about <a href="https://arxiv.org/abs/1211.5063">Gradient Clipping</a></p>
<hr>
<h2 id="summary-of-the-result-of-classification-task-with-fashion-mnist-dataset">Summary of the result of Classification task with Fashion MNIST dataset</h2>
<table>
<thead>
<tr>
<th>Initialization</th>
<th align="center">Activation fuction</th>
<th align="center">Train set accuracy</th>
<th align="center">Val set accuracy</th>
<th align="center">Running time (seconds)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Glorot - Zeros/Ones</td>
<td align="center">ReLU</td>
<td align="center">10.08%</td>
<td align="center">9.25%</td>
<td align="center">69.43/67.22</td>
</tr>
<tr>
<td>Glorot</td>
<td align="center">None</td>
<td align="center">85.32%</td>
<td align="center">84.95%</td>
<td align="center">104.5</td>
</tr>
<tr>
<td>Glorot - Normal Dist</td>
<td align="center">ReLU</td>
<td align="center">85.26%</td>
<td align="center">85.18%</td>
<td align="center">99.03</td>
</tr>
<tr>
<td>He - Normal Dist</td>
<td align="center">ReLU</td>
<td align="center">86.72%</td>
<td align="center">86.37%</td>
<td align="center">99.76</td>
</tr>
<tr>
<td>He - Uniform  Dist</td>
<td align="center">ReLU</td>
<td align="center">87.05%</td>
<td align="center">86.27%</td>
<td align="center">100.82</td>
</tr>
<tr>
<td>He - Normal  Dist</td>
<td align="center">Leaky ReLU</td>
<td align="center">86.7%</td>
<td align="center">86.15%</td>
<td align="center">101.87</td>
</tr>
<tr>
<td>He - Normal  Dist</td>
<td align="center">Randomized LeakyReLU</td>
<td align="center">86.67%</td>
<td align="center">86.3%</td>
<td align="center">113.58</td>
</tr>
<tr>
<td>LeCun</td>
<td align="center">SELU</td>
<td align="center">87.63%</td>
<td align="center">86.47%</td>
<td align="center">106.25</td>
</tr>
<tr>
<td>Batch normalization He - Normal  Dist</td>
<td align="center">ReLU</td>
<td align="center">86.45%</td>
<td align="center">86.85%</td>
<td align="center">167.618</td>
</tr>
</tbody>
</table>
<h2 id="summary-of-the-result-of-regression-task-with-california-housing-dataset">Summary of the result of Regression task with California housing dataset</h2>
<table>
<thead>
<tr>
<th>Initialization</th>
<th align="center">Activation fuction</th>
<th align="center">Train set MSE</th>
<th align="center">Val set MSE</th>
<th align="center">Running time (seconds)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Glorot</td>
<td align="center">None</td>
<td align="center">0.3985</td>
<td align="center">0.3899</td>
<td align="center">9.34</td>
</tr>
<tr>
<td>Glorot - Normal Dist</td>
<td align="center">ReLU</td>
<td align="center">0.3779</td>
<td align="center">0.3819</td>
<td align="center">9.36</td>
</tr>
<tr>
<td>He - Normal Dist</td>
<td align="center">ReLU</td>
<td align="center">0.3517</td>
<td align="center">0.35</td>
<td align="center">9.19</td>
</tr>
<tr>
<td>He - Normal  Dist</td>
<td align="center">Leaky ReLU</td>
<td align="center">0.3517</td>
<td align="center">0.35</td>
<td align="center">9.48</td>
</tr>
<tr>
<td>He - Normal  Dist</td>
<td align="center">Randomized LeakyReLU</td>
<td align="center">0.3517</td>
<td align="center">0.35</td>
<td align="center">10.71</td>
</tr>
<tr>
<td>LeCun</td>
<td align="center">SELU</td>
<td align="center">0.3423</td>
<td align="center">0.326</td>
<td align="center">9.38</td>
</tr>
<tr>
<td>Batch normalization He - Normal  Dist</td>
<td align="center">ReLU</td>
<td align="center">0.4365</td>
<td align="center">0.5728</td>
<td align="center">13.64</td>
</tr>
</tbody>
</table>
<p>MSE of Train and Validation set</p>
<figure>
  <img src="MSE.png" alt="" style="width:100%">
  <figcaption></figcaption>
</figure>
<figure>
  <img src="regression_BN.png" alt="" style="width:35%">
  <figcaption>Fashion MNIST consists of image on 10 types of fashion</figcaption>
</figure>
<div class="alert alert-note">
  <div>
    These performances are subject to change depending on the dataset and NN's architecture
  </div>
</div>
<h1 id="final-thoughts-on-this-part-">Final thoughts on this part 🔆</h1>
<ul>
<li>Glorot Initialization is the good starting point for most of the cases. He Initialization technique sometimes performs better than Glorot (slower in the above Classification example while faster in Regression example).</li>
<li>ReLU or Leaky ReLU are great choices if running time is the priority.</li>
<li>ReLU should be avoided if high Learning rate is used.</li>
<li>SELU is the good choice for complex dataset and deep neural network but might be traded off by running time. However, if the NN's architecture does not allow <em>self-normalization</em>, use ELU instead of SELU.</li>
<li>SELU and Batch Normalization cannot be applied in RNN. Gradient Clipping is the alternative strategy for Batch Normalization in RNN.</li>
</ul>
<hr>
<h2 id="5-transfer-learning">5. Transfer Learning</h2>
<p>Another important technique too improve the performance of DNN is <strong>Transfer Learning</strong>, using pretrained layers to train similar new task. There is much to say about this technique and it will be covered in another post.</p>
<p>Source code can be accessed <a href="https://github.com/geniusnhu/DNN-Improvement/blob/master/Improve_DNN_performance.ipynb">here</a></p>
<hr>
<p>Reference:</p>
<ol>
<li>Glorot, X., &amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. PMLR</li>
<li>He, K., Zhang, X., Ren,S., &amp; Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</li>
<li>Geron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media, Inc.,</li>
<li>Xu, B., Wang, N., Chen, T., &amp; Li, M. (2015). Empirical Evaluation of Rectified Activations in Convolutional Network. Retrieved from <a href="https://arxiv.org/abs/1505.00853">https://arxiv.org/abs/1505.00853</a> on May 5, 2020.</li>
<li>Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. Advances in Neural Information Processing Systems 30 (NIPS 2017)</li>
</ol>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep learning</a>
  
  <a class="badge badge-light" href="/tags/data-science/">Data Science</a>
  
  <a class="badge badge-light" href="/tags/initialization/">Initialization</a>
  
  <a class="badge badge-light" href="/tags/activation-function/">Activation function</a>
  
  <a class="badge badge-light" href="/tags/batch-normalization/">Batch Normalization</a>
  
  <a class="badge badge-light" href="/tags/tensorflow/">Tensorflow</a>
  
  <a class="badge badge-light" href="/tags/hyperparameter-tuning/">Hyperparameter tuning</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/&amp;text=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/&amp;t=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net&amp;body=/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/&amp;title=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net%20/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/&amp;title=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  
    
  
  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hueadf2c06d268c6c318a52f80e38ba7bf_8103393_250x250_fill_q90_lanczos_center.JPG" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Nhu Hoang</a></h5>
      <h6 class="card-subtitle">Data Scientist at White Narwhal Japan</h6>
      <p class="card-text">Specialized in Time series forecast, Recommendation system; Machine learning and Deep learning. Exploiting is my gut and exploring is my drive.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/nhu-hoang/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/geniusnhu" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@hoanganhquynhnhu" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>




<section id="comments">
  
    

  
</section>




<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/project/2020-06-14-optimization-and-neural-network-performance/" rel="next">Improve deep neural network training speed and performance with Optimization</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/project/2020-04-30-support-vector-machine-explanation-and-application/" rel="prev">Support Vector Machine explanation and application</a>
  </div>
  
</div>

</div>



  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/project/2020-03-30-complete-guide-for-time-series-visualization/">Complete guide for Time series Visualization</a></li>
      
      <li><a href="/2020/02/12/the-beauty-of-transformer-in-bringing-more-applications-to-life/">The beauty of Transformer in bringing more applications to life</a></li>
      
      <li><a href="/project/2020-01-26-arima-autoregressive-intergreated-moving-average/">ARIMA Autoregressive Integrated Moving Average model family</a></li>
      
      <li><a href="/2020/01/26/practical-flow-of-a-data-science-project/">Practical flow of a Data Science Project</a></li>
      
    </ul>
  </div>
  



    <div class="project-related-pages content-widget-hr">
      
      

      
      
      

      
      
      

      
      
      
    </div>
  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.3/mermaid.min.js" integrity="" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/markdown.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a0d331bcd05dbe8b31e244f796710f08.js"></script>

    


  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  <p class="powered-by">
    © 2019 Nhu Hoang<br/>Served by <a href="https://geniusnhu.netlify.app/">Netlify</a> &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "nhu-hoang" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
