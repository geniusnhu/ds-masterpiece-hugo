<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Nhu Anh Quynh Hoang">

  
  
  
    
  
  <meta name="description" content="DSpeed up deep neural network training by tuning Optimizer in Tensorflow">

  
  <link rel="alternate" hreflang="en-us" href="/project/2020-05-30-optimization-and-neural-network-performance/">

  


  
  
  
  <meta name="theme-color" content="#01395e">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/agate.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/agate.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Comfortaa:700%7CWork+Sans&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158640946-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-158640946-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hubeb4088d9e1ef3a12a3be812bce5943c_42313_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hubeb4088d9e1ef3a12a3be812bce5943c_42313_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/project/2020-05-30-optimization-and-neural-network-performance/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@nhu_hoang">
  <meta property="twitter:creator" content="@nhu_hoang">
  
  <meta property="og:site_name" content="Nhu Hoang">
  <meta property="og:url" content="/project/2020-05-30-optimization-and-neural-network-performance/">
  <meta property="og:title" content="Speed up training and improve performance in deep neural net | Nhu Hoang">
  <meta property="og:description" content="DSpeed up deep neural network training by tuning Optimizer in Tensorflow"><meta property="og:image" content="/project/2020-05-30-optimization-and-neural-network-performance/featured.jpg">
  <meta property="twitter:image" content="/project/2020-05-30-optimization-and-neural-network-performance/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-06-14T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-06-14T00:00:00&#43;00:00">
  

  


    











<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/project/2020-05-30-optimization-and-neural-network-performance/"
  },
  "headline": "Speed up training and improve performance in deep neural net",
  
  "image": [
    "/project/2020-05-30-optimization-and-neural-network-performance/featured.jpg"
  ],
  
  "datePublished": "2020-06-14T00:00:00Z",
  "dateModified": "2020-06-14T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Nhu Anh Quynh Hoang"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Nhu Hoang",
    "logo": {
      "@type": "ImageObject",
      "url": "img//"
    }
  },
  "description": "DSpeed up deep neural network training by tuning Optimizer in Tensorflow"
}
</script>

  

  


  


  





  <title>Speed up training and improve performance in deep neural net | Nhu Hoang</title>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  






  


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/"><img src="/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_0x70_resize_lanczos_2.png" alt="Nhu Hoang"></a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/"><img src="/images/logo_hubeb4088d9e1ef3a12a3be812bce5943c_42313_0x70_resize_lanczos_2.png" alt="Nhu Hoang"></a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#portfolios"><span>Portfolio</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts/News</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#skills"><span>Experience/Skill</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      

      

    </ul>

  </div>
</nav>


  <article class="article article-project">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Speed up training and improve performance in deep neural net</h1>

  
  <p class="page-subtitle">Improve deep neural network training speed and performance using Optimization</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Nhu Anh Quynh Hoang</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jun 14, 2020
  </span>
  

  

  

  
  
  
  <span class="middot-divider"></span>
  <a href="/project/2020-05-30-optimization-and-neural-network-performance/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/data-science/">Data Science</a>, <a href="/categories/deep-learning/">Deep learning</a></span>
  

</div>

  














</div>


<div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width: 1200px; max-height: 629px;">
  <div style="position: relative">
    <img src="/project/2020-05-30-optimization-and-neural-network-performance/featured_huc7c3cf35002a2878e0d6cf921f0c8be5_139478_1200x0_resize_q90_lanczos.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p><a href="https://geniusnhu.netlify.app/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/">Part 1: Initialization, Activation function and Batch Normalization/Gradient Clipping</a></p>
<p><strong>Part 2: Optimizers</strong></p>
<p>Training a deep neural network is an extremely time-consuming task especially with complex problems. Using a faster optimizer for the network is an efficient way to speed up the training speed, rather than simply using the regular Gradient Descent optimizer. Below, I will discuss and show training results/speed of 5 popular Optimizer approaches: <strong>Gradient Descent with momentum and Nesterov Accelerated Gradient, AdaGrad, RMSProp, Adam, and Nadam optimizer</strong>.</p>
<p>One of the dangers of using inappropriate optimizers is that the model takes a long time to converge to a global minimum or it will be stuck at a local minimum, resulting in a worse model. Therefore, knowing which Optimizer suits mostly on the problem will save you tons of training hours.
The main purpose of tuning Optimizer is to speed up the training speed but it also helps to improve the model's performance.</p>
<h2 id="1-gradient-descent">1. Gradient Descent</h2>
<p>Computing the gradient of the associated cost function with regard to each theta and getting the gradient vector pointing uphill, then going in the opposite direction with the vector direction (downhill) using the below equation:</p>
<p>$\theta_{next step} = \theta - \eta* ???_{\theta}J(\theta)$</p>
<p>$\theta$: weight of the model</p>
<p>$\eta$: learning rate</p>
<p>Therefore, the speed of Gradient Descent optimizer depends solely on the learning rate parameter (eta). With a small learning rate, GD will take small and unchanged steps downward on a gentle surface, and a bit faster steps on a steep surface. Consequently, in a large neural network, it repeats millions of slow steps until it reaches the global minimum (or gets lost in the local minimum). Therefore, the runtime becomes extremely slow.</p>
<p>Result of training with Fashion MNIST dataset usign SGD</p>
<figure>
  <img src="SGD.png" alt="" style="width:80%">
  <figcaption>Firgure 1: Loss and accuracy of model using SGD with learning rate 0.001</figcaption>
</figure>
<p>The loss declined gradually and will be closer and closer to global minimum after several more epochs</p>
<p>There are other versions of Gradient Descent such as <strong>Batch Gradient Descent</strong> (running on a full dataset), <strong>Mini-batch Gradient Descent</strong> (running on random subsets of a dataset), <strong>Stochastic Gradient Descent - SGD</strong> (picking a random instance at each step), and all have pros and cons. <strong>Batch Gradient Descent</strong> can reach the global minimum at a terribly slow pace. <strong>Mini-batch Gradient Descent</strong> gets to the global minimum faster than BGD but it is easier to get stuck in the local minimum, and <strong>SGD</strong> is usually harder to get to the global minimum compared to the other two.</p>
<h2 id="2-momentum-optimization">2. Momentum Optimization</h2>
<p>Let's imagine, when a ball rolls from the summit, downward the sloping side to the foot of the hill, it will start slowly then increase the speed as the momentum picks up and eventually reaches a fast pace toward the minimum. This is how <strong>Momentum Optimization</strong> works. This is enabled by adding a momentum vector m and update the theta parameter with this new weight from <em>momentum vector</em> $m$</p>
<p>$m$ ← $\beta m - \eta * ???_{\theta}J(\theta)$</p>
<p>$\theta_{next step}$ ← $\theta + m$</p>
<p>Gradient descent does not take into account the previous gradients. By adding the momentum vector, it updates the weight m after each iteration. The momentum ?? is the parameter controls how fast the terminal velocity is, which is typically set at 0.9 but it should be tuned from 0.5 to 0.9. As a result, Momentum Optimizer converges better and faster than SGD.</p>
<pre><code class="language-python"># Implement Momentum optimizer in Tensorflow
optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.9)
</code></pre>
<figure>
  <img src="SDG_momentum.png" alt="" style="width:80%">
  <figcaption>Figure 2: Loss and accuracy of models using SGD compared to momentum optimizer</figcaption>
</figure>
<p>Momentum converges faster and eventually reaches a better result than SGD.</p>
<h2 id="3-nesterov-accelerated-gradient">3. Nesterov Accelerated Gradient</h2>
<p>Another variation of Momentum Optimizer is NAG.</p>
<p>$m$ ← $\beta m - \eta * ???_{\theta}J(\theta + \beta m)$</p>
<p>$\theta_{next step}$ ← $\theta + m$</p>
<p>The gradient of the cost function is measured at location $\theta + \beta m$ (instead of ?? in the original momentum optimization). The reason behind this is that momentum optimization has already pointed toward the right direction, so we should use a slightly ahead location (an approximately next position of the $\theta$) to moderately accelerating the speed of convergence.</p>
<pre><code class="language-python"># Implement Nesterov Accelerated Gradient optimizer in Tensorflow
optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.9,nesterov=True
</code></pre>
<figure>
  <img src="NAG_momentum.png" alt="" style="width:80%">
  <figcaption>Figure 3: Loss and accuracy of models using momentum compared to Nesterov Accelerated Gradient optimizer</figcaption>
</figure>
<p>NAG showed only a slightly better result than original Momentum.</p>
<h2 id="4-adagrad">4. AdaGrad</h2>
<p>One of the <em>Adaptive learning rate methods</em>, in which the algorithm goes faster down the steep slopes than the gentle slopes.
<strong>AdaGrad</strong> performs well in a simple quadratic problem but not in training a neural network because it tends to slow down a bit too fast and stops before reaching the global minimum. Due to this drawback, I do not usually use <strong>AdaGrad</strong> for Neural Network but instead apply <strong>RMSProp</strong>, an alternative of <strong>AdaGrad</strong>.</p>
<h2 id="5-rmsprop---root-mean-square-prop">5. RMSProp - Root Mean Square Prop</h2>
<p>This is one of the most frequently used optimizers, which continues the idea of <em>Adagrad</em> in trying to minimize the vertical movement and updating the model in a horizontal direction toward the global minimum.</p>
<p><em>Adagrad</em> sums the gradients from the first iteration and that is why it usually never converges to the global minimum, while <strong>RMSProp</strong> accumulates the gradients from the previous iterations:</p>
<p>$s$ ← $\beta s - (1-\beta) ???_{\theta}J(\theta)^2$</p>
<p>$\theta_{nextstep}$ ← $\theta + \frac{\eta ???_{\theta}J(\theta)}{\sqrt{s + \epsilon}}$</p>
<p>$\beta$: decay rate, typically set at 0.9</p>
<p>$s$: exponential average square of past gradients</p>
<pre><code class="language-python"># Implement RMSProp optimizer in Tensorflow
optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9)
</code></pre>
<figure>
  <img src="adagrad_rmsprop.png" alt="" style="width:80%">
  <figcaption>Figure 4: Loss and accuracy of models using RMSProp compared to Adagrad optimizer</figcaption>
</figure>
<p>RMSProp converges better than Adagrad which is lost at a plateau.</p>
<h2 id="6-adam">6. Adam</h2>
<p>Adam optimizer is the combination of momentum and RMSProp optimizers. In other words, it takes into account both the exponential decay average of past gradients and the exponential decay average of past squared gradients.</p>
<p>With these characteristics, Adam is suitable for handling sparse gradients on complex problems with complex data and a large number of features.</p>
<p>$m$ ← $\beta_1 m - (1-\beta_1) ???_{\theta}J(\theta)$</p>
<p>$s$ ← $\beta_2 s - (1-\beta_2) ???_{\theta}J(\theta)$</p>
<p>$\hat{m}$ ← $\frac{m}{1-\beta_1^T}$    ;    $\hat{s}$ ← $\frac{s}{1-\beta_2^T}$</p>
<p>$\theta_{nextstep}$ ← $\theta + \frac{\eta \hat{m}}{\sqrt{\hat{s} + \epsilon}}$</p>
<p>$\eta$: learning rate</p>
<p>$s$: exponential average square of past gradients</p>
<p>$m$: momentum vector</p>
<p>$\beta_1$: momentum decay, typlically set at 0.9</p>
<p>$\beta_2$: scaling decay, typlically set at 0.999</p>
<p>$\epsilon$: smoothing term</p>
<pre><code class="language-python"># Implement Adam optimizer in Tensorflow
optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
</code></pre>
<figure>
  <img src="adagrad_rmsprop_adam.png" alt="" style="width:80%">
  <figcaption>Figure 5: Loss and accuracy of models using Adagrad, RMSProp, and Adam</figcaption>
</figure>
<h2 id="7-nadam">7. Nadam</h2>
<p>Another variation of Adam is Nadam (using Adam optimizer with Nesterov technique), resulting in a little faster training time than Adam.</p>
<pre><code class="language-python"># Implement Nadam optimizer in Tensorflow
optimizer=keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)
</code></pre>
<figure>
  <img src="rmsprop_adam_nadam.png" alt="" style="width:80%">
  <figcaption>Figure 6: Loss and accuracy of models using RMSProp, Adam and Nadam</figcaption>
</figure>
<p><strong>Adagrad, RMSProp, Ada, Nadam, and Adamax</strong> are <em>Adaptive learning rate algorithms</em>, which require less tuning on hyperparameters. In case the performance of the model does not meet your expectation, you can try to change back to <strong>Momentum optimizer</strong> or <strong>Nesterov Accelerated Gradient</strong>.</p>
<h2 id="final-words-">Final words ????</h2>
<p>In conclusion, most of the time, <em>Adaptive learning rate algorithms</em> outperform <em>Gradient descent</em> and its variants in terms of speed, especially in a deep neural network. However, <em>Adaptive learning rate algorithms</em> do not ensure an absolute convergence to the global minimum.</p>
<p>If your model is not too complex with a small number of features, and training time is not your priority, using <strong>Momentum</strong>, <strong>Nesterov Accelerated Gradient</strong> or <strong>SGD</strong> is the optimal starting point, then tune the learning rate, activation functions, change Initialization technique to improve the model rather than using <strong>Adaptive learning rate Optimizers</strong> because the later ones hinder the risk of not converging to the global minimum.</p>
<figure>
  <img src="summary.png" alt="" style="width:80%">
  <figcaption>Figure 7: Summary model performance on training loss of different optimization techniques</figcaption>
</figure>
<ul>
<li>Regular SGD or regular Gradient Descent takes much more time to converge to the global minimum. Adagrad often stops too early before reaching the global minimum so in time it becomes the worse optimizer.</li>
<li>With the Fashion MNIST dataset, Adam/Nadam eventually performs better than RMSProp and Momentum/Nesterov Accelerated Gradient. This depends on the model, usually, Nadam outperforms Adam but sometimes RMSProp gives the best performance.</li>
<li>With my experience, I found out that Momentum, RMSProp, and Adam (or Nadam) should be the first try of the model.</li>
</ul>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Optimizer</th>
<th align="center">Training speed</th>
<th align="center">Converge quality</th>
<th align="center">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Descent / SGD</td>
<td align="center">Medium for simple model<br>Slow for complex model</td>
<td align="center">Good</td>
<td align="center">Risk of converging to local minimum.<br>Can be controled by assigning the correct learning rate</td>
</tr>
<tr>
<td>Momentum</td>
<td align="center">Fast for simple model<br>Medium for complex model</td>
<td align="center">Good</td>
<td align="center">Suitable for less complex NN with small number of features<br>Need to consider tuning the momentum hyperparameter</td>
</tr>
<tr>
<td>Nesterov Accelerated<br>Gradient</td>
<td align="center">Fast for simple model<br>Medium for complex model</td>
<td align="center">Good</td>
<td align="center">Suitable for less complex NN with small number of features<br>Need to consider tuning the momentum hyperparameter</td>
</tr>
<tr>
<td>AdaGrad</td>
<td align="center">Fast</td>
<td align="center">Usually miss global minimum<br>due to early stopping</td>
<td align="center">Suitable for simple quadratic problem, not NN</td>
</tr>
<tr>
<td>RMSProp</td>
<td align="center">Fast</td>
<td align="center">Acceptable</td>
<td align="center">Suitable for complex NN<br>Need to tune Decay rate for better performance</td>
</tr>
<tr>
<td>Adam</td>
<td align="center">Fast</td>
<td align="center">Acceptable</td>
<td align="center">Suitable for sparse gradients on complex model<br>with a large number of features</td>
</tr>
<tr>
<td>Nadam</td>
<td align="center">Fast</td>
<td align="center">Good</td>
<td align="center">Suitable for sparse gradients on complex model<br>with a large number of features</td>
</tr>
</tbody>
</table>
<hr>
<p>This article was originally published in <a href="https://towardsdatascience.com/full-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78">Towards Data Science</a></p>
<p>Source code: <a href="https://github.com/geniusnhu/DNN-Improvement/blob/master/Tuning_Optimizer.ipynb">here</a></p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep learning</a>
  
  <a class="badge badge-light" href="/tags/data-science/">Data Science</a>
  
  <a class="badge badge-light" href="/tags/optimization/">Optimization</a>
  
  <a class="badge badge-light" href="/tags/hyperparameter-tuning/">Hyperparameter tuning</a>
  
  <a class="badge badge-light" href="/tags/tensorflow/">Tensorflow</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/project/2020-05-30-optimization-and-neural-network-performance/&amp;text=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/project/2020-05-30-optimization-and-neural-network-performance/&amp;t=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net&amp;body=/project/2020-05-30-optimization-and-neural-network-performance/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/project/2020-05-30-optimization-and-neural-network-performance/&amp;title=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net%20/project/2020-05-30-optimization-and-neural-network-performance/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/project/2020-05-30-optimization-and-neural-network-performance/&amp;title=Speed%20up%20training%20and%20improve%20performance%20in%20deep%20neural%20net" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  
    
  
  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hueadf2c06d268c6c318a52f80e38ba7bf_8103393_250x250_fill_q90_lanczos_center.JPG" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Nhu Anh Quynh Hoang</a></h5>
      <h6 class="card-subtitle">Data Scientist / Portfolio Planning Specialist - Amway Japan</h6>
      <p class="card-text">My interest is Time series forecasting, Customer segmentation, classification; Machine learning and Deep learning. I earned Master Degree in Interdisciplinary Studies from Tohoku University and have decent experience in various industries.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/nhu-hoang/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/geniusnhu" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@hoanganhquynhnhu" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>




<section id="comments">
  
    

  
</section>




<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/" rel="prev">Speed up training and improve performance in deep neural net</a>
  </div>
  
</div>

</div>



  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/project/2020-05-30-speed-up-training-time-in-deep-neuron-net/">Speed up training and improve performance in deep neural net</a></li>
      
      <li><a href="/project/2020-03-30-complete-guide-for-time-series-visualization/">Complete guide for Time series Visualization</a></li>
      
      <li><a href="/2020/02/12/the-beauty-of-transformer-in-bringing-more-applications-to-life/">The beauty of Transformer in bringing more applications to life</a></li>
      
      <li><a href="/project/2020-01-26-arima-autoregressive-intergreated-moving-average/">ARIMA Autoregressive Integrated Moving Average model family</a></li>
      
      <li><a href="/2020/01/26/practical-flow-of-a-data-science-project/">Practical flow of a Data Science Project</a></li>
      
    </ul>
  </div>
  



    <div class="project-related-pages content-widget-hr">
      
      

      
      
      

      
      
      

      
      
      
    </div>
  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.3/mermaid.min.js" integrity="" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/markdown.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a0d331bcd05dbe8b31e244f796710f08.js"></script>

    


  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  <p class="powered-by">
    © 2019 Nhu Hoang<br/>Served by <a href="https://geniusnhu.netlify.app/">Netlify</a> &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "nhu-hoang" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
